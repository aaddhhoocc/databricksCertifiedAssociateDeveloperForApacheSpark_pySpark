{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84aeae45-5db8-4cc6-a671-721575844b4f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-23T11:27:44.180935Z",
     "iopub.status.busy": "2024-06-23T11:27:44.180408Z",
     "iopub.status.idle": "2024-06-23T11:27:51.006787Z",
     "shell.execute_reply": "2024-06-23T11:27:51.006367Z",
     "shell.execute_reply.started": "2024-06-23T11:27:44.180905Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/06/23 13:27:50 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/06/23 13:27:50 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('instance').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e450550-c6f1-4846-ad00-02279a30d8de",
   "metadata": {},
   "source": [
    "## Overview of writting files from Spark Data Frames\n",
    "* writting files using direct API such as `csv`, `json`, etc. where df is of type DataFrameWritter\n",
    "* writting files using `format` and `save` under `df.write`\n",
    "* specyfying options as arguments as well as using `option` or `options`\n",
    "* supported file formats `text`, `csv`, `json`, `parquet`, `orc`, etc.\n",
    "* other common file formats `xml`, `avro`\n",
    "* for certification the following formats are important `csv`, `json`, `parquet`\n",
    "* writting into compressed files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb5f1a89-fcd1-4c00-b3ee-d02e31243b36",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-23T11:46:10.928042Z",
     "iopub.status.busy": "2024-06-23T11:46:10.927797Z",
     "iopub.status.idle": "2024-06-23T11:46:11.172807Z",
     "shell.execute_reply": "2024-06-23T11:46:11.172522Z",
     "shell.execute_reply.started": "2024-06-23T11:46:10.928022Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- course_id: long (nullable = true)\n",
      " |-- course_title: string (nullable = true)\n",
      " |-- course_published_dt: date (nullable = true)\n",
      " |-- is_active: boolean (nullable = true)\n",
      " |-- last_updated_ts: timestamp (nullable = true)\n",
      "\n",
      "+---------+--------------------+-------------------+---------+-------------------+\n",
      "|course_id|        course_title|course_published_dt|is_active|    last_updated_ts|\n",
      "+---------+--------------------+-------------------+---------+-------------------+\n",
      "|        1|    Mastering Python|         2021-01-14|     true|2021-02-18 16:57:25|\n",
      "|        2|Data Engineering ...|         2021-02-10|     true|2021-03-05 12:07:33|\n",
      "|        3|   Mastering Pyspark|         2021-01-07|     true|2021-04-06 10:05:42|\n",
      "|        4|      AWS Essentials|         2021-03-19|    false|2021-04-10 02:25:36|\n",
      "|        5|          Docker 101|         2021-02-28|     true|2021-03-21 07:18:52|\n",
      "+---------+--------------------+-------------------+---------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "from pyspark.sql import Row\n",
    "\n",
    "courses = [\n",
    "    {\n",
    "        'course_id': 1,\n",
    "        'course_title': 'Mastering Python',\n",
    "        'course_published_dt': datetime.date(2021, 1, 14),\n",
    "        'is_active': True,\n",
    "        'last_updated_ts': datetime.datetime(2021, 2, 18, 16, 57, 25)\n",
    "    },\n",
    "    {\n",
    "        'course_id': 2,\n",
    "        'course_title': 'Data Engineering Essentials',\n",
    "        'course_published_dt': datetime.date(2021, 2, 10),\n",
    "        'is_active': True,\n",
    "        'last_updated_ts': datetime.datetime(2021, 3, 5, 12, 7, 33)\n",
    "    },\n",
    "    {\n",
    "        'course_id': 3,\n",
    "        'course_title': 'Mastering Pyspark',\n",
    "        'course_published_dt': datetime.date(2021, 1, 7),\n",
    "        'is_active': True,\n",
    "        'last_updated_ts': datetime.datetime(2021, 4, 6, 10, 5, 42)\n",
    "    },\n",
    "    {\n",
    "        'course_id': 4,\n",
    "        'course_title': 'AWS Essentials',\n",
    "        'course_published_dt': datetime.date(2021, 3, 19),\n",
    "        'is_active': False,\n",
    "        'last_updated_ts': datetime.datetime(2021, 4, 10, 2, 25, 36)\n",
    "    },\n",
    "    {\n",
    "        'course_id': 5,\n",
    "        'course_title': 'Docker 101',\n",
    "        'course_published_dt': datetime.date(2021, 2, 28),\n",
    "        'is_active': True,\n",
    "        'last_updated_ts': datetime.datetime(2021, 3, 21, 7, 18, 52)\n",
    "    }\n",
    "]\n",
    "\n",
    "#both work\n",
    "courses_df = spark.createDataFrame([Row(**course) for course in courses]) # maintained column order\n",
    "#courses_df = spark.createDataFrame(courses)\n",
    "courses_df.printSchema()\n",
    "courses_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb48ea22-c186-4dd3-96ae-1f64430dc45b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-23T11:46:33.249912Z",
     "iopub.status.busy": "2024-06-23T11:46:33.249381Z",
     "iopub.status.idle": "2024-06-23T11:46:33.274548Z",
     "shell.execute_reply": "2024-06-23T11:46:33.274092Z",
     "shell.execute_reply.started": "2024-06-23T11:46:33.249880Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.readwriter.DataFrameWriter"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(courses_df.write)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ece12901-1750-4ab5-9529-71ee53019911",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-23T11:49:52.037877Z",
     "iopub.status.busy": "2024-06-23T11:49:52.037165Z",
     "iopub.status.idle": "2024-06-23T11:49:52.427743Z",
     "shell.execute_reply": "2024-06-23T11:49:52.427460Z",
     "shell.execute_reply.started": "2024-06-23T11:49:52.037836Z"
    }
   },
   "outputs": [],
   "source": [
    "output_dir = '/Users/adhoc/git/retail_db/json/courses'\n",
    "#courses_df.write.json(output_dir)\n",
    "courses_df.write.json(output_dir, mode='overwrite')\n",
    "courses_df.write.format('json').save(output_dir, mode='overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e80fe4e-00e8-4741-984e-49bfe31905d2",
   "metadata": {},
   "source": [
    "## Steps to follow while writting Spark Data Frames into files\n",
    "* make sure to analyze the schema of the Data Frame\n",
    "* make sure you got write permissions on the target location\n",
    "* understand whether you want to `overwrite`, `append`, `ignore` or `throw exception` in case the target location exists\n",
    "* decide whether you want to compress the data or not\n",
    "* make sure you understand the data will be compressed or not by default\n",
    "* use appropriate API along with right arguments based up on the requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733d53c0-3d9a-48f3-915f-9302916205c5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-23T11:58:56.645670Z",
     "iopub.status.busy": "2024-06-23T11:58:56.644922Z",
     "iopub.status.idle": "2024-06-23T11:58:56.659048Z",
     "shell.execute_reply": "2024-06-23T11:58:56.658068Z",
     "shell.execute_reply.started": "2024-06-23T11:58:56.645623Z"
    }
   },
   "source": [
    "## Writting CSV files into Spark Data Frames\n",
    "There are multiply approches:\n",
    "* Approach 1: `df.write.csv(path_to_folder)`\n",
    "* Approach 2: `df.write.format('csv').save(path_to_folder)`\n",
    "* The column names from the schema can be added as header to each of the files by setting `header=True`\n",
    "* You can save files using customized delimiter by setting `sep` option\n",
    "* You can compress the data while writting data frame into csv files "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ad8591-0a94-4e85-a8b5-bd38976f24ec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-23T12:06:14.531378Z",
     "iopub.status.busy": "2024-06-23T12:06:14.530822Z",
     "iopub.status.idle": "2024-06-23T12:06:14.540911Z",
     "shell.execute_reply": "2024-06-23T12:06:14.536302Z",
     "shell.execute_reply.started": "2024-06-23T12:06:14.531347Z"
    }
   },
   "source": [
    "## Specyfing header while writting to CSV files from Spark Dada Frame "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "30040463-067e-4072-b8d0-6042e7da2d63",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-23T14:06:14.498700Z",
     "iopub.status.busy": "2024-06-23T14:06:14.498083Z",
     "iopub.status.idle": "2024-06-23T14:06:15.039658Z",
     "shell.execute_reply": "2024-06-23T14:06:15.039362Z",
     "shell.execute_reply.started": "2024-06-23T14:06:14.498669Z"
    }
   },
   "outputs": [],
   "source": [
    "output_path = '/Users/adhoc/git/retail_db/csv'\n",
    "courses_df.coalesce(1).write.format('csv').save(output_path, mode='overwrite', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2b4aefd5-df3d-4e41-bbcd-81a69586554a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-23T14:06:16.595231Z",
     "iopub.status.busy": "2024-06-23T14:06:16.594715Z",
     "iopub.status.idle": "2024-06-23T14:06:16.665082Z",
     "shell.execute_reply": "2024-06-23T14:06:16.664639Z",
     "shell.execute_reply.started": "2024-06-23T14:06:16.595203Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------------------+\n",
      "|value                                                                      |\n",
      "+---------------------------------------------------------------------------+\n",
      "|course_id,course_title,course_published_dt,is_active,last_updated_ts       |\n",
      "|1,Mastering Python,2021-01-14,true,2021-02-18T16:57:25.000+01:00           |\n",
      "|2,Data Engineering Essentials,2021-02-10,true,2021-03-05T12:07:33.000+01:00|\n",
      "|3,Mastering Pyspark,2021-01-07,true,2021-04-06T10:05:42.000+02:00          |\n",
      "|4,AWS Essentials,2021-03-19,false,2021-04-10T02:25:36.000+02:00            |\n",
      "|5,Docker 101,2021-02-28,true,2021-03-21T07:18:52.000+01:00                 |\n",
      "+---------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.text(output_path).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "13da010a-b1cc-483f-8739-2c9fb77a971c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-23T14:08:32.483671Z",
     "iopub.status.busy": "2024-06-23T14:08:32.483105Z",
     "iopub.status.idle": "2024-06-23T14:08:32.686636Z",
     "shell.execute_reply": "2024-06-23T14:08:32.686398Z",
     "shell.execute_reply.started": "2024-06-23T14:08:32.483641Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------------------------+-------------------+---------+-----------------------------+\n",
      "|course_id|course_title               |course_published_dt|is_active|last_updated_ts              |\n",
      "+---------+---------------------------+-------------------+---------+-----------------------------+\n",
      "|1        |Mastering Python           |2021-01-14         |true     |2021-02-18T16:57:25.000+01:00|\n",
      "|2        |Data Engineering Essentials|2021-02-10         |true     |2021-03-05T12:07:33.000+01:00|\n",
      "|3        |Mastering Pyspark          |2021-01-07         |true     |2021-04-06T10:05:42.000+02:00|\n",
      "|4        |AWS Essentials             |2021-03-19         |false    |2021-04-10T02:25:36.000+02:00|\n",
      "|5        |Docker 101                 |2021-02-28         |true     |2021-03-21T07:18:52.000+01:00|\n",
      "+---------+---------------------------+-------------------+---------+-----------------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('course_id', 'string'),\n",
       " ('course_title', 'string'),\n",
       " ('course_published_dt', 'string'),\n",
       " ('is_active', 'string'),\n",
       " ('last_updated_ts', 'string')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.csv(output_path, header=True).show(truncate=False)\n",
    "spark.read.csv(output_path, header=True).dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6336066a-d979-4bea-8baf-0b34310c7e89",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-23T14:09:15.743984Z",
     "iopub.status.busy": "2024-06-23T14:09:15.743412Z",
     "iopub.status.idle": "2024-06-23T14:09:15.950757Z",
     "shell.execute_reply": "2024-06-23T14:09:15.950522Z",
     "shell.execute_reply.started": "2024-06-23T14:09:15.743955Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------------------------+-------------------+---------+-------------------+\n",
      "|course_id|course_title               |course_published_dt|is_active|last_updated_ts    |\n",
      "+---------+---------------------------+-------------------+---------+-------------------+\n",
      "|1        |Mastering Python           |2021-01-14         |true     |2021-02-18 16:57:25|\n",
      "|2        |Data Engineering Essentials|2021-02-10         |true     |2021-03-05 12:07:33|\n",
      "|3        |Mastering Pyspark          |2021-01-07         |true     |2021-04-06 10:05:42|\n",
      "|4        |AWS Essentials             |2021-03-19         |false    |2021-04-10 02:25:36|\n",
      "|5        |Docker 101                 |2021-02-28         |true     |2021-03-21 07:18:52|\n",
      "+---------+---------------------------+-------------------+---------+-------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('course_id', 'int'),\n",
       " ('course_title', 'string'),\n",
       " ('course_published_dt', 'date'),\n",
       " ('is_active', 'boolean'),\n",
       " ('last_updated_ts', 'timestamp')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.csv(output_path, header=True, inferSchema=True).show(truncate=False)\n",
    "spark.read.csv(output_path, header=True, inferSchema=True).dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "da38a599-368a-4c35-9e25-3226fb1c0299",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-23T14:09:56.105630Z",
     "iopub.status.busy": "2024-06-23T14:09:56.104841Z",
     "iopub.status.idle": "2024-06-23T14:09:56.211905Z",
     "shell.execute_reply": "2024-06-23T14:09:56.211646Z",
     "shell.execute_reply.started": "2024-06-23T14:09:56.105583Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['course_id',\n",
       " 'course_title',\n",
       " 'course_published_dt',\n",
       " 'is_active',\n",
       " 'last_updated_ts']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.csv(output_path, header=True, inferSchema=True).columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d18a7158-0de8-48fa-addc-7124f1672771",
   "metadata": {},
   "source": [
    "## Using compression while writting files from Spark Data Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f61b7ed8-f6b5-4fa8-9a7b-95b10166b73d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-23T14:17:10.115333Z",
     "iopub.status.busy": "2024-06-23T14:17:10.114717Z",
     "iopub.status.idle": "2024-06-23T14:17:10.689213Z",
     "shell.execute_reply": "2024-06-23T14:17:10.688922Z",
     "shell.execute_reply.started": "2024-06-23T14:17:10.115297Z"
    }
   },
   "outputs": [],
   "source": [
    "# compress using gzip\n",
    "courses_df. \\\n",
    "    coalesce(1).\\\n",
    "    write. \\\n",
    "    format('csv'). \\\n",
    "    save(output_path, header=True, mode='overwrite', compression='gzip',)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "427a2400-83cf-446c-bde2-849c958effad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-23T14:21:06.420678Z",
     "iopub.status.busy": "2024-06-23T14:21:06.419894Z",
     "iopub.status.idle": "2024-06-23T14:21:06.612176Z",
     "shell.execute_reply": "2024-06-23T14:21:06.611824Z",
     "shell.execute_reply.started": "2024-06-23T14:21:06.420633Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+-------------------+---------+-------------------+\n",
      "|course_id|        course_title|course_published_dt|is_active|    last_updated_ts|\n",
      "+---------+--------------------+-------------------+---------+-------------------+\n",
      "|        1|    Mastering Python|         2021-01-14|     true|2021-02-18 16:57:25|\n",
      "|        2|Data Engineering ...|         2021-02-10|     true|2021-03-05 12:07:33|\n",
      "|        3|   Mastering Pyspark|         2021-01-07|     true|2021-04-06 10:05:42|\n",
      "|        4|      AWS Essentials|         2021-03-19|    false|2021-04-10 02:25:36|\n",
      "|        5|          Docker 101|         2021-02-28|     true|2021-03-21 07:18:52|\n",
      "+---------+--------------------+-------------------+---------+-------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('course_id', 'int'),\n",
       " ('course_title', 'string'),\n",
       " ('course_published_dt', 'date'),\n",
       " ('is_active', 'boolean'),\n",
       " ('last_updated_ts', 'timestamp')]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.csv(output_path, header=True, inferSchema=True).show()\n",
    "spark.read.csv(output_path, header=True, inferSchema=True).dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "10bcafb6-fafd-4295-b196-39ec8e14c84c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-23T14:23:02.502054Z",
     "iopub.status.busy": "2024-06-23T14:23:02.501456Z",
     "iopub.status.idle": "2024-06-23T14:23:03.557216Z",
     "shell.execute_reply": "2024-06-23T14:23:03.556702Z",
     "shell.execute_reply.started": "2024-06-23T14:23:02.502028Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#checking snappy for CSV\n",
    "courses_df. \\\n",
    "    coalesce(1).\\\n",
    "    write. \\\n",
    "    format('csv'). \\\n",
    "    save(output_path, header=True, mode='overwrite', compression='snappy',)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "64a784f3-fa1a-445e-809a-7cfe10b35d79",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-23T14:23:38.252346Z",
     "iopub.status.busy": "2024-06-23T14:23:38.251521Z",
     "iopub.status.idle": "2024-06-23T14:23:38.438876Z",
     "shell.execute_reply": "2024-06-23T14:23:38.438565Z",
     "shell.execute_reply.started": "2024-06-23T14:23:38.252317Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+-------------------+---------+-------------------+\n",
      "|course_id|        course_title|course_published_dt|is_active|    last_updated_ts|\n",
      "+---------+--------------------+-------------------+---------+-------------------+\n",
      "|        1|    Mastering Python|         2021-01-14|     true|2021-02-18 16:57:25|\n",
      "|        2|Data Engineering ...|         2021-02-10|     true|2021-03-05 12:07:33|\n",
      "|        3|   Mastering Pyspark|         2021-01-07|     true|2021-04-06 10:05:42|\n",
      "|        4|      AWS Essentials|         2021-03-19|    false|2021-04-10 02:25:36|\n",
      "|        5|          Docker 101|         2021-02-28|     true|2021-03-21 07:18:52|\n",
      "+---------+--------------------+-------------------+---------+-------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('course_id', 'int'),\n",
       " ('course_title', 'string'),\n",
       " ('course_published_dt', 'date'),\n",
       " ('is_active', 'boolean'),\n",
       " ('last_updated_ts', 'timestamp')]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.csv(output_path, header=True, inferSchema=True).show()\n",
    "spark.read.csv(output_path, header=True, inferSchema=True).dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd34e6c9-c771-453c-ba5c-3d865563ce91",
   "metadata": {},
   "source": [
    "## Specifying delimiter while writting Spark Data Frame into files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e6522a24-f661-48fc-bbf3-713f9d02c994",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-23T16:41:07.107042Z",
     "iopub.status.busy": "2024-06-23T16:41:07.106254Z",
     "iopub.status.idle": "2024-06-23T16:41:07.223477Z",
     "shell.execute_reply": "2024-06-23T16:41:07.223220Z",
     "shell.execute_reply.started": "2024-06-23T16:41:07.106997Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+-----+---------------+\n",
      "|_c0|                 _c1|  _c2|            _c3|\n",
      "+---+--------------------+-----+---------------+\n",
      "|  1|2013-07-25 00:00:...|11599|         CLOSED|\n",
      "|  2|2013-07-25 00:00:...|  256|PENDING_PAYMENT|\n",
      "|  3|2013-07-25 00:00:...|12111|       COMPLETE|\n",
      "|  4|2013-07-25 00:00:...| 8827|         CLOSED|\n",
      "|  5|2013-07-25 00:00:...|11318|       COMPLETE|\n",
      "|  6|2013-07-25 00:00:...| 7130|       COMPLETE|\n",
      "|  7|2013-07-25 00:00:...| 4530|       COMPLETE|\n",
      "|  8|2013-07-25 00:00:...| 2911|     PROCESSING|\n",
      "|  9|2013-07-25 00:00:...| 5657|PENDING_PAYMENT|\n",
      "| 10|2013-07-25 00:00:...| 5648|PENDING_PAYMENT|\n",
      "| 11|2013-07-25 00:00:...|  918| PAYMENT_REVIEW|\n",
      "| 12|2013-07-25 00:00:...| 1837|         CLOSED|\n",
      "| 13|2013-07-25 00:00:...| 9149|PENDING_PAYMENT|\n",
      "| 14|2013-07-25 00:00:...| 9842|     PROCESSING|\n",
      "| 15|2013-07-25 00:00:...| 2568|       COMPLETE|\n",
      "| 16|2013-07-25 00:00:...| 7276|PENDING_PAYMENT|\n",
      "| 17|2013-07-25 00:00:...| 2667|       COMPLETE|\n",
      "| 18|2013-07-25 00:00:...| 1205|         CLOSED|\n",
      "| 19|2013-07-25 00:00:...| 9488|PENDING_PAYMENT|\n",
      "| 20|2013-07-25 00:00:...| 9198|     PROCESSING|\n",
      "+---+--------------------+-----+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('_c0', 'string'), ('_c1', 'string'), ('_c2', 'string'), ('_c3', 'string')]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_path = '/Users/adhoc/git/retail_db/orders'\n",
    "orders = spark.read.csv(input_path)\n",
    "orders.show()\n",
    "orders.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "73467855-2141-4180-a0dc-f20a930d0c83",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-23T16:43:33.050966Z",
     "iopub.status.busy": "2024-06-23T16:43:33.050156Z",
     "iopub.status.idle": "2024-06-23T16:43:33.237883Z",
     "shell.execute_reply": "2024-06-23T16:43:33.237599Z",
     "shell.execute_reply.started": "2024-06-23T16:43:33.050918Z"
    }
   },
   "outputs": [],
   "source": [
    "output_path = '/Users/adhoc/git/retail_db/csv/pipe/orders'\n",
    "orders.coalesce(1).write.mode('overwrite').csv(output_path, sep='|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "284f9c06-d384-44f9-994e-2b5941f80667",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-23T16:45:49.509002Z",
     "iopub.status.busy": "2024-06-23T16:45:49.508246Z",
     "iopub.status.idle": "2024-06-23T16:45:49.745967Z",
     "shell.execute_reply": "2024-06-23T16:45:49.745734Z",
     "shell.execute_reply.started": "2024-06-23T16:45:49.508955Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------+\n",
      "|_c0                                          |\n",
      "+---------------------------------------------+\n",
      "|1|2013-07-25 00:00:00.0|11599|CLOSED         |\n",
      "|2|2013-07-25 00:00:00.0|256|PENDING_PAYMENT  |\n",
      "|3|2013-07-25 00:00:00.0|12111|COMPLETE       |\n",
      "|4|2013-07-25 00:00:00.0|8827|CLOSED          |\n",
      "|5|2013-07-25 00:00:00.0|11318|COMPLETE       |\n",
      "|6|2013-07-25 00:00:00.0|7130|COMPLETE        |\n",
      "|7|2013-07-25 00:00:00.0|4530|COMPLETE        |\n",
      "|8|2013-07-25 00:00:00.0|2911|PROCESSING      |\n",
      "|9|2013-07-25 00:00:00.0|5657|PENDING_PAYMENT |\n",
      "|10|2013-07-25 00:00:00.0|5648|PENDING_PAYMENT|\n",
      "|11|2013-07-25 00:00:00.0|918|PAYMENT_REVIEW  |\n",
      "|12|2013-07-25 00:00:00.0|1837|CLOSED         |\n",
      "|13|2013-07-25 00:00:00.0|9149|PENDING_PAYMENT|\n",
      "|14|2013-07-25 00:00:00.0|9842|PROCESSING     |\n",
      "|15|2013-07-25 00:00:00.0|2568|COMPLETE       |\n",
      "|16|2013-07-25 00:00:00.0|7276|PENDING_PAYMENT|\n",
      "|17|2013-07-25 00:00:00.0|2667|COMPLETE       |\n",
      "|18|2013-07-25 00:00:00.0|1205|CLOSED         |\n",
      "|19|2013-07-25 00:00:00.0|9488|PENDING_PAYMENT|\n",
      "|20|2013-07-25 00:00:00.0|9198|PROCESSING     |\n",
      "+---------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+---+---------------------+-----+---------------+\n",
      "|_c0|_c1                  |_c2  |_c3            |\n",
      "+---+---------------------+-----+---------------+\n",
      "|1  |2013-07-25 00:00:00.0|11599|CLOSED         |\n",
      "|2  |2013-07-25 00:00:00.0|256  |PENDING_PAYMENT|\n",
      "|3  |2013-07-25 00:00:00.0|12111|COMPLETE       |\n",
      "|4  |2013-07-25 00:00:00.0|8827 |CLOSED         |\n",
      "|5  |2013-07-25 00:00:00.0|11318|COMPLETE       |\n",
      "|6  |2013-07-25 00:00:00.0|7130 |COMPLETE       |\n",
      "|7  |2013-07-25 00:00:00.0|4530 |COMPLETE       |\n",
      "|8  |2013-07-25 00:00:00.0|2911 |PROCESSING     |\n",
      "|9  |2013-07-25 00:00:00.0|5657 |PENDING_PAYMENT|\n",
      "|10 |2013-07-25 00:00:00.0|5648 |PENDING_PAYMENT|\n",
      "|11 |2013-07-25 00:00:00.0|918  |PAYMENT_REVIEW |\n",
      "|12 |2013-07-25 00:00:00.0|1837 |CLOSED         |\n",
      "|13 |2013-07-25 00:00:00.0|9149 |PENDING_PAYMENT|\n",
      "|14 |2013-07-25 00:00:00.0|9842 |PROCESSING     |\n",
      "|15 |2013-07-25 00:00:00.0|2568 |COMPLETE       |\n",
      "|16 |2013-07-25 00:00:00.0|7276 |PENDING_PAYMENT|\n",
      "|17 |2013-07-25 00:00:00.0|2667 |COMPLETE       |\n",
      "|18 |2013-07-25 00:00:00.0|1205 |CLOSED         |\n",
      "|19 |2013-07-25 00:00:00.0|9488 |PENDING_PAYMENT|\n",
      "|20 |2013-07-25 00:00:00.0|9198 |PROCESSING     |\n",
      "+---+---------------------+-----+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('_c0', 'string'), ('_c1', 'string'), ('_c2', 'string'), ('_c3', 'string')]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.csv(output_path).show(truncate=False)\n",
    "spark.read.csv(output_path, sep='|').show(truncate=False)\n",
    "spark.read.csv(output_path, sep='|').dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "690ffee5-4310-43b3-a277-eeaa1cbe90f3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-23T16:58:13.098344Z",
     "iopub.status.busy": "2024-06-23T16:58:13.097305Z",
     "iopub.status.idle": "2024-06-23T16:58:13.224408Z",
     "shell.execute_reply": "2024-06-23T16:58:13.224091Z",
     "shell.execute_reply.started": "2024-06-23T16:58:13.098316Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+-----------------+------------+\n",
      "|order_id|order_date|order_customer_id|order_status|\n",
      "+--------+----------+-----------------+------------+\n",
      "|NULL    |NULL      |NULL             |NULL        |\n",
      "|NULL    |NULL      |NULL             |NULL        |\n",
      "|NULL    |NULL      |NULL             |NULL        |\n",
      "|NULL    |NULL      |NULL             |NULL        |\n",
      "|NULL    |NULL      |NULL             |NULL        |\n",
      "|NULL    |NULL      |NULL             |NULL        |\n",
      "|NULL    |NULL      |NULL             |NULL        |\n",
      "|NULL    |NULL      |NULL             |NULL        |\n",
      "|NULL    |NULL      |NULL             |NULL        |\n",
      "|NULL    |NULL      |NULL             |NULL        |\n",
      "|NULL    |NULL      |NULL             |NULL        |\n",
      "|NULL    |NULL      |NULL             |NULL        |\n",
      "|NULL    |NULL      |NULL             |NULL        |\n",
      "|NULL    |NULL      |NULL             |NULL        |\n",
      "|NULL    |NULL      |NULL             |NULL        |\n",
      "|NULL    |NULL      |NULL             |NULL        |\n",
      "|NULL    |NULL      |NULL             |NULL        |\n",
      "|NULL    |NULL      |NULL             |NULL        |\n",
      "|NULL    |NULL      |NULL             |NULL        |\n",
      "|NULL    |NULL      |NULL             |NULL        |\n",
      "+--------+----------+-----------------+------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+--------+-------------------+-----------------+---------------+\n",
      "|order_id|         order_date|order_customer_id|   order_status|\n",
      "+--------+-------------------+-----------------+---------------+\n",
      "|       1|2013-07-25 00:00:00|            11599|         CLOSED|\n",
      "|       2|2013-07-25 00:00:00|              256|PENDING_PAYMENT|\n",
      "|       3|2013-07-25 00:00:00|            12111|       COMPLETE|\n",
      "|       4|2013-07-25 00:00:00|             8827|         CLOSED|\n",
      "|       5|2013-07-25 00:00:00|            11318|       COMPLETE|\n",
      "|       6|2013-07-25 00:00:00|             7130|       COMPLETE|\n",
      "|       7|2013-07-25 00:00:00|             4530|       COMPLETE|\n",
      "|       8|2013-07-25 00:00:00|             2911|     PROCESSING|\n",
      "|       9|2013-07-25 00:00:00|             5657|PENDING_PAYMENT|\n",
      "|      10|2013-07-25 00:00:00|             5648|PENDING_PAYMENT|\n",
      "|      11|2013-07-25 00:00:00|              918| PAYMENT_REVIEW|\n",
      "|      12|2013-07-25 00:00:00|             1837|         CLOSED|\n",
      "|      13|2013-07-25 00:00:00|             9149|PENDING_PAYMENT|\n",
      "|      14|2013-07-25 00:00:00|             9842|     PROCESSING|\n",
      "|      15|2013-07-25 00:00:00|             2568|       COMPLETE|\n",
      "|      16|2013-07-25 00:00:00|             7276|PENDING_PAYMENT|\n",
      "|      17|2013-07-25 00:00:00|             2667|       COMPLETE|\n",
      "|      18|2013-07-25 00:00:00|             1205|         CLOSED|\n",
      "|      19|2013-07-25 00:00:00|             9488|PENDING_PAYMENT|\n",
      "|      20|2013-07-25 00:00:00|             9198|     PROCESSING|\n",
      "+--------+-------------------+-----------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('order_id', 'int'),\n",
       " ('order_date', 'timestamp'),\n",
       " ('order_customer_id', 'int'),\n",
       " ('order_status', 'string')]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_path = '/Users/adhoc/git/retail_db/csv/pipe/orders'\n",
    "schema = \"\"\"\n",
    "    order_id INT,\n",
    "    order_date TIMESTAMP,\n",
    "    order_customer_id INT,\n",
    "    order_status STRING\n",
    "\"\"\"\n",
    "spark.read.csv(input_path, schema=schema).show(truncate=False)\n",
    "spark.read.csv(input_path, sep='|', schema=schema).show()\n",
    "spark.read.csv(input_path, sep='|', schema=schema).dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6d2ef7-e97a-4945-83dc-bf5b76effb14",
   "metadata": {},
   "source": [
    "## Using options while writting Spark Data Frames into files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5d945750-9460-406b-b602-fa7bf6e337b4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-23T17:05:17.615410Z",
     "iopub.status.busy": "2024-06-23T17:05:17.614838Z",
     "iopub.status.idle": "2024-06-23T17:05:17.678982Z",
     "shell.execute_reply": "2024-06-23T17:05:17.678524Z",
     "shell.execute_reply.started": "2024-06-23T17:05:17.615378Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+-----------------+---------------+\n",
      "|order_id|         order_date|order_customer_id|   order_status|\n",
      "+--------+-------------------+-----------------+---------------+\n",
      "|       1|2013-07-25 00:00:00|            11599|         CLOSED|\n",
      "|       2|2013-07-25 00:00:00|              256|PENDING_PAYMENT|\n",
      "|       3|2013-07-25 00:00:00|            12111|       COMPLETE|\n",
      "|       4|2013-07-25 00:00:00|             8827|         CLOSED|\n",
      "|       5|2013-07-25 00:00:00|            11318|       COMPLETE|\n",
      "|       6|2013-07-25 00:00:00|             7130|       COMPLETE|\n",
      "|       7|2013-07-25 00:00:00|             4530|       COMPLETE|\n",
      "|       8|2013-07-25 00:00:00|             2911|     PROCESSING|\n",
      "|       9|2013-07-25 00:00:00|             5657|PENDING_PAYMENT|\n",
      "|      10|2013-07-25 00:00:00|             5648|PENDING_PAYMENT|\n",
      "|      11|2013-07-25 00:00:00|              918| PAYMENT_REVIEW|\n",
      "|      12|2013-07-25 00:00:00|             1837|         CLOSED|\n",
      "|      13|2013-07-25 00:00:00|             9149|PENDING_PAYMENT|\n",
      "|      14|2013-07-25 00:00:00|             9842|     PROCESSING|\n",
      "|      15|2013-07-25 00:00:00|             2568|       COMPLETE|\n",
      "|      16|2013-07-25 00:00:00|             7276|PENDING_PAYMENT|\n",
      "|      17|2013-07-25 00:00:00|             2667|       COMPLETE|\n",
      "|      18|2013-07-25 00:00:00|             1205|         CLOSED|\n",
      "|      19|2013-07-25 00:00:00|             9488|PENDING_PAYMENT|\n",
      "|      20|2013-07-25 00:00:00|             9198|     PROCESSING|\n",
      "+--------+-------------------+-----------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('order_id', 'int'),\n",
       " ('order_date', 'timestamp'),\n",
       " ('order_customer_id', 'int'),\n",
       " ('order_status', 'string')]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_path = '/Users/adhoc/git/retail_db/orders'\n",
    "output_path = '/Users/adhoc/git/retail_db/csv/pipe/orders'\n",
    "\n",
    "schema = \"\"\"\n",
    "    order_id INT,\n",
    "    order_date TIMESTAMP,\n",
    "    order_customer_id INT,\n",
    "    order_status STRING\n",
    "\"\"\"\n",
    "orders = spark.read.csv(input_path, schema=schema)\n",
    "orders.show()\n",
    "orders.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "6d590d5c-a249-485b-ac5c-161678495ea4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-23T17:13:59.604048Z",
     "iopub.status.busy": "2024-06-23T17:13:59.603191Z",
     "iopub.status.idle": "2024-06-23T17:14:00.066564Z",
     "shell.execute_reply": "2024-06-23T17:14:00.066268Z",
     "shell.execute_reply.started": "2024-06-23T17:13:59.603999Z"
    }
   },
   "outputs": [],
   "source": [
    "# there is no option for mode so it needs to be specified as mode function or csv argument\n",
    "# if mode is specified as option it will be ignored\n",
    "orders. \\\n",
    "    coalesce(1). \\\n",
    "    write. \\\n",
    "    mode('overwrite'). \\\n",
    "    option('sep', '|'). \\\n",
    "    option('compression', 'gzip'). \\\n",
    "    option('header', True). \\\n",
    "    csv(output_path)\n",
    "orders. \\\n",
    "    coalesce(1). \\\n",
    "    write. \\\n",
    "    mode('overwrite'). \\\n",
    "    option('header', True). \\\n",
    "    csv(output_path, sep='|', compression='gzip', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "954552a7-fd68-44af-a982-6c89960973e7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-23T17:14:01.702383Z",
     "iopub.status.busy": "2024-06-23T17:14:01.701860Z",
     "iopub.status.idle": "2024-06-23T17:14:02.119692Z",
     "shell.execute_reply": "2024-06-23T17:14:02.119419Z",
     "shell.execute_reply.started": "2024-06-23T17:14:01.702355Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+-----------------+---------------+\n",
      "|order_id|         order_date|order_customer_id|   order_status|\n",
      "+--------+-------------------+-----------------+---------------+\n",
      "|       1|2013-07-25 00:00:00|            11599|         CLOSED|\n",
      "|       2|2013-07-25 00:00:00|              256|PENDING_PAYMENT|\n",
      "|       3|2013-07-25 00:00:00|            12111|       COMPLETE|\n",
      "|       4|2013-07-25 00:00:00|             8827|         CLOSED|\n",
      "|       5|2013-07-25 00:00:00|            11318|       COMPLETE|\n",
      "|       6|2013-07-25 00:00:00|             7130|       COMPLETE|\n",
      "|       7|2013-07-25 00:00:00|             4530|       COMPLETE|\n",
      "|       8|2013-07-25 00:00:00|             2911|     PROCESSING|\n",
      "|       9|2013-07-25 00:00:00|             5657|PENDING_PAYMENT|\n",
      "|      10|2013-07-25 00:00:00|             5648|PENDING_PAYMENT|\n",
      "|      11|2013-07-25 00:00:00|              918| PAYMENT_REVIEW|\n",
      "|      12|2013-07-25 00:00:00|             1837|         CLOSED|\n",
      "|      13|2013-07-25 00:00:00|             9149|PENDING_PAYMENT|\n",
      "|      14|2013-07-25 00:00:00|             9842|     PROCESSING|\n",
      "|      15|2013-07-25 00:00:00|             2568|       COMPLETE|\n",
      "|      16|2013-07-25 00:00:00|             7276|PENDING_PAYMENT|\n",
      "|      17|2013-07-25 00:00:00|             2667|       COMPLETE|\n",
      "|      18|2013-07-25 00:00:00|             1205|         CLOSED|\n",
      "|      19|2013-07-25 00:00:00|             9488|PENDING_PAYMENT|\n",
      "|      20|2013-07-25 00:00:00|             9198|     PROCESSING|\n",
      "+--------+-------------------+-----------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('order_id', 'int'),\n",
       " ('order_date', 'timestamp'),\n",
       " ('order_customer_id', 'int'),\n",
       " ('order_status', 'string')]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.csv(output_path, sep='|', header=True, inferSchema=True).show()\n",
    "spark.read.csv(output_path, sep='|', header=True, inferSchema=True).dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "4663c2b5-628d-496a-848f-602b5095101b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-23T17:18:23.938305Z",
     "iopub.status.busy": "2024-06-23T17:18:23.937760Z",
     "iopub.status.idle": "2024-06-23T17:18:24.206758Z",
     "shell.execute_reply": "2024-06-23T17:18:24.206362Z",
     "shell.execute_reply.started": "2024-06-23T17:18:23.938277Z"
    }
   },
   "outputs": [],
   "source": [
    "orders. \\\n",
    "    coalesce(1). \\\n",
    "    write. \\\n",
    "    mode('overwrite'). \\\n",
    "    options(sep='|', compression='gzip', header=True). \\\n",
    "    csv(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a81e8218-16ba-4524-8cd7-5ecff5e6cb74",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-23T17:19:07.814604Z",
     "iopub.status.busy": "2024-06-23T17:19:07.813999Z",
     "iopub.status.idle": "2024-06-23T17:19:08.239250Z",
     "shell.execute_reply": "2024-06-23T17:19:08.238979Z",
     "shell.execute_reply.started": "2024-06-23T17:19:07.814571Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+-----------------+---------------+\n",
      "|order_id|         order_date|order_customer_id|   order_status|\n",
      "+--------+-------------------+-----------------+---------------+\n",
      "|       1|2013-07-25 00:00:00|            11599|         CLOSED|\n",
      "|       2|2013-07-25 00:00:00|              256|PENDING_PAYMENT|\n",
      "|       3|2013-07-25 00:00:00|            12111|       COMPLETE|\n",
      "|       4|2013-07-25 00:00:00|             8827|         CLOSED|\n",
      "|       5|2013-07-25 00:00:00|            11318|       COMPLETE|\n",
      "|       6|2013-07-25 00:00:00|             7130|       COMPLETE|\n",
      "|       7|2013-07-25 00:00:00|             4530|       COMPLETE|\n",
      "|       8|2013-07-25 00:00:00|             2911|     PROCESSING|\n",
      "|       9|2013-07-25 00:00:00|             5657|PENDING_PAYMENT|\n",
      "|      10|2013-07-25 00:00:00|             5648|PENDING_PAYMENT|\n",
      "|      11|2013-07-25 00:00:00|              918| PAYMENT_REVIEW|\n",
      "|      12|2013-07-25 00:00:00|             1837|         CLOSED|\n",
      "|      13|2013-07-25 00:00:00|             9149|PENDING_PAYMENT|\n",
      "|      14|2013-07-25 00:00:00|             9842|     PROCESSING|\n",
      "|      15|2013-07-25 00:00:00|             2568|       COMPLETE|\n",
      "|      16|2013-07-25 00:00:00|             7276|PENDING_PAYMENT|\n",
      "|      17|2013-07-25 00:00:00|             2667|       COMPLETE|\n",
      "|      18|2013-07-25 00:00:00|             1205|         CLOSED|\n",
      "|      19|2013-07-25 00:00:00|             9488|PENDING_PAYMENT|\n",
      "|      20|2013-07-25 00:00:00|             9198|     PROCESSING|\n",
      "+--------+-------------------+-----------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('order_id', 'int'),\n",
       " ('order_date', 'timestamp'),\n",
       " ('order_customer_id', 'int'),\n",
       " ('order_status', 'string')]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.csv(output_path, sep='|', header=True, inferSchema=True).show()\n",
    "spark.read.csv(output_path, sep='|', header=True, inferSchema=True).dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "8edb33f0-cd0c-48e9-a12d-f51db79e8c4d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-23T17:23:23.949524Z",
     "iopub.status.busy": "2024-06-23T17:23:23.948986Z",
     "iopub.status.idle": "2024-06-23T17:23:24.144575Z",
     "shell.execute_reply": "2024-06-23T17:23:24.144277Z",
     "shell.execute_reply.started": "2024-06-23T17:23:23.949498Z"
    }
   },
   "outputs": [],
   "source": [
    "options = {\n",
    "    'sep': '|',\n",
    "    'inferSchema': True,\n",
    "    'header': True,\n",
    "    'compression': 'snappy'\n",
    "}\n",
    "\n",
    "orders.coalesce(1).write.mode('overwrite').options(**options).csv(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "24369afe-2b34-497c-9958-70e922aaabe0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-23T17:23:26.123552Z",
     "iopub.status.busy": "2024-06-23T17:23:26.122777Z",
     "iopub.status.idle": "2024-06-23T17:23:26.539094Z",
     "shell.execute_reply": "2024-06-23T17:23:26.538819Z",
     "shell.execute_reply.started": "2024-06-23T17:23:26.123507Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+-----------------+---------------+\n",
      "|order_id|         order_date|order_customer_id|   order_status|\n",
      "+--------+-------------------+-----------------+---------------+\n",
      "|       1|2013-07-25 00:00:00|            11599|         CLOSED|\n",
      "|       2|2013-07-25 00:00:00|              256|PENDING_PAYMENT|\n",
      "|       3|2013-07-25 00:00:00|            12111|       COMPLETE|\n",
      "|       4|2013-07-25 00:00:00|             8827|         CLOSED|\n",
      "|       5|2013-07-25 00:00:00|            11318|       COMPLETE|\n",
      "|       6|2013-07-25 00:00:00|             7130|       COMPLETE|\n",
      "|       7|2013-07-25 00:00:00|             4530|       COMPLETE|\n",
      "|       8|2013-07-25 00:00:00|             2911|     PROCESSING|\n",
      "|       9|2013-07-25 00:00:00|             5657|PENDING_PAYMENT|\n",
      "|      10|2013-07-25 00:00:00|             5648|PENDING_PAYMENT|\n",
      "|      11|2013-07-25 00:00:00|              918| PAYMENT_REVIEW|\n",
      "|      12|2013-07-25 00:00:00|             1837|         CLOSED|\n",
      "|      13|2013-07-25 00:00:00|             9149|PENDING_PAYMENT|\n",
      "|      14|2013-07-25 00:00:00|             9842|     PROCESSING|\n",
      "|      15|2013-07-25 00:00:00|             2568|       COMPLETE|\n",
      "|      16|2013-07-25 00:00:00|             7276|PENDING_PAYMENT|\n",
      "|      17|2013-07-25 00:00:00|             2667|       COMPLETE|\n",
      "|      18|2013-07-25 00:00:00|             1205|         CLOSED|\n",
      "|      19|2013-07-25 00:00:00|             9488|PENDING_PAYMENT|\n",
      "|      20|2013-07-25 00:00:00|             9198|     PROCESSING|\n",
      "+--------+-------------------+-----------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('order_id', 'int'),\n",
       " ('order_date', 'timestamp'),\n",
       " ('order_customer_id', 'int'),\n",
       " ('order_status', 'string')]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.csv(output_path, sep='|', header=True, inferSchema=True).show()\n",
    "spark.read.csv(output_path, sep='|', header=True, inferSchema=True).dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cfe57ef-896e-4034-808f-2a2ca8f296fc",
   "metadata": {},
   "source": [
    "## Write JSON files from Spark Data Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "17639500-3296-40f1-a34c-2c5156c88498",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-23T17:26:06.145870Z",
     "iopub.status.busy": "2024-06-23T17:26:06.145298Z",
     "iopub.status.idle": "2024-06-23T17:26:06.437944Z",
     "shell.execute_reply": "2024-06-23T17:26:06.437705Z",
     "shell.execute_reply.started": "2024-06-23T17:26:06.145839Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- course_id: long (nullable = true)\n",
      " |-- course_title: string (nullable = true)\n",
      " |-- course_published_dt: date (nullable = true)\n",
      " |-- is_active: boolean (nullable = true)\n",
      " |-- last_updated_ts: timestamp (nullable = true)\n",
      "\n",
      "+---------+--------------------+-------------------+---------+-------------------+\n",
      "|course_id|        course_title|course_published_dt|is_active|    last_updated_ts|\n",
      "+---------+--------------------+-------------------+---------+-------------------+\n",
      "|        1|    Mastering Python|         2021-01-14|     true|2021-02-18 16:57:25|\n",
      "|        2|Data Engineering ...|         2021-02-10|     true|2021-03-05 12:07:33|\n",
      "|        3|   Mastering Pyspark|         2021-01-07|     true|2021-04-06 10:05:42|\n",
      "|        4|      AWS Essentials|         2021-03-19|    false|2021-04-10 02:25:36|\n",
      "|        5|          Docker 101|         2021-02-28|     true|2021-03-21 07:18:52|\n",
      "+---------+--------------------+-------------------+---------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "from pyspark.sql import Row\n",
    "\n",
    "courses = [\n",
    "    {\n",
    "        'course_id': 1,\n",
    "        'course_title': 'Mastering Python',\n",
    "        'course_published_dt': datetime.date(2021, 1, 14),\n",
    "        'is_active': True,\n",
    "        'last_updated_ts': datetime.datetime(2021, 2, 18, 16, 57, 25)\n",
    "    },\n",
    "    {\n",
    "        'course_id': 2,\n",
    "        'course_title': 'Data Engineering Essentials',\n",
    "        'course_published_dt': datetime.date(2021, 2, 10),\n",
    "        'is_active': True,\n",
    "        'last_updated_ts': datetime.datetime(2021, 3, 5, 12, 7, 33)\n",
    "    },\n",
    "    {\n",
    "        'course_id': 3,\n",
    "        'course_title': 'Mastering Pyspark',\n",
    "        'course_published_dt': datetime.date(2021, 1, 7),\n",
    "        'is_active': True,\n",
    "        'last_updated_ts': datetime.datetime(2021, 4, 6, 10, 5, 42)\n",
    "    },\n",
    "    {\n",
    "        'course_id': 4,\n",
    "        'course_title': 'AWS Essentials',\n",
    "        'course_published_dt': datetime.date(2021, 3, 19),\n",
    "        'is_active': False,\n",
    "        'last_updated_ts': datetime.datetime(2021, 4, 10, 2, 25, 36)\n",
    "    },\n",
    "    {\n",
    "        'course_id': 5,\n",
    "        'course_title': 'Docker 101',\n",
    "        'course_published_dt': datetime.date(2021, 2, 28),\n",
    "        'is_active': True,\n",
    "        'last_updated_ts': datetime.datetime(2021, 3, 21, 7, 18, 52)\n",
    "    }\n",
    "]\n",
    "\n",
    "#both work\n",
    "courses_df = spark.createDataFrame([Row(**course) for course in courses]) # maintained column order\n",
    "#courses_df = spark.createDataFrame(courses)\n",
    "courses_df.printSchema()\n",
    "courses_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "a9a710ae-a8d8-494f-988b-203266b3df5c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-23T17:29:02.243477Z",
     "iopub.status.busy": "2024-06-23T17:29:02.242740Z",
     "iopub.status.idle": "2024-06-23T17:29:02.794632Z",
     "shell.execute_reply": "2024-06-23T17:29:02.794350Z",
     "shell.execute_reply.started": "2024-06-23T17:29:02.243434Z"
    }
   },
   "outputs": [],
   "source": [
    "json_path = '/Users/adhoc/git/retail_db/json/courses'\n",
    "courses_df.coalesce(1).write.json(json_path, mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "aa7a43f2-9cd8-41f0-b318-a7566d627c2d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-23T17:30:22.913793Z",
     "iopub.status.busy": "2024-06-23T17:30:22.912965Z",
     "iopub.status.idle": "2024-06-23T17:30:23.029729Z",
     "shell.execute_reply": "2024-06-23T17:30:23.029448Z",
     "shell.execute_reply.started": "2024-06-23T17:30:22.913763Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------------+--------------------+---------+--------------------+\n",
      "|course_id|course_published_dt|        course_title|is_active|     last_updated_ts|\n",
      "+---------+-------------------+--------------------+---------+--------------------+\n",
      "|        1|         2021-01-14|    Mastering Python|     true|2021-02-18T16:57:...|\n",
      "|        2|         2021-02-10|Data Engineering ...|     true|2021-03-05T12:07:...|\n",
      "|        3|         2021-01-07|   Mastering Pyspark|     true|2021-04-06T10:05:...|\n",
      "|        4|         2021-03-19|      AWS Essentials|    false|2021-04-10T02:25:...|\n",
      "|        5|         2021-02-28|          Docker 101|     true|2021-03-21T07:18:...|\n",
      "+---------+-------------------+--------------------+---------+--------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([('course_id', 'bigint'),\n",
       "  ('course_published_dt', 'string'),\n",
       "  ('course_title', 'string'),\n",
       "  ('is_active', 'boolean'),\n",
       "  ('last_updated_ts', 'string')],\n",
       " ['file:///Users/adhoc/git/retail_db/json/courses/part-00000-41eec3cf-3911-4850-964d-4a5aba5f810a-c000.json'])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.json(json_path).show()\n",
    "spark.read.json(json_path).dtypes, spark.read.json(json_path).inputFiles()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "44ebb4d9-2ec1-4424-889e-186992a1ca94",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-23T17:33:45.475147Z",
     "iopub.status.busy": "2024-06-23T17:33:45.474115Z",
     "iopub.status.idle": "2024-06-23T17:33:46.046038Z",
     "shell.execute_reply": "2024-06-23T17:33:46.045718Z",
     "shell.execute_reply.started": "2024-06-23T17:33:45.475115Z"
    }
   },
   "outputs": [],
   "source": [
    "courses_df.coalesce(1).write.format('json').save(json_path, mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "f93f4e10-929a-4d9d-87a7-749fcf288510",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-23T17:33:56.610075Z",
     "iopub.status.busy": "2024-06-23T17:33:56.609531Z",
     "iopub.status.idle": "2024-06-23T17:33:56.735968Z",
     "shell.execute_reply": "2024-06-23T17:33:56.735730Z",
     "shell.execute_reply.started": "2024-06-23T17:33:56.610042Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------------+--------------------+---------+--------------------+\n",
      "|course_id|course_published_dt|        course_title|is_active|     last_updated_ts|\n",
      "+---------+-------------------+--------------------+---------+--------------------+\n",
      "|        1|         2021-01-14|    Mastering Python|     true|2021-02-18T16:57:...|\n",
      "|        2|         2021-02-10|Data Engineering ...|     true|2021-03-05T12:07:...|\n",
      "|        3|         2021-01-07|   Mastering Pyspark|     true|2021-04-06T10:05:...|\n",
      "|        4|         2021-03-19|      AWS Essentials|    false|2021-04-10T02:25:...|\n",
      "|        5|         2021-02-28|          Docker 101|     true|2021-03-21T07:18:...|\n",
      "+---------+-------------------+--------------------+---------+--------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([('course_id', 'bigint'),\n",
       "  ('course_published_dt', 'string'),\n",
       "  ('course_title', 'string'),\n",
       "  ('is_active', 'boolean'),\n",
       "  ('last_updated_ts', 'string')],\n",
       " ['file:///Users/adhoc/git/retail_db/json/courses/part-00000-1d122581-e16a-4d94-a6e5-0c87f5d8bf5e-c000.json'])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.json(json_path).show()\n",
    "spark.read.json(json_path).dtypes, spark.read.json(json_path).inputFiles()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e9de1c-00d7-4951-b634-5ca6eef2b1ff",
   "metadata": {},
   "source": [
    "## Compression while writting JSON files from Spark Data frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "33ace7d4-0c78-4d26-be54-e27e2bad8a92",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-23T17:39:01.098631Z",
     "iopub.status.busy": "2024-06-23T17:39:01.098092Z",
     "iopub.status.idle": "2024-06-23T17:39:01.663634Z",
     "shell.execute_reply": "2024-06-23T17:39:01.663224Z",
     "shell.execute_reply.started": "2024-06-23T17:39:01.098601Z"
    }
   },
   "outputs": [],
   "source": [
    "courses_df.coalesce(1).write.json(json_path, mode='overwrite', compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "4d37c5de-a400-4c29-b53e-a25279c9759b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-23T17:39:03.397824Z",
     "iopub.status.busy": "2024-06-23T17:39:03.396982Z",
     "iopub.status.idle": "2024-06-23T17:39:03.524156Z",
     "shell.execute_reply": "2024-06-23T17:39:03.523805Z",
     "shell.execute_reply.started": "2024-06-23T17:39:03.397773Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------------+--------------------+---------+--------------------+\n",
      "|course_id|course_published_dt|        course_title|is_active|     last_updated_ts|\n",
      "+---------+-------------------+--------------------+---------+--------------------+\n",
      "|        1|         2021-01-14|    Mastering Python|     true|2021-02-18T16:57:...|\n",
      "|        2|         2021-02-10|Data Engineering ...|     true|2021-03-05T12:07:...|\n",
      "|        3|         2021-01-07|   Mastering Pyspark|     true|2021-04-06T10:05:...|\n",
      "|        4|         2021-03-19|      AWS Essentials|    false|2021-04-10T02:25:...|\n",
      "|        5|         2021-02-28|          Docker 101|     true|2021-03-21T07:18:...|\n",
      "+---------+-------------------+--------------------+---------+--------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([('course_id', 'bigint'),\n",
       "  ('course_published_dt', 'string'),\n",
       "  ('course_title', 'string'),\n",
       "  ('is_active', 'boolean'),\n",
       "  ('last_updated_ts', 'string')],\n",
       " ['file:///Users/adhoc/git/retail_db/json/courses/part-00000-8bc8af52-0262-4ebb-8c25-054edff0ffe1-c000.json.gz'])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.json(json_path).show()\n",
    "spark.read.json(json_path).dtypes, spark.read.json(json_path).inputFiles()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "12811d31-ca40-40ff-af37-21272b79f02c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-23T17:40:17.656373Z",
     "iopub.status.busy": "2024-06-23T17:40:17.655665Z",
     "iopub.status.idle": "2024-06-23T17:40:18.211337Z",
     "shell.execute_reply": "2024-06-23T17:40:18.210979Z",
     "shell.execute_reply.started": "2024-06-23T17:40:17.656341Z"
    }
   },
   "outputs": [],
   "source": [
    "courses_df.coalesce(1).write.format('json').save(json_path, mode='overwrite', compression='snappy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "de786c03-e024-4701-8b80-88e4e1c60b2a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-23T17:40:19.584688Z",
     "iopub.status.busy": "2024-06-23T17:40:19.584259Z",
     "iopub.status.idle": "2024-06-23T17:40:19.723534Z",
     "shell.execute_reply": "2024-06-23T17:40:19.723314Z",
     "shell.execute_reply.started": "2024-06-23T17:40:19.584659Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------------+--------------------+---------+--------------------+\n",
      "|course_id|course_published_dt|        course_title|is_active|     last_updated_ts|\n",
      "+---------+-------------------+--------------------+---------+--------------------+\n",
      "|        1|         2021-01-14|    Mastering Python|     true|2021-02-18T16:57:...|\n",
      "|        2|         2021-02-10|Data Engineering ...|     true|2021-03-05T12:07:...|\n",
      "|        3|         2021-01-07|   Mastering Pyspark|     true|2021-04-06T10:05:...|\n",
      "|        4|         2021-03-19|      AWS Essentials|    false|2021-04-10T02:25:...|\n",
      "|        5|         2021-02-28|          Docker 101|     true|2021-03-21T07:18:...|\n",
      "+---------+-------------------+--------------------+---------+--------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([('course_id', 'bigint'),\n",
       "  ('course_published_dt', 'string'),\n",
       "  ('course_title', 'string'),\n",
       "  ('is_active', 'boolean'),\n",
       "  ('last_updated_ts', 'string')],\n",
       " ['file:///Users/adhoc/git/retail_db/json/courses/part-00000-936399bb-c7e8-460b-94fb-e996670e5138-c000.json.snappy'])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.json(json_path).show()\n",
    "spark.read.json(json_path).dtypes, spark.read.json(json_path).inputFiles()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c09ccd6-2c78-4142-ae8f-4d1f2975c456",
   "metadata": {},
   "source": [
    "## Writting Spark Data Frames into parquet files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "be8b12df-cfb9-4424-a8c8-cbc959293d16",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-23T18:57:30.278863Z",
     "iopub.status.busy": "2024-06-23T18:57:30.278336Z",
     "iopub.status.idle": "2024-06-23T18:57:30.843382Z",
     "shell.execute_reply": "2024-06-23T18:57:30.843050Z",
     "shell.execute_reply.started": "2024-06-23T18:57:30.278835Z"
    }
   },
   "outputs": [],
   "source": [
    "# lzo, lz4, snappy (default), gzip, zstd, brotli, uncompressed \n",
    "\n",
    "parquet_path = '/Users/adhoc/git/retail_db/parquet/courses'\n",
    "courses_df.coalesce(1).write.parquet(parquet_path, mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "4cf74e67-ff41-4bc0-8771-46601d362cf6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-23T18:57:32.864574Z",
     "iopub.status.busy": "2024-06-23T18:57:32.863929Z",
     "iopub.status.idle": "2024-06-23T18:57:33.016998Z",
     "shell.execute_reply": "2024-06-23T18:57:33.016759Z",
     "shell.execute_reply.started": "2024-06-23T18:57:32.864548Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+-------------------+---------+-------------------+\n",
      "|course_id|        course_title|course_published_dt|is_active|    last_updated_ts|\n",
      "+---------+--------------------+-------------------+---------+-------------------+\n",
      "|        1|    Mastering Python|         2021-01-14|     true|2021-02-18 16:57:25|\n",
      "|        2|Data Engineering ...|         2021-02-10|     true|2021-03-05 12:07:33|\n",
      "|        3|   Mastering Pyspark|         2021-01-07|     true|2021-04-06 10:05:42|\n",
      "|        4|      AWS Essentials|         2021-03-19|    false|2021-04-10 02:25:36|\n",
      "|        5|          Docker 101|         2021-02-28|     true|2021-03-21 07:18:52|\n",
      "+---------+--------------------+-------------------+---------+-------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([('course_id', 'bigint'),\n",
       "  ('course_title', 'string'),\n",
       "  ('course_published_dt', 'date'),\n",
       "  ('is_active', 'boolean'),\n",
       "  ('last_updated_ts', 'timestamp')],\n",
       " ['file:///Users/adhoc/git/retail_db/parquet/courses/part-00000-8e22f48f-0680-437c-a72a-1adfd5274dec-c000.snappy.parquet'])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.parquet(parquet_path).show()\n",
    "spark.read.parquet(parquet_path).dtypes, spark.read.parquet(parquet_path).inputFiles()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "0a8b825b-22a9-49d6-909b-2f0c3776ea15",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-23T19:03:58.154106Z",
     "iopub.status.busy": "2024-06-23T19:03:58.153338Z",
     "iopub.status.idle": "2024-06-23T19:03:58.714499Z",
     "shell.execute_reply": "2024-06-23T19:03:58.714220Z",
     "shell.execute_reply.started": "2024-06-23T19:03:58.154060Z"
    }
   },
   "outputs": [],
   "source": [
    "courses_df.coalesce(1).write.format('parquet').save(parquet_path, mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "b5a6f478-f04a-4795-b8bb-744370cf6a41",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-23T19:04:06.959524Z",
     "iopub.status.busy": "2024-06-23T19:04:06.958883Z",
     "iopub.status.idle": "2024-06-23T19:04:07.094445Z",
     "shell.execute_reply": "2024-06-23T19:04:07.094181Z",
     "shell.execute_reply.started": "2024-06-23T19:04:06.959494Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+-------------------+---------+-------------------+\n",
      "|course_id|        course_title|course_published_dt|is_active|    last_updated_ts|\n",
      "+---------+--------------------+-------------------+---------+-------------------+\n",
      "|        1|    Mastering Python|         2021-01-14|     true|2021-02-18 16:57:25|\n",
      "|        2|Data Engineering ...|         2021-02-10|     true|2021-03-05 12:07:33|\n",
      "|        3|   Mastering Pyspark|         2021-01-07|     true|2021-04-06 10:05:42|\n",
      "|        4|      AWS Essentials|         2021-03-19|    false|2021-04-10 02:25:36|\n",
      "|        5|          Docker 101|         2021-02-28|     true|2021-03-21 07:18:52|\n",
      "+---------+--------------------+-------------------+---------+-------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([('course_id', 'bigint'),\n",
       "  ('course_title', 'string'),\n",
       "  ('course_published_dt', 'date'),\n",
       "  ('is_active', 'boolean'),\n",
       "  ('last_updated_ts', 'timestamp')],\n",
       " ['file:///Users/adhoc/git/retail_db/parquet/courses/part-00000-fd60c8e3-4286-4e7e-876e-390a045492e0-c000.snappy.parquet'])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.parquet(parquet_path).show()\n",
    "spark.read.parquet(parquet_path).dtypes, spark.read.parquet(parquet_path).inputFiles()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622f2d68-2e7d-406a-83c0-6563972f428e",
   "metadata": {},
   "source": [
    "## Compression while writtng Spark Data Frames into parquet files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "3cca8161-41c2-4c4b-be81-2fe1e8ccef7f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-23T20:13:55.529916Z",
     "iopub.status.busy": "2024-06-23T20:13:55.529172Z",
     "iopub.status.idle": "2024-06-23T20:13:55.543658Z",
     "shell.execute_reply": "2024-06-23T20:13:55.542970Z",
     "shell.execute_reply.started": "2024-06-23T20:13:55.529871Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'snappy'"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.get('spark.sql.parquet.compression.codec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "5208992e-7fd8-4d5a-9f22-426a5339a7b0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-23T20:16:36.553655Z",
     "iopub.status.busy": "2024-06-23T20:16:36.553096Z",
     "iopub.status.idle": "2024-06-23T20:16:37.140388Z",
     "shell.execute_reply": "2024-06-23T20:16:37.140087Z",
     "shell.execute_reply.started": "2024-06-23T20:16:36.553626Z"
    }
   },
   "outputs": [],
   "source": [
    "courses_df.coalesce(1).write.parquet(parquet_path, mode='overwrite', compression='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "7b92d661-e11b-41a9-96e7-5dc7230bc657",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-23T20:16:49.292745Z",
     "iopub.status.busy": "2024-06-23T20:16:49.292115Z",
     "iopub.status.idle": "2024-06-23T20:16:49.513959Z",
     "shell.execute_reply": "2024-06-23T20:16:49.513727Z",
     "shell.execute_reply.started": "2024-06-23T20:16:49.292714Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+-------------------+---------+-------------------+\n",
      "|course_id|        course_title|course_published_dt|is_active|    last_updated_ts|\n",
      "+---------+--------------------+-------------------+---------+-------------------+\n",
      "|        1|    Mastering Python|         2021-01-14|     true|2021-02-18 16:57:25|\n",
      "|        2|Data Engineering ...|         2021-02-10|     true|2021-03-05 12:07:33|\n",
      "|        3|   Mastering Pyspark|         2021-01-07|     true|2021-04-06 10:05:42|\n",
      "|        4|      AWS Essentials|         2021-03-19|    false|2021-04-10 02:25:36|\n",
      "|        5|          Docker 101|         2021-02-28|     true|2021-03-21 07:18:52|\n",
      "+---------+--------------------+-------------------+---------+-------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([('course_id', 'bigint'),\n",
       "  ('course_title', 'string'),\n",
       "  ('course_published_dt', 'date'),\n",
       "  ('is_active', 'boolean'),\n",
       "  ('last_updated_ts', 'timestamp')],\n",
       " ['file:///Users/adhoc/git/retail_db/parquet/courses/part-00000-168a32ca-03b7-423b-9c97-ab816594b557-c000.parquet'])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.parquet(parquet_path).show()\n",
    "spark.read.parquet(parquet_path).dtypes, spark.read.parquet(parquet_path).inputFiles()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "641a52ac-2b0a-42c7-985b-ab35f3fe9ebb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-23T20:18:27.886683Z",
     "iopub.status.busy": "2024-06-23T20:18:27.885804Z",
     "iopub.status.idle": "2024-06-23T20:18:28.476565Z",
     "shell.execute_reply": "2024-06-23T20:18:28.476272Z",
     "shell.execute_reply.started": "2024-06-23T20:18:27.886655Z"
    }
   },
   "outputs": [],
   "source": [
    "courses_df.coalesce(1).write.format('parquet').save(parquet_path, mode='overwrite', compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "8b1b8c04-e825-492e-aa96-6af45e72687e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-23T20:20:25.587558Z",
     "iopub.status.busy": "2024-06-23T20:20:25.586046Z",
     "iopub.status.idle": "2024-06-23T20:20:25.729959Z",
     "shell.execute_reply": "2024-06-23T20:20:25.729702Z",
     "shell.execute_reply.started": "2024-06-23T20:20:25.587518Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+-------------------+---------+-------------------+\n",
      "|course_id|        course_title|course_published_dt|is_active|    last_updated_ts|\n",
      "+---------+--------------------+-------------------+---------+-------------------+\n",
      "|        1|    Mastering Python|         2021-01-14|     true|2021-02-18 16:57:25|\n",
      "|        2|Data Engineering ...|         2021-02-10|     true|2021-03-05 12:07:33|\n",
      "|        3|   Mastering Pyspark|         2021-01-07|     true|2021-04-06 10:05:42|\n",
      "|        4|      AWS Essentials|         2021-03-19|    false|2021-04-10 02:25:36|\n",
      "|        5|          Docker 101|         2021-02-28|     true|2021-03-21 07:18:52|\n",
      "+---------+--------------------+-------------------+---------+-------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([('course_id', 'bigint'),\n",
       "  ('course_title', 'string'),\n",
       "  ('course_published_dt', 'date'),\n",
       "  ('is_active', 'boolean'),\n",
       "  ('last_updated_ts', 'timestamp')],\n",
       " ['file:///Users/adhoc/git/retail_db/parquet/courses/part-00000-2c4e4cc1-4f83-4918-9479-44caad90d3d2-c000.parquet'])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.parquet(parquet_path).show()\n",
    "spark.read.parquet(parquet_path).dtypes, spark.read.parquet(parquet_path).inputFiles()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "b9a7b295-6db4-471e-934b-d2759ec66bdb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-23T20:19:49.981569Z",
     "iopub.status.busy": "2024-06-23T20:19:49.981006Z",
     "iopub.status.idle": "2024-06-23T20:19:49.991443Z",
     "shell.execute_reply": "2024-06-23T20:19:49.990711Z",
     "shell.execute_reply.started": "2024-06-23T20:19:49.981539Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'None'"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.set('spark.sql.parquet.compression.codec', 'None')\n",
    "spark.conf.get('spark.sql.parquet.compression.codec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "f13b0b70-6534-4468-9544-5ef03f9fbe87",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-23T20:20:14.142486Z",
     "iopub.status.busy": "2024-06-23T20:20:14.141928Z",
     "iopub.status.idle": "2024-06-23T20:20:14.729507Z",
     "shell.execute_reply": "2024-06-23T20:20:14.729188Z",
     "shell.execute_reply.started": "2024-06-23T20:20:14.142458Z"
    }
   },
   "outputs": [],
   "source": [
    "courses_df.coalesce(1).write.format('parquet').save(parquet_path, mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "bc66303f-3587-4b10-8631-ac76ec55fc26",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-23T20:20:28.280505Z",
     "iopub.status.busy": "2024-06-23T20:20:28.279908Z",
     "iopub.status.idle": "2024-06-23T20:20:28.424107Z",
     "shell.execute_reply": "2024-06-23T20:20:28.423822Z",
     "shell.execute_reply.started": "2024-06-23T20:20:28.280475Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+-------------------+---------+-------------------+\n",
      "|course_id|        course_title|course_published_dt|is_active|    last_updated_ts|\n",
      "+---------+--------------------+-------------------+---------+-------------------+\n",
      "|        1|    Mastering Python|         2021-01-14|     true|2021-02-18 16:57:25|\n",
      "|        2|Data Engineering ...|         2021-02-10|     true|2021-03-05 12:07:33|\n",
      "|        3|   Mastering Pyspark|         2021-01-07|     true|2021-04-06 10:05:42|\n",
      "|        4|      AWS Essentials|         2021-03-19|    false|2021-04-10 02:25:36|\n",
      "|        5|          Docker 101|         2021-02-28|     true|2021-03-21 07:18:52|\n",
      "+---------+--------------------+-------------------+---------+-------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([('course_id', 'bigint'),\n",
       "  ('course_title', 'string'),\n",
       "  ('course_published_dt', 'date'),\n",
       "  ('is_active', 'boolean'),\n",
       "  ('last_updated_ts', 'timestamp')],\n",
       " ['file:///Users/adhoc/git/retail_db/parquet/courses/part-00000-2c4e4cc1-4f83-4918-9479-44caad90d3d2-c000.parquet'])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.parquet(parquet_path).show()\n",
    "spark.read.parquet(parquet_path).dtypes, spark.read.parquet(parquet_path).inputFiles()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "bcb76e7f-4ad4-4a4b-8a04-11a751b1bab0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-23T20:20:49.602355Z",
     "iopub.status.busy": "2024-06-23T20:20:49.601506Z",
     "iopub.status.idle": "2024-06-23T20:20:49.619355Z",
     "shell.execute_reply": "2024-06-23T20:20:49.616721Z",
     "shell.execute_reply.started": "2024-06-23T20:20:49.602324Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'snappy'"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.set('spark.sql.parquet.compression.codec', 'snappy')\n",
    "spark.conf.get('spark.sql.parquet.compression.codec')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc2a63e-ce1f-40fd-84fd-19881a3c96a4",
   "metadata": {},
   "source": [
    "## Different modes for writting parquet files from Spark Data Frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "f1e7521d-9645-464c-88f4-acfe05b6f206",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-23T20:29:35.136577Z",
     "iopub.status.busy": "2024-06-23T20:29:35.136026Z",
     "iopub.status.idle": "2024-06-23T20:29:37.213823Z",
     "shell.execute_reply": "2024-06-23T20:29:37.213497Z",
     "shell.execute_reply.started": "2024-06-23T20:29:35.136548Z"
    }
   },
   "outputs": [],
   "source": [
    "# parquet modes: append, overwrite, error (default), ignore\n",
    "courses_df.coalesce(1).write.mode('overwrite').parquet(parquet_path)\n",
    "courses_df.coalesce(1).write.parquet(parquet_path, mode='overwrite')\n",
    "courses_df.coalesce(1).write.mode('overwrite').format('parquet').save(parquet_path)\n",
    "courses_df.coalesce(1).write.format('parquet').save(parquet_path, mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "fb435f7e-1834-41bc-b60a-7ebf2a5ed52c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-23T20:30:34.203371Z",
     "iopub.status.busy": "2024-06-23T20:30:34.203046Z",
     "iopub.status.idle": "2024-06-23T20:30:34.601309Z",
     "shell.execute_reply": "2024-06-23T20:30:34.600992Z",
     "shell.execute_reply.started": "2024-06-23T20:30:34.203352Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+-------------------+---------+-------------------+\n",
      "|course_id|        course_title|course_published_dt|is_active|    last_updated_ts|\n",
      "+---------+--------------------+-------------------+---------+-------------------+\n",
      "|        1|    Mastering Python|         2021-01-14|     true|2021-02-18 16:57:25|\n",
      "|        2|Data Engineering ...|         2021-02-10|     true|2021-03-05 12:07:33|\n",
      "|        3|   Mastering Pyspark|         2021-01-07|     true|2021-04-06 10:05:42|\n",
      "|        4|      AWS Essentials|         2021-03-19|    false|2021-04-10 02:25:36|\n",
      "|        5|          Docker 101|         2021-02-28|     true|2021-03-21 07:18:52|\n",
      "+---------+--------------------+-------------------+---------+-------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([('course_id', 'bigint'),\n",
       "  ('course_title', 'string'),\n",
       "  ('course_published_dt', 'date'),\n",
       "  ('is_active', 'boolean'),\n",
       "  ('last_updated_ts', 'timestamp')],\n",
       " ['file:///Users/adhoc/git/retail_db/parquet/courses/part-00000-e13cf42d-e65a-4bbd-badf-5a2126a2295e-c000.snappy.parquet'],\n",
       " 5)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.parquet(parquet_path).show()\n",
    "spark.read.parquet(parquet_path).dtypes, \\\n",
    "spark.read.parquet(parquet_path).inputFiles(), \\\n",
    "spark.read.parquet(parquet_path).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2fba2d-d4cf-4470-84f8-a5a6413da875",
   "metadata": {},
   "source": [
    "## Coalesce and repartitioning of Spark Data Frames\n",
    "* `coalesce` and `repartitioning` are function defined on top of `DataFrame`\n",
    "* `coalesce` is typically used to **reduce number of partitions** to deal with as part of downstream processes\n",
    "* `repartition` is typically used to reshuffle the **data to higher or lower number** of partitions to deal with as part of downstream processes\n",
    "----------\n",
    "* `repartition` incurs **shuffling** and it takes time as the data has to be shuffled to newer number of partitions\n",
    "* you can repartition the data frame on the specified columns\n",
    "* `coalesce` does not incur shuffling\n",
    "* `coalesce` is used quite often before writting the data fewer number of files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "f6d041b1-90d5-45c4-b2e3-70350cafb11c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-23T20:46:10.114902Z",
     "iopub.status.busy": "2024-06-23T20:46:10.114053Z",
     "iopub.status.idle": "2024-06-23T20:46:10.129054Z",
     "shell.execute_reply": "2024-06-23T20:46:10.128379Z",
     "shell.execute_reply.started": "2024-06-23T20:46:10.114871Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "courses_df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "9fd237f0-cc4f-4237-bc3b-c808fae98998",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-23T20:48:12.796094Z",
     "iopub.status.busy": "2024-06-23T20:48:12.795516Z",
     "iopub.status.idle": "2024-06-23T20:48:12.829814Z",
     "shell.execute_reply": "2024-06-23T20:48:12.829396Z",
     "shell.execute_reply.started": "2024-06-23T20:48:12.796062Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 10)"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# you cannot increase number of partitions with coalesce (no shuffling)\n",
    "# if the number is higher than existing number of partitions it is ignored\n",
    "courses_df.coalesce(5).rdd.getNumPartitions(), \\\n",
    "courses_df.coalesce(15).rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "d4c4d778-22c4-4f26-b6b3-35a5733ea0d6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-23T20:52:33.827262Z",
     "iopub.status.busy": "2024-06-23T20:52:33.826635Z",
     "iopub.status.idle": "2024-06-23T20:52:34.153170Z",
     "shell.execute_reply": "2024-06-23T20:52:34.152874Z",
     "shell.execute_reply.started": "2024-06-23T20:52:33.827205Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 15, 15)"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## repartition incurs shuffling\n",
    "# number of unique values for the repartitioning columns matters\n",
    "courses_df.repartition(5).rdd.getNumPartitions(), \\\n",
    "courses_df.repartition(15).rdd.getNumPartitions(), \\\n",
    "courses_df.repartition(15, 'course_published_dt').rdd.getNumPartitions()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

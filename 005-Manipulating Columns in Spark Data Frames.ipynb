{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df313673-2471-4164-b434-fc09ea4b2c1e",
   "metadata": {},
   "source": [
    "## Processing Column Data\n",
    "\n",
    "As part of this module we will explore the functions available under `pyspark.sql.functions` to derive new values from existing column values with in a Data Frame.\n",
    "\n",
    "* Pre-defined Functions\n",
    "* Create Dummy Data Frame\n",
    "* Categories of Functions\n",
    "* Special Functions - `col` and `lit`\n",
    "* String Manipulation Functions - 1\n",
    "* String Manipulation Functions - 2\n",
    "* Date and Time Overview\n",
    "* Date and Time Arithmetic\n",
    "* Date and Time - `trunc` and `date_trunc`\n",
    "* Date and Time - Extracting Information\n",
    "* Dealing with Unix Timestamp\n",
    "* Example - Word Count\n",
    "* Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a78b1a0-d3e2-4fdc-8e38-7a18eef6792b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-12T21:48:01.418496Z",
     "iopub.status.busy": "2024-06-12T21:48:01.417913Z",
     "iopub.status.idle": "2024-06-12T21:48:08.477199Z",
     "shell.execute_reply": "2024-06-12T21:48:08.476849Z",
     "shell.execute_reply.started": "2024-06-12T21:48:01.418463Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/06/12 23:48:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession.builder.appName('instance').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8425b021-3d06-4439-86ee-c2c134feddc5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-30T22:12:18.362376Z",
     "iopub.status.busy": "2024-05-30T22:12:18.361603Z",
     "iopub.status.idle": "2024-05-30T22:12:18.488489Z",
     "shell.execute_reply": "2024-05-30T22:12:18.488206Z",
     "shell.execute_reply.started": "2024-05-30T22:12:18.362326Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_id: integer (nullable = true)\n",
      " |-- order_date: string (nullable = true)\n",
      " |-- order_customer_id: integer (nullable = true)\n",
      " |-- order_status: string (nullable = true)\n",
      "\n",
      "+--------+--------------------+-----------------+---------------+\n",
      "|order_id|          order_date|order_customer_id|   order_status|\n",
      "+--------+--------------------+-----------------+---------------+\n",
      "|       1|2013-07-25 00:00:...|            11599|         CLOSED|\n",
      "|       2|2013-07-25 00:00:...|              256|PENDING_PAYMENT|\n",
      "|       3|2013-07-25 00:00:...|            12111|       COMPLETE|\n",
      "|       4|2013-07-25 00:00:...|             8827|         CLOSED|\n",
      "|       5|2013-07-25 00:00:...|            11318|       COMPLETE|\n",
      "+--------+--------------------+-----------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "68883"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orders=spark.read.csv(\n",
    "    './retail_db/orders', \\\n",
    "    schema='order_id INT, order_date STRING, order_customer_id INT, order_status STRING'\n",
    ")\n",
    "orders.printSchema()\n",
    "orders.show(5)\n",
    "orders.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b3032608-26df-4d5e-9580-f0139550026a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-30T21:53:17.320211Z",
     "iopub.status.busy": "2024-05-30T21:53:17.319694Z",
     "iopub.status.idle": "2024-05-30T21:53:17.328887Z",
     "shell.execute_reply": "2024-05-30T21:53:17.327843Z",
     "shell.execute_reply.started": "2024-05-30T21:53:17.320184Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import date_format\n",
    "#date_format? \n",
    "#help(date_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c1543c4a-2145-48ba-bb89-b97b094427c4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-30T21:57:43.366459Z",
     "iopub.status.busy": "2024-05-30T21:57:43.365664Z",
     "iopub.status.idle": "2024-05-30T21:57:43.438756Z",
     "shell.execute_reply": "2024-05-30T21:57:43.438417Z",
     "shell.execute_reply.started": "2024-05-30T21:57:43.366410Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+-----------------+---------------+-----------+\n",
      "|order_id|          order_date|order_customer_id|   order_status|order_month|\n",
      "+--------+--------------------+-----------------+---------------+-----------+\n",
      "|       1|2013-07-25 00:00:...|            11599|         CLOSED|     201307|\n",
      "|       2|2013-07-25 00:00:...|              256|PENDING_PAYMENT|     201307|\n",
      "|       3|2013-07-25 00:00:...|            12111|       COMPLETE|     201307|\n",
      "|       4|2013-07-25 00:00:...|             8827|         CLOSED|     201307|\n",
      "|       5|2013-07-25 00:00:...|            11318|       COMPLETE|     201307|\n",
      "+--------+--------------------+-----------------+---------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders.select('*', date_format(orders['order_date'], 'yyyyMM').alias('order_month')).show(5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "904137eb-8ee5-4dac-b4f7-5f5b20b7f4a0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-30T21:59:54.089930Z",
     "iopub.status.busy": "2024-05-30T21:59:54.089097Z",
     "iopub.status.idle": "2024-05-30T21:59:54.183207Z",
     "shell.execute_reply": "2024-05-30T21:59:54.182839Z",
     "shell.execute_reply.started": "2024-05-30T21:59:54.089903Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+-----------------+---------------+-----------+\n",
      "|order_id|          order_date|order_customer_id|   order_status|order_month|\n",
      "+--------+--------------------+-----------------+---------------+-----------+\n",
      "|       1|2013-07-25 00:00:...|            11599|         CLOSED|     201307|\n",
      "|       2|2013-07-25 00:00:...|              256|PENDING_PAYMENT|     201307|\n",
      "|       3|2013-07-25 00:00:...|            12111|       COMPLETE|     201307|\n",
      "|       4|2013-07-25 00:00:...|             8827|         CLOSED|     201307|\n",
      "|       5|2013-07-25 00:00:...|            11318|       COMPLETE|     201307|\n",
      "+--------+--------------------+-----------------+---------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders.withColumn('order_month', date_format('order_date', 'yyyyMM')).select('*').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "53cb0c7f-a4ad-4f15-8991-06574eabf8bd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-30T22:02:10.593349Z",
     "iopub.status.busy": "2024-05-30T22:02:10.592437Z",
     "iopub.status.idle": "2024-05-30T22:02:10.838445Z",
     "shell.execute_reply": "2024-05-30T22:02:10.838183Z",
     "shell.execute_reply.started": "2024-05-30T22:02:10.593321Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+-----------------+---------------+\n",
      "|order_id|          order_date|order_customer_id|   order_status|\n",
      "+--------+--------------------+-----------------+---------------+\n",
      "|   25876|2014-01-01 00:00:...|             3414|PENDING_PAYMENT|\n",
      "|   25877|2014-01-01 00:00:...|             5549|PENDING_PAYMENT|\n",
      "|   25878|2014-01-01 00:00:...|             9084|        PENDING|\n",
      "|   25879|2014-01-01 00:00:...|             5118|        PENDING|\n",
      "|   25880|2014-01-01 00:00:...|            10146|       CANCELED|\n",
      "+--------+--------------------+-----------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders. \\\n",
    "    filter(date_format('order_date', 'yyyyMM')==201401). \\\n",
    "    show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d000ffe5-36cb-44c2-a94a-573489ed06b8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-30T22:05:08.396718Z",
     "iopub.status.busy": "2024-05-30T22:05:08.395940Z",
     "iopub.status.idle": "2024-05-30T22:05:08.549505Z",
     "shell.execute_reply": "2024-05-30T22:05:08.549141Z",
     "shell.execute_reply.started": "2024-05-30T22:05:08.396670Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|order_month|count|\n",
      "+-----------+-----+\n",
      "|     201401| 5908|\n",
      "|     201405| 5467|\n",
      "|     201312| 5892|\n",
      "|     201310| 5335|\n",
      "|     201311| 6381|\n",
      "+-----------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders. \\\n",
    "    groupBy(date_format('order_date', 'yyyyMM').alias('order_month')). \\\n",
    "    count(). \\\n",
    "    show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "305ca6d8-34c5-4326-b60d-e1bef59639e4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-30T22:06:49.498376Z",
     "iopub.status.busy": "2024-05-30T22:06:49.497344Z",
     "iopub.status.idle": "2024-05-30T22:06:49.507947Z",
     "shell.execute_reply": "2024-05-30T22:06:49.506872Z",
     "shell.execute_reply.started": "2024-05-30T22:06:49.498337Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "#help(F.date_format)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74bd80ea-e807-4ec6-a33a-83674cb1328d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Create Dummy Spark Data Frame\n",
    "* Oracle dual (view)\n",
    "* dual - dummy CHAR(1)\n",
    "* 'X' one record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "210c98f1-dcae-43eb-889f-05171fbe337c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-09T01:35:25.529638Z",
     "iopub.status.busy": "2024-06-09T01:35:25.529064Z",
     "iopub.status.idle": "2024-06-09T01:35:25.825889Z",
     "shell.execute_reply": "2024-06-09T01:35:25.825593Z",
     "shell.execute_reply.started": "2024-06-09T01:35:25.529604Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|dummy|\n",
      "+-----+\n",
      "|    X|\n",
      "+-----+\n",
      "\n",
      "root\n",
      " |-- dummy: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "l = [('X',)]\n",
    "df=spark.createDataFrame(l, 'dummy STRING')\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "56080d83-a60b-458f-a8db-a0755eb39b6a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-30T22:26:34.654680Z",
     "iopub.status.busy": "2024-05-30T22:26:34.653772Z",
     "iopub.status.idle": "2024-05-30T22:26:34.917000Z",
     "shell.execute_reply": "2024-05-30T22:26:34.916705Z",
     "shell.execute_reply.started": "2024-05-30T22:26:34.654649Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|current_date|\n",
      "+------------+\n",
      "|  2024-05-31|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# simialar to Oracle - select sysdate from dual\n",
    "df.select(F.current_date().alias('current_date')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "90232d84-1815-4e4c-b21e-0f888fb2a8b2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-09T02:25:18.114957Z",
     "iopub.status.busy": "2024-06-09T02:25:18.114172Z",
     "iopub.status.idle": "2024-06-09T02:25:18.388284Z",
     "shell.execute_reply": "2024-06-09T02:25:18.387986Z",
     "shell.execute_reply.started": "2024-06-09T02:25:18.114910Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+--------------+----------------+-----------+\n",
      "|employee_id|first_name|last_name|salary|   nationality|    phone_number|        ssn|\n",
      "+-----------+----------+---------+------+--------------+----------------+-----------+\n",
      "|          1|     Scott|    Tiger|1000.0| united states| +1 123 456 7890|123 45 6789|\n",
      "|          2|     Henry|     Ford|1250.0|         India|+91 234 567 8901|456 78 9123|\n",
      "|          3|      Nick|   Junior| 750.0|united KINGDOM|+44 111 111 1111|222 33 4444|\n",
      "|          4|      Bill|    Gomes|1500.0|     AUSTRALIA|+61 987 654 3210|789 12 6118|\n",
      "+-----------+----------+---------+------+--------------+----------------+-----------+\n",
      "\n",
      "root\n",
      " |-- employee_id: integer (nullable = true)\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- salary: float (nullable = true)\n",
      " |-- nationality: string (nullable = true)\n",
      " |-- phone_number: string (nullable = true)\n",
      " |-- ssn: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4, None, None)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "employees = [\n",
    "    (1, \"Scott\", \"Tiger\", 1000.0, \"united states\", \"+1 123 456 7890\", \"123 45 6789\"),\n",
    "    (2, \"Henry\", \"Ford\", 1250.0, \"India\", \"+91 234 567 8901\", \"456 78 9123\"),\n",
    "    (3, \"Nick\", \"Junior\", 750.0, \"united KINGDOM\", \"+44 111 111 1111\", \"222 33 4444\"),\n",
    "    (4, \"Bill\", \"Gomes\", 1500.0, \"AUSTRALIA\", \"+61 987 654 3210\", \"789 12 6118\")\n",
    "]\n",
    "\n",
    "employeesDF=spark.createDataFrame( \\\n",
    "    employees, \\\n",
    "    schema='employee_id INT, first_name STRING, last_name STRING, salary FLOAT, nationality STRING, phone_number STRING, ssn STRING' \\\n",
    ")\n",
    "len(employees), employeesDF.show(), employeesDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f85055b-48dd-4429-99b7-d106cf20aa4a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-30T22:44:21.767521Z",
     "iopub.status.busy": "2024-05-30T22:44:21.766915Z",
     "iopub.status.idle": "2024-05-30T22:44:21.779197Z",
     "shell.execute_reply": "2024-05-30T22:44:21.777853Z",
     "shell.execute_reply.started": "2024-05-30T22:44:21.767475Z"
    }
   },
   "source": [
    "## Categories of Functions to Manipulate Columns in Spark Data Frames\n",
    "* String Manipulation Functions\n",
    "  * Case Conversion - `lower`,  `upper`\n",
    "  * Getting Length -  `length`\n",
    "  * Extracting substrings - `substring`, `split`\n",
    "  * Trimming - `trim`, `ltrim`, `rtrim`\n",
    "  * Padding - `lpad`, `rpad`\n",
    "  * Concatenating string - `concat`, `concat_ws`\n",
    "* Date Manipulation Functions\n",
    "  * Getting current date and time - `current_date`, `current_timestamp`\n",
    "  * Date Arithmetic - `date_add`, `date_sub`, `datediff`, `months_between`, `add_months`, `next_day`\n",
    "  * Beginning and Ending Date or Time - `last_day`, `trunc`, `date_trunc`\n",
    "  * Formatting Date - `date_format`\n",
    "  * Extracting Information - `dayofyear`, `dayofmonth`, `dayofweek`, `year`, `month`\n",
    "* Aggregate Functions\n",
    "  * `count`, `countDistinct`\n",
    "  * `sum`, `avg`\n",
    "  * `min`, `max`\n",
    "* Other Functions - We will explore depending on the use cases.\n",
    "  * `CASE` and `WHEN`\n",
    "  * `CAST` for type casting\n",
    "  * Functions to manage special types such as `ARRAY`, `MAP`, `STRUCT` type columns\n",
    "  * Many others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5758ed6-f3c4-4e51-bc0d-fea9ab341407",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-09T00:24:24.823145Z",
     "iopub.status.busy": "2024-06-09T00:24:24.822355Z",
     "iopub.status.idle": "2024-06-09T00:24:25.104010Z",
     "shell.execute_reply": "2024-06-09T00:24:25.103747Z",
     "shell.execute_reply.started": "2024-06-09T00:24:24.823115Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+--------------+----------------+-----------+\n",
      "|employee_id|first_name|last_name|salary|   nationality|    phone_number|        ssn|\n",
      "+-----------+----------+---------+------+--------------+----------------+-----------+\n",
      "|          1|     Scott|    Tiger|1000.0| united states| +1 123 456 7890|123 45 6789|\n",
      "|          2|     Henry|     Ford|1250.0|         india|+91 234 567 8901|456 78 9123|\n",
      "|          3|      Nick|   Junior| 750.0|united kingdom|+44 111 111 1111|222 33 4444|\n",
      "|          4|      Bill|    Gomes|1500.0|     australia|+61 987 654 3210|789 12 6118|\n",
      "+-----------+----------+---------+------+--------------+----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "employeesDF.withColumn('nationality', F.lower('nationality')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c035f37-aeb7-4829-a7fa-ce59bf27454e",
   "metadata": {},
   "source": [
    "## Special functions - col and lit\n",
    "* col - takes column name or expression as argument and return Column object\n",
    "* lit (literal) - takes constant value as an argument and return Coulmn object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a3a3d388-2274-461e-a9e0-ebc42d556673",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-09T00:25:01.496348Z",
     "iopub.status.busy": "2024-06-09T00:25:01.495526Z",
     "iopub.status.idle": "2024-06-09T00:25:01.748852Z",
     "shell.execute_reply": "2024-06-09T00:25:01.748493Z",
     "shell.execute_reply.started": "2024-06-09T00:25:01.496300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+\n",
      "|first_name|last_name|\n",
      "+----------+---------+\n",
      "|     Scott|    Tiger|\n",
      "|     Henry|     Ford|\n",
      "|      Nick|   Junior|\n",
      "|      Bill|    Gomes|\n",
      "+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF.select('first_name', 'last_name').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9cbc1fbf-bab5-403c-b99c-5dc13cb54c6d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-09T00:26:20.337419Z",
     "iopub.status.busy": "2024-06-09T00:26:20.336781Z",
     "iopub.status.idle": "2024-06-09T00:26:20.837023Z",
     "shell.execute_reply": "2024-06-09T00:26:20.836710Z",
     "shell.execute_reply.started": "2024-06-09T00:26:20.337386Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----+\n",
      "|   nationality|count|\n",
      "+--------------+-----+\n",
      "| united states|    1|\n",
      "|         India|    1|\n",
      "|united KINGDOM|    1|\n",
      "|     AUSTRALIA|    1|\n",
      "+--------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF. \\\n",
    "    groupBy('nationality'). \\\n",
    "    count(). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "44f75598-424b-41f5-9d67-61e090af03a2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-09T00:27:14.110497Z",
     "iopub.status.busy": "2024-06-09T00:27:14.109816Z",
     "iopub.status.idle": "2024-06-09T00:27:14.326684Z",
     "shell.execute_reply": "2024-06-09T00:27:14.326352Z",
     "shell.execute_reply.started": "2024-06-09T00:27:14.110468Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+--------------+----------------+-----------+\n",
      "|employee_id|first_name|last_name|salary|   nationality|    phone_number|        ssn|\n",
      "+-----------+----------+---------+------+--------------+----------------+-----------+\n",
      "|          1|     Scott|    Tiger|1000.0| united states| +1 123 456 7890|123 45 6789|\n",
      "|          2|     Henry|     Ford|1250.0|         India|+91 234 567 8901|456 78 9123|\n",
      "|          3|      Nick|   Junior| 750.0|united KINGDOM|+44 111 111 1111|222 33 4444|\n",
      "|          4|      Bill|    Gomes|1500.0|     AUSTRALIA|+61 987 654 3210|789 12 6118|\n",
      "+-----------+----------+---------+------+--------------+----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF.orderBy('employee_id').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "27b202aa-9ff7-4d75-9f46-f068c0fb8f72",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-09T00:30:14.696172Z",
     "iopub.status.busy": "2024-06-09T00:30:14.695630Z",
     "iopub.status.idle": "2024-06-09T00:30:14.711102Z",
     "shell.execute_reply": "2024-06-09T00:30:14.710479Z",
     "shell.execute_reply.started": "2024-06-09T00:30:14.696145Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.column.Column"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "type(col('first_name'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "308e8d07-c313-424d-8428-3553f6aaf4eb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-09T00:31:01.957320Z",
     "iopub.status.busy": "2024-06-09T00:31:01.956733Z",
     "iopub.status.idle": "2024-06-09T00:31:02.231092Z",
     "shell.execute_reply": "2024-06-09T00:31:02.230814Z",
     "shell.execute_reply.started": "2024-06-09T00:31:01.957291Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+\n",
      "|first_name|last_name|\n",
      "+----------+---------+\n",
      "|     Scott|    Tiger|\n",
      "|     Henry|     Ford|\n",
      "|      Nick|   Junior|\n",
      "|      Bill|    Gomes|\n",
      "+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF.select(col('first_name'), col('last_name')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "670beca5-4e44-491a-b03a-bb3384e14c59",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-09T00:34:29.880595Z",
     "iopub.status.busy": "2024-06-09T00:34:29.879989Z",
     "iopub.status.idle": "2024-06-09T00:34:30.265741Z",
     "shell.execute_reply": "2024-06-09T00:34:30.265244Z",
     "shell.execute_reply.started": "2024-06-09T00:34:29.880558Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+----------------+\n",
      "|upper(first_name)|upper(last_name)|\n",
      "+-----------------+----------------+\n",
      "|            SCOTT|           TIGER|\n",
      "|            HENRY|            FORD|\n",
      "|             NICK|          JUNIOR|\n",
      "|             BILL|           GOMES|\n",
      "+-----------------+----------------+\n",
      "\n",
      "+-----------------+----------------+\n",
      "|upper(first_name)|upper(last_name)|\n",
      "+-----------------+----------------+\n",
      "|            SCOTT|           TIGER|\n",
      "|            HENRY|            FORD|\n",
      "|             NICK|          JUNIOR|\n",
      "|             BILL|           GOMES|\n",
      "+-----------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import upper\n",
    "\n",
    "employeesDF.select(upper('first_name'), upper('last_name')).show()\n",
    "type(upper('first_name'))\n",
    "employeesDF.select(upper(col('first_name')), upper(employeesDF['last_name'])).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "03d8bb2e-8380-47b0-a64b-f2ce1f166e75",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-09T00:37:10.480641Z",
     "iopub.status.busy": "2024-06-09T00:37:10.479761Z",
     "iopub.status.idle": "2024-06-09T00:37:10.664998Z",
     "shell.execute_reply": "2024-06-09T00:37:10.664707Z",
     "shell.execute_reply.started": "2024-06-09T00:37:10.480609Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+--------------+----------------+-----------+\n",
      "|employee_id|first_name|last_name|salary|   nationality|    phone_number|        ssn|\n",
      "+-----------+----------+---------+------+--------------+----------------+-----------+\n",
      "|          4|      Bill|    Gomes|1500.0|     AUSTRALIA|+61 987 654 3210|789 12 6118|\n",
      "|          3|      Nick|   Junior| 750.0|united KINGDOM|+44 111 111 1111|222 33 4444|\n",
      "|          2|     Henry|     Ford|1250.0|         India|+91 234 567 8901|456 78 9123|\n",
      "|          1|     Scott|    Tiger|1000.0| united states| +1 123 456 7890|123 45 6789|\n",
      "+-----------+----------+---------+------+--------------+----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# employeesDF.orderBy('employee_id'.desc()).show() #FAILS\n",
    "employeesDF.orderBy(col('employee_id').desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1de18b91-d7a0-4366-8d56-6bae16ae3b79",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-09T00:41:01.562187Z",
     "iopub.status.busy": "2024-06-09T00:41:01.561311Z",
     "iopub.status.idle": "2024-06-09T00:41:01.839897Z",
     "shell.execute_reply": "2024-06-09T00:41:01.839550Z",
     "shell.execute_reply.started": "2024-06-09T00:41:01.562135Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+--------------+----------------+-----------+\n",
      "|employee_id|first_name|last_name|salary|   nationality|    phone_number|        ssn|\n",
      "+-----------+----------+---------+------+--------------+----------------+-----------+\n",
      "|          4|      Bill|    Gomes|1500.0|     AUSTRALIA|+61 987 654 3210|789 12 6118|\n",
      "|          2|     Henry|     Ford|1250.0|         India|+91 234 567 8901|456 78 9123|\n",
      "|          3|      Nick|   Junior| 750.0|united KINGDOM|+44 111 111 1111|222 33 4444|\n",
      "|          1|     Scott|    Tiger|1000.0| united states| +1 123 456 7890|123 45 6789|\n",
      "+-----------+----------+---------+------+--------------+----------------+-----------+\n",
      "\n",
      "+-----------+----------+---------+------+--------------+----------------+-----------+\n",
      "|employee_id|first_name|last_name|salary|   nationality|    phone_number|        ssn|\n",
      "+-----------+----------+---------+------+--------------+----------------+-----------+\n",
      "|          4|      Bill|    Gomes|1500.0|     AUSTRALIA|+61 987 654 3210|789 12 6118|\n",
      "|          2|     Henry|     Ford|1250.0|         India|+91 234 567 8901|456 78 9123|\n",
      "|          3|      Nick|   Junior| 750.0|united KINGDOM|+44 111 111 1111|222 33 4444|\n",
      "|          1|     Scott|    Tiger|1000.0| united states| +1 123 456 7890|123 45 6789|\n",
      "+-----------+----------+---------+------+--------------+----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF.orderBy(upper(employeesDF['first_name']).alias('first_name')).show()\n",
    "employeesDF.orderBy(upper(employeesDF.first_name).alias('first_name')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e5fcb8a8-64a3-4a79-b252-69780d530239",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-09T00:44:31.597663Z",
     "iopub.status.busy": "2024-06-09T00:44:31.596469Z",
     "iopub.status.idle": "2024-06-09T00:44:31.975310Z",
     "shell.execute_reply": "2024-06-09T00:44:31.974975Z",
     "shell.execute_reply.started": "2024-06-09T00:44:31.597614Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+---------+\n",
      "|first_name| , |last_name|\n",
      "+----------+---+---------+\n",
      "|     Scott| , |    Tiger|\n",
      "|     Henry| , |     Ford|\n",
      "|      Nick| , |   Junior|\n",
      "|      Bill| , |    Gomes|\n",
      "+----------+---+---------+\n",
      "\n",
      "+------------+\n",
      "|   full_name|\n",
      "+------------+\n",
      "|Scott, Tiger|\n",
      "| Henry, Ford|\n",
      "|Nick, Junior|\n",
      "| Bill, Gomes|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit, concat\n",
    "employeesDF.select('first_name', lit(', '), 'last_name').show()\n",
    "employeesDF.select(concat('first_name', lit(', '), 'last_name').alias('full_name')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6eb5f82b-1de4-4dea-8e2f-25464884d46a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-09T00:48:39.289832Z",
     "iopub.status.busy": "2024-06-09T00:48:39.289267Z",
     "iopub.status.idle": "2024-06-09T00:48:39.595570Z",
     "shell.execute_reply": "2024-06-09T00:48:39.595319Z",
     "shell.execute_reply.started": "2024-06-09T00:48:39.289803Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+--------------+----------------+-----------+-----+\n",
      "|employee_id|first_name|last_name|salary|   nationality|    phone_number|        ssn|bonus|\n",
      "+-----------+----------+---------+------+--------------+----------------+-----------+-----+\n",
      "|          1|     Scott|    Tiger|1000.0| united states| +1 123 456 7890|123 45 6789|200.0|\n",
      "|          2|     Henry|     Ford|1250.0|         India|+91 234 567 8901|456 78 9123|250.0|\n",
      "|          3|      Nick|   Junior| 750.0|united KINGDOM|+44 111 111 1111|222 33 4444|150.0|\n",
      "|          4|      Bill|    Gomes|1500.0|     AUSTRALIA|+61 987 654 3210|789 12 6118|300.0|\n",
      "+-----------+----------+---------+------+--------------+----------------+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF.withColumn('bonus', col('salary')*0.2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bcda1bfb-9d3b-4daa-b946-5952ee281557",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-09T00:57:34.107501Z",
     "iopub.status.busy": "2024-06-09T00:57:34.106941Z",
     "iopub.status.idle": "2024-06-09T00:57:34.491803Z",
     "shell.execute_reply": "2024-06-09T00:57:34.491365Z",
     "shell.execute_reply.started": "2024-06-09T00:57:34.107472Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|   full_name|\n",
      "+------------+\n",
      "|Scott, Tiger|\n",
      "| Henry, Ford|\n",
      "|Nick, Junior|\n",
      "| Bill, Gomes|\n",
      "+------------+\n",
      "\n",
      "+-----------+----------+---------+------+--------------+----------------+-----------+------------+\n",
      "|employee_id|first_name|last_name|salary|   nationality|    phone_number|        ssn|   full_name|\n",
      "+-----------+----------+---------+------+--------------+----------------+-----------+------------+\n",
      "|          1|     Scott|    Tiger|1000.0| united states| +1 123 456 7890|123 45 6789|Scott, Tiger|\n",
      "|          2|     Henry|     Ford|1250.0|         India|+91 234 567 8901|456 78 9123| Henry, Ford|\n",
      "|          3|      Nick|   Junior| 750.0|united KINGDOM|+44 111 111 1111|222 33 4444|Nick, Junior|\n",
      "|          4|      Bill|    Gomes|1500.0|     AUSTRALIA|+61 987 654 3210|789 12 6118| Bill, Gomes|\n",
      "+-----------+----------+---------+------+--------------+----------------+-----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import concat_ws\n",
    "employeesDF.select(concat_ws(', ', 'first_name', 'last_name').alias('full_name')).show()\n",
    "employeesDF.withColumn('full_name', concat_ws(', ', 'first_name', 'last_name')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9ce21a6b-a87e-4cd1-afd0-bcc4e8ae4e42",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-09T01:20:10.428841Z",
     "iopub.status.busy": "2024-06-09T01:20:10.428055Z",
     "iopub.status.idle": "2024-06-09T01:20:10.638071Z",
     "shell.execute_reply": "2024-06-09T01:20:10.637728Z",
     "shell.execute_reply.started": "2024-06-09T01:20:10.428796Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+-------+---+-----------+--------------+\n",
      "|             address|         city|country| id|postal_code|         state|\n",
      "+--------------------+-------------+-------+---+-----------+--------------+\n",
      "|   36155 Dayton Hill|       Newton|     US|  1|      02162| Massachusetts|\n",
      "|      1551 6th Plaza|      Modesto|     US|  2|      95354|   Californial|\n",
      "|  21370 Waubesa Pass|New York City|     US|  3|      10120|      New York|\n",
      "|     7849 Ohio Drive|  Springfield|     US|  4|      65805|      Missouri|\n",
      "|   6268 Marcy Center|     Brooklyn|     US|  5|      11254|      New York|\n",
      "|3315 Schlimgen Place|    Lexington|     US|  6|      40576|      Kentucky|\n",
      "|     44 Surrey Plaza|   Saint Paul|     US|  7|      55115|     Minnesota|\n",
      "|  512 Carpenter Lane|    Charlotte|     US|  8|      28205|North Carolina|\n",
      "|   566 Kipling Court|       Austin|     US|  9|      78764|         Texas|\n",
      "|      9 Debs Parkway|New York City|     US| 10|      10090|      New York|\n",
      "+--------------------+-------------+-------+---+-----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "addresses = [{'address': '36155 Dayton Hill', 'city': 'Newton', 'country': 'US', 'id': 1, 'postal_code': '02162', 'state': 'Massachusetts'}\n",
    "           , {'address': '1551 6th Plaza', 'city': 'Modesto', 'country': 'US', 'id': 2, 'postal_code': '95354', 'state': 'Californial'}\n",
    "           , {'address': '21370 Waubesa Pass', 'city': 'New York City', 'country': 'US', 'id': 3, 'postal_code': '10120', 'state': 'New York'}\n",
    "           , {'address': '7849 Ohio Drive', 'city': 'Springfield', 'country': 'US', 'id': 4, 'postal_code': '65805', 'state': 'Missouri'}\n",
    "           , {'address': '6268 Marcy Center', 'city': 'Brooklyn', 'country': 'US', 'id': 5, 'postal_code': '11254', 'state': 'New York'}\n",
    "           , {'address': '3315 Schlimgen Place', 'city': 'Lexington', 'country': 'US', 'id': 6, 'postal_code': '40576', 'state': 'Kentucky'}\n",
    "           , {'address': '44 Surrey Plaza', 'city': 'Saint Paul', 'country': 'US', 'id': 7, 'postal_code': '55115', 'state': 'Minnesota'}\n",
    "           , {'address': '512 Carpenter Lane', 'city': 'Charlotte', 'country': 'US', 'id': 8, 'postal_code': '28205', 'state': 'North Carolina'}\n",
    "           , {'address': '566 Kipling Court', 'city': 'Austin', 'country': 'US', 'id': 9, 'postal_code': '78764', 'state': 'Texas'}\n",
    "           , {'address': '9 Debs Parkway', 'city': 'New York City', 'country': 'US', 'id': 10, 'postal_code': '10090', 'state': 'New York'}]\n",
    "addressesDF=spark.createDataFrame(addresses)\n",
    "addressesDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4a35e400-8588-45db-a5d2-ef76813328c3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-09T01:22:21.630290Z",
     "iopub.status.busy": "2024-06-09T01:22:21.628968Z",
     "iopub.status.idle": "2024-06-09T01:22:21.843259Z",
     "shell.execute_reply": "2024-06-09T01:22:21.842828Z",
     "shell.execute_reply.started": "2024-06-09T01:22:21.630238Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------------------------------------------+\n",
      "|id |full_address                                            |\n",
      "+---+--------------------------------------------------------+\n",
      "|1  |36155 Dayton Hill, Newton, Massachusetts, US, 02162     |\n",
      "|2  |1551 6th Plaza, Modesto, Californial, US, 95354         |\n",
      "|3  |21370 Waubesa Pass, New York City, New York, US, 10120  |\n",
      "|4  |7849 Ohio Drive, Springfield, Missouri, US, 65805       |\n",
      "|5  |6268 Marcy Center, Brooklyn, New York, US, 11254        |\n",
      "|6  |3315 Schlimgen Place, Lexington, Kentucky, US, 40576    |\n",
      "|7  |44 Surrey Plaza, Saint Paul, Minnesota, US, 55115       |\n",
      "|8  |512 Carpenter Lane, Charlotte, North Carolina, US, 28205|\n",
      "|9  |566 Kipling Court, Austin, Texas, US, 78764             |\n",
      "|10 |9 Debs Parkway, New York City, New York, US, 10090      |\n",
      "+---+--------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "addressesDF. \\\n",
    "    select('id', concat_ws(', ', 'address', 'city', 'state', 'country', 'postal_code').alias('full_address')). \\\n",
    "    show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e75f84a3-89b1-4349-ab7a-39652cb1ea0f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-09T01:24:55.739846Z",
     "iopub.status.busy": "2024-06-09T01:24:55.739083Z",
     "iopub.status.idle": "2024-06-09T01:24:55.747296Z",
     "shell.execute_reply": "2024-06-09T01:24:55.746181Z",
     "shell.execute_reply.started": "2024-06-09T01:24:55.739799Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, lower, upper, initcap, length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "550d11ec-9877-44e5-a2f9-bcfbbf182927",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-09T01:25:23.504107Z",
     "iopub.status.busy": "2024-06-09T01:25:23.503359Z",
     "iopub.status.idle": "2024-06-09T01:25:23.775642Z",
     "shell.execute_reply": "2024-06-09T01:25:23.775370Z",
     "shell.execute_reply.started": "2024-06-09T01:25:23.504078Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+--------------+----------------+-----------+\n",
      "|employee_id|first_name|last_name|salary|   nationality|    phone_number|        ssn|\n",
      "+-----------+----------+---------+------+--------------+----------------+-----------+\n",
      "|          1|     Scott|    Tiger|1000.0| united states| +1 123 456 7890|123 45 6789|\n",
      "|          2|     Henry|     Ford|1250.0|         India|+91 234 567 8901|456 78 9123|\n",
      "|          3|      Nick|   Junior| 750.0|united KINGDOM|+44 111 111 1111|222 33 4444|\n",
      "|          4|      Bill|    Gomes|1500.0|     AUSTRALIA|+61 987 654 3210|789 12 6118|\n",
      "+-----------+----------+---------+------+--------------+----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9c8ae3e8-7027-436f-87d7-2646aa9a72c0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-09T01:29:30.554267Z",
     "iopub.status.busy": "2024-06-09T01:29:30.553428Z",
     "iopub.status.idle": "2024-06-09T01:29:30.847339Z",
     "shell.execute_reply": "2024-06-09T01:29:30.847043Z",
     "shell.execute_reply.started": "2024-06-09T01:29:30.554236Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------+-----------------+-----------------+-------------------+------------------+\n",
      "|employee_id|   nationality|nationality_upper|nationality_lower|nationality_initcap|nationality_length|\n",
      "+-----------+--------------+-----------------+-----------------+-------------------+------------------+\n",
      "|          1| united states|    UNITED STATES|    united states|      United States|                13|\n",
      "|          2|         India|            INDIA|            india|              India|                 5|\n",
      "|          3|united KINGDOM|   UNITED KINGDOM|   united kingdom|     United Kingdom|                14|\n",
      "|          4|     AUSTRALIA|        AUSTRALIA|        australia|          Australia|                 9|\n",
      "+-----------+--------------+-----------------+-----------------+-------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF.select('employee_id', 'nationality').\\\n",
    "    withColumn('nationality_upper', upper('nationality')). \\\n",
    "    withColumn('nationality_lower', lower('nationality')). \\\n",
    "    withColumn('nationality_initcap', initcap('nationality')). \\\n",
    "    withColumn('nationality_length', length('nationality')). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90379f2b-af4e-4473-858d-4f95de019825",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Extracting Strings using Substring from Spark Data Frame Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "02617b29-c961-4724-826c-d5dc3db1c639",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-09T01:33:45.245716Z",
     "iopub.status.busy": "2024-06-09T01:33:45.245170Z",
     "iopub.status.idle": "2024-06-09T01:33:45.258433Z",
     "shell.execute_reply": "2024-06-09T01:33:45.257624Z",
     "shell.execute_reply.started": "2024-06-09T01:33:45.245686Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Hello', 'ell')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s =  'Hello World'\n",
    "s[:5], s[1:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a945a75d-fa5e-4a3b-96c4-a2005240c36a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-09T01:44:32.447026Z",
     "iopub.status.busy": "2024-06-09T01:44:32.445770Z",
     "iopub.status.idle": "2024-06-09T01:44:32.654613Z",
     "shell.execute_reply": "2024-06-09T01:44:32.654103Z",
     "shell.execute_reply.started": "2024-06-09T01:44:32.446973Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+-----------------------------+\n",
      "|substring(Hello World, 7, 5)|substring(Hello World, -5, 5)|\n",
      "+----------------------------+-----------------------------+\n",
      "|                       World|                        World|\n",
      "+----------------------------+-----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import substring, cast\n",
    "df. \\\n",
    "    select(substring(lit('Hello World'), 7, 5), substring(lit('Hello World'), -5, 5)). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "56539749-8cbe-49b6-bca5-b189eb5f69dd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-09T01:46:27.623524Z",
     "iopub.status.busy": "2024-06-09T01:46:27.622747Z",
     "iopub.status.idle": "2024-06-09T01:46:28.009318Z",
     "shell.execute_reply": "2024-06-09T01:46:28.008818Z",
     "shell.execute_reply.started": "2024-06-09T01:46:27.623479Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+----------------+-----------+---------+\n",
      "|employee_id|        ssn|    phone_number|phone_last4|ssn_last4|\n",
      "+-----------+-----------+----------------+-----------+---------+\n",
      "|          1|123 45 6789| +1 123 456 7890|       7890|     6789|\n",
      "|          2|456 78 9123|+91 234 567 8901|       8901|     9123|\n",
      "|          3|222 33 4444|+44 111 111 1111|       1111|     4444|\n",
      "|          4|789 12 6118|+61 987 654 3210|       3210|     6118|\n",
      "+-----------+-----------+----------------+-----------+---------+\n",
      "\n",
      "+-----------+-----------+----------------+-----------+----------+\n",
      "|employee_id|        ssn|    phone_number|phone_last4|ssn_last4_|\n",
      "+-----------+-----------+----------------+-----------+----------+\n",
      "|          1|123 45 6789| +1 123 456 7890|       7890|      6789|\n",
      "|          2|456 78 9123|+91 234 567 8901|       8901|      9123|\n",
      "|          3|222 33 4444|+44 111 111 1111|       1111|      4444|\n",
      "|          4|789 12 6118|+61 987 654 3210|       3210|      6118|\n",
      "+-----------+-----------+----------------+-----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF. \\\n",
    "    select('employee_id', 'ssn', 'phone_number'). \\\n",
    "    withColumn('phone_last4', substring('phone_number', -4, 4).cast('int')). \\\n",
    "    withColumn('ssn_last4', substring('ssn', -4, 4).cast('int')). \\\n",
    "    show()\n",
    "\n",
    "employeesDF. \\\n",
    "    select('employee_id', 'ssn', 'phone_number'). \\\n",
    "    withColumn('phone_last4', substring('phone_number', -4, 4).cast('int')). \\\n",
    "    withColumn('ssn_last4_', substring('ssn', 8, 4).cast('int')). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ade4cc-bc48-401c-abb5-86fc01f90614",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-09T01:47:46.229403Z",
     "iopub.status.busy": "2024-06-09T01:47:46.228549Z",
     "iopub.status.idle": "2024-06-09T01:47:46.248886Z",
     "shell.execute_reply": "2024-06-09T01:47:46.247229Z",
     "shell.execute_reply.started": "2024-06-09T01:47:46.229375Z"
    },
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Extracting Strings using split from Spark Data Frame Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "84af55bb-bff7-4e31-adff-a2844744c47a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-09T01:55:56.130060Z",
     "iopub.status.busy": "2024-06-09T01:55:56.129259Z",
     "iopub.status.idle": "2024-06-09T01:55:56.687169Z",
     "shell.execute_reply": "2024-06-09T01:55:56.686705Z",
     "shell.execute_reply.started": "2024-06-09T01:55:56.130014Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------+\n",
      "|split(Hello World, how are you,  , -1)|\n",
      "+--------------------------------------+\n",
      "|[Hello, World,, how, are, you]        |\n",
      "+--------------------------------------+\n",
      "\n",
      "+-----------------------------------------+\n",
      "|split(Hello World, how are you,  , -1)[2]|\n",
      "+-----------------------------------------+\n",
      "|how                                      |\n",
      "+-----------------------------------------+\n",
      "\n",
      "+------+\n",
      "|  word|\n",
      "+------+\n",
      "| Hello|\n",
      "|World,|\n",
      "|   how|\n",
      "|   are|\n",
      "|   you|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit, split, explode\n",
    "df.select(split(lit('Hello World, how are you'), ' ')).show(truncate=False)\n",
    "df.select(split(lit('Hello World, how are you'), ' ')[2]).show(truncate=False) # 3rd element from the array\n",
    "df.select(explode(split(lit('Hello World, how are you'), ' ')).alias('word')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "97cc0bf1-a6d6-47d2-a95d-7c5d86ed38e9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-09T02:02:26.034835Z",
     "iopub.status.busy": "2024-06-09T02:02:26.034045Z",
     "iopub.status.idle": "2024-06-09T02:02:26.239329Z",
     "shell.execute_reply": "2024-06-09T02:02:26.238371Z",
     "shell.execute_reply.started": "2024-06-09T02:02:26.034791Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+--------------+--------------------+-----------+\n",
      "|employee_id|first_name|last_name|salary|   nationality|       phone_numbers|        ssn|\n",
      "+-----------+----------+---------+------+--------------+--------------------+-----------+\n",
      "|          1|     Scott|    Tiger|1000.0| united states|+1 123 456 7890,+...|123 45 6789|\n",
      "|          2|     Henry|     Ford|1250.0|         India|    +91 234 567 8901|456 78 9123|\n",
      "|          3|      Nick|   Junior| 750.0|united KINGDOM|+44 111 111 1111,...|222 33 4444|\n",
      "|          4|      Bill|    Gomes|1500.0|     AUSTRALIA|+61 987 654 3210,...|789 12 6118|\n",
      "+-----------+----------+---------+------+--------------+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employees = [\n",
    "    (1, \"Scott\", \"Tiger\", 1000.0, \"united states\", \"+1 123 456 7890,+1 234 567 8901\", \"123 45 6789\"),\n",
    "    (2, \"Henry\", \"Ford\", 1250.0, \"India\", \"+91 234 567 8901\", \"456 78 9123\"),\n",
    "    (3, \"Nick\", \"Junior\", 750.0, \"united KINGDOM\", \"+44 111 111 1111,+44 222 222 2222\", \"222 33 4444\"),\n",
    "    (4, \"Bill\", \"Gomes\", 1500.0, \"AUSTRALIA\", \"+61 987 654 3210,+61 876 543 2109\", \"789 12 6118\")\n",
    "]\n",
    "\n",
    "employeesDF=spark.createDataFrame( \\\n",
    "    employees, \\\n",
    "    schema='employee_id INT, first_name STRING, last_name STRING, salary FLOAT, nationality STRING, phone_numbers STRING, ssn STRING' \\\n",
    ")\n",
    "employeesDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "156673cc-9a2b-44ce-a1ca-2b4d00c2f88a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-09T02:12:39.437293Z",
     "iopub.status.busy": "2024-06-09T02:12:39.436740Z",
     "iopub.status.idle": "2024-06-09T02:12:40.038430Z",
     "shell.execute_reply": "2024-06-09T02:12:40.038014Z",
     "shell.execute_reply.started": "2024-06-09T02:12:39.437265Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+--------------+---------------------------------+-----------+\n",
      "|employee_id|first_name|last_name|salary|nationality   |phone_numbers                    |ssn        |\n",
      "+-----------+----------+---------+------+--------------+---------------------------------+-----------+\n",
      "|1          |Scott     |Tiger    |1000.0|united states |+1 123 456 7890,+1 234 567 8901  |123 45 6789|\n",
      "|2          |Henry     |Ford     |1250.0|India         |+91 234 567 8901                 |456 78 9123|\n",
      "|3          |Nick      |Junior   |750.0 |united KINGDOM|+44 111 111 1111,+44 222 222 2222|222 33 4444|\n",
      "|4          |Bill      |Gomes    |1500.0|AUSTRALIA     |+61 987 654 3210,+61 876 543 2109|789 12 6118|\n",
      "+-----------+----------+---------+------+--------------+---------------------------------+-----------+\n",
      "\n",
      "+-----------+---------------------------------+----------------+\n",
      "|employee_id|phone_numbers                    |phone_number    |\n",
      "+-----------+---------------------------------+----------------+\n",
      "|1          |+1 123 456 7890,+1 234 567 8901  |+1 123 456 7890 |\n",
      "|1          |+1 123 456 7890,+1 234 567 8901  |+1 234 567 8901 |\n",
      "|2          |+91 234 567 8901                 |+91 234 567 8901|\n",
      "|3          |+44 111 111 1111,+44 222 222 2222|+44 111 111 1111|\n",
      "|3          |+44 111 111 1111,+44 222 222 2222|+44 222 222 2222|\n",
      "|4          |+61 987 654 3210,+61 876 543 2109|+61 987 654 3210|\n",
      "|4          |+61 987 654 3210,+61 876 543 2109|+61 876 543 2109|\n",
      "+-----------+---------------------------------+----------------+\n",
      "\n",
      "+-----------+----------------+-----------+---------+-----------+---------+\n",
      "|employee_id|phone_number    |ssn        |area_code|phone_last4|ssn_last4|\n",
      "+-----------+----------------+-----------+---------+-----------+---------+\n",
      "|1          |+1 123 456 7890 |123 45 6789|123      |7890       |6789     |\n",
      "|1          |+1 234 567 8901 |123 45 6789|234      |8901       |6789     |\n",
      "|2          |+91 234 567 8901|456 78 9123|234      |8901       |9123     |\n",
      "|3          |+44 111 111 1111|222 33 4444|111      |1111       |4444     |\n",
      "|3          |+44 222 222 2222|222 33 4444|222      |2222       |4444     |\n",
      "|4          |+61 987 654 3210|789 12 6118|987      |3210       |6118     |\n",
      "|4          |+61 876 543 2109|789 12 6118|876      |2109       |6118     |\n",
      "+-----------+----------------+-----------+---------+-----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF.show(truncate=False)\n",
    "employeesDF.select('employee_id', 'phone_numbers'). \\\n",
    "    withColumn('phone_number', explode(split('phone_numbers', ','))).\\\n",
    "    show(truncate=False)\n",
    "employeesDF. \\\n",
    "    withColumn('phone_number', explode(split('phone_numbers', ','))).\\\n",
    "    select('employee_id', 'phone_number', 'ssn'). \\\n",
    "    withColumn('area_code', split('phone_number', ' ')[1].cast('int')). \\\n",
    "    withColumn('phone_last4', split('phone_number', ' ')[3].cast('int')). \\\n",
    "    withColumn('ssn_last4', split('ssn', ' ')[2].cast('int')). \\\n",
    "    show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "32479fa9-c880-4f2c-85e8-8adc5c769953",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-09T02:19:31.669151Z",
     "iopub.status.busy": "2024-06-09T02:19:31.668604Z",
     "iopub.status.idle": "2024-06-09T02:19:32.024615Z",
     "shell.execute_reply": "2024-06-09T02:19:32.024095Z",
     "shell.execute_reply.started": "2024-06-09T02:19:31.669123Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|employee_id|count|\n",
      "+-----------+-----+\n",
      "|          1|    2|\n",
      "|          2|    1|\n",
      "|          3|    2|\n",
      "|          4|    2|\n",
      "+-----------+-----+\n",
      "\n",
      "+-----------+-----------+\n",
      "|employee_id|#_of_phones|\n",
      "+-----------+-----------+\n",
      "|          1|          2|\n",
      "|          2|          1|\n",
      "|          3|          2|\n",
      "|          4|          2|\n",
      "+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# number of phones each employee has\n",
    "employeesDF. \\\n",
    "    withColumn('phone_number', explode(split('phone_numbers', ','))). \\\n",
    "    select('employee_id', 'phone_number'). \\\n",
    "    groupBy('employee_id'). \\\n",
    "    count(). \\\n",
    "    show()\n",
    "\n",
    "from pyspark.sql.functions import size\n",
    "employeesDF. \\\n",
    "    withColumn('#_of_phones', size(split('phone_numbers', ','))). \\\n",
    "    select('employee_id', '#_of_phones'). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bfe5912-fbfe-46e1-89e8-7e83bb28acf2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Padding characters around String using Spark Data Frame Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "c730a7ea-500d-48d6-b07b-6c2e0b4da0f4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-09T02:23:39.951411Z",
     "iopub.status.busy": "2024-06-09T02:23:39.950645Z",
     "iopub.status.idle": "2024-06-09T02:23:40.241790Z",
     "shell.execute_reply": "2024-06-09T02:23:40.241509Z",
     "shell.execute_reply.started": "2024-06-09T02:23:39.951364Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|lpad(Hello, 10, -)|\n",
      "+------------------+\n",
      "|        -----Hello|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit, rpad, lpad\n",
    "df.select(lpad(lit('Hello'), 10, '-')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "77fa6629-0894-4088-9570-a26f029c181a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-09T02:33:31.825738Z",
     "iopub.status.busy": "2024-06-09T02:33:31.825145Z",
     "iopub.status.idle": "2024-06-09T02:33:32.029341Z",
     "shell.execute_reply": "2024-06-09T02:33:32.028747Z",
     "shell.execute_reply.started": "2024-06-09T02:33:31.825713Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------------------+\n",
      "|employee                                                            |\n",
      "+--------------------------------------------------------------------+\n",
      "|00001Scott-----00001000.0united states--+1 123 456 7890--123 45 6789|\n",
      "|00002Henry-----00001250.0India----------+91 234 567 8901-456 78 9123|\n",
      "|00003Nick------00000750.0united KINGDOM-+44 111 111 1111-222 33 4444|\n",
      "|00004Bill------00001500.0AUSTRALIA------+61 987 654 3210-789 12 6118|\n",
      "+--------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "empFixDF = employeesDF. \\\n",
    "    select(concat(lpad('employee_id', 5, '0'), \\\n",
    "                  rpad('first_name', 10,  '-'), \\\n",
    "                  lpad('salary', 10, '0'), \\\n",
    "                  rpad('nationality', 15,  '-'), \\\n",
    "                  rpad('phone_number', 17,  '-'), \\\n",
    "                  'ssn'\n",
    "                 ).\n",
    "           alias('employee')\\\n",
    "          )\n",
    "empFixDF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff67b60-6159-459c-bd3c-ace4c702f57a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Trimming characters from String in Spark Data Frame Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "16e6a2f7-4fe4-4170-93d4-870c565f142d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-09T02:41:11.973187Z",
     "iopub.status.busy": "2024-06-09T02:41:11.972643Z",
     "iopub.status.idle": "2024-06-09T02:41:12.178240Z",
     "shell.execute_reply": "2024-06-09T02:41:12.177875Z",
     "shell.execute_reply.started": "2024-06-09T02:41:11.973160Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|         dummy|\n",
      "+--------------+\n",
      "|    Hello.    |\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#l = [('    Hello    ', '    World    ',)]\n",
    "#df=spark.createDataFrame(l).toDF('A', 'B') #column list\n",
    "\n",
    "l = [('    Hello.    ',)]\n",
    "df=spark.createDataFrame(l).toDF('dummy') #column list\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "ad59bdca-bcdf-4074-892a-eaa7b5f53bfd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-09T02:44:41.016120Z",
     "iopub.status.busy": "2024-06-09T02:44:41.015345Z",
     "iopub.status.idle": "2024-06-09T02:44:41.286785Z",
     "shell.execute_reply": "2024-06-09T02:44:41.286519Z",
     "shell.execute_reply.started": "2024-06-09T02:44:41.016074Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------+----------+------+\n",
      "|         dummy|     rtrim|     ltrim|  trim|\n",
      "+--------------+----------+----------+------+\n",
      "|    Hello.    |    Hello.|Hello.    |Hello.|\n",
      "+--------------+----------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, rtrim, ltrim, trim\n",
    "df. \\\n",
    "    withColumn('rtrim', rtrim('dummy')). \\\n",
    "    withColumn('ltrim', ltrim('dummy')). \\\n",
    "    withColumn('trim', trim('dummy')). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "3b10fbb3-00a1-4f58-aaac-c4cc06e579a5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-09T02:47:11.809614Z",
     "iopub.status.busy": "2024-06-09T02:47:11.809069Z",
     "iopub.status.idle": "2024-06-09T02:47:11.848403Z",
     "shell.execute_reply": "2024-06-09T02:47:11.847769Z",
     "shell.execute_reply.started": "2024-06-09T02:47:11.809583Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------------+\n",
      "|function_desc                                                                  |\n",
      "+-------------------------------------------------------------------------------+\n",
      "|Function: rtrim                                                                |\n",
      "|Class: org.apache.spark.sql.catalyst.expressions.StringTrimRight               |\n",
      "|Usage: \\n    rtrim(str) - Removes the trailing space characters from `str`.\\n  |\n",
      "+-------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "spark.sql('DESCRIBE FUNCTION rtrim').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "35cdf9b8-5ea0-4403-9a85-1adf9125dd43",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-09T03:02:07.893384Z",
     "iopub.status.busy": "2024-06-09T03:02:07.892267Z",
     "iopub.status.idle": "2024-06-09T03:02:08.170344Z",
     "shell.execute_reply": "2024-06-09T03:02:08.170088Z",
     "shell.execute_reply.started": "2024-06-09T03:02:07.893337Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------+---------+------+\n",
      "|         dummy|     rtrim|    ltrim|  trim|\n",
      "+--------------+----------+---------+------+\n",
      "|    Hello.    |Hello.    |    Hello|Hello.|\n",
      "+--------------+----------+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df. \\\n",
    "    withColumn('rtrim', expr(\"trim(leading ' ' from dummy)\")). \\\n",
    "    withColumn('ltrim', expr(\"trim(trailing '.' from rtrim(dummy))\")). \\\n",
    "    withColumn('trim',  expr(\"trim(both ' ' from dummy)\")). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79fb381-f800-49f0-8321-d48e8c9bc4df",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Date and Time Manipulation Functions using Spark Data Frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fbb7523d-6377-4303-b69e-e14eda0f250b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-09T16:59:30.213403Z",
     "iopub.status.busy": "2024-06-09T16:59:30.212581Z",
     "iopub.status.idle": "2024-06-09T16:59:30.520386Z",
     "shell.execute_reply": "2024-06-09T16:59:30.520076Z",
     "shell.execute_reply.started": "2024-06-09T16:59:30.213353Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|dummy|\n",
      "+-----+\n",
      "|    x|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([('x',)]).toDF('dummy')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "14ed1955-55bb-4ef9-aeb1-6d0c467fce9a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-09T16:59:48.461281Z",
     "iopub.status.busy": "2024-06-09T16:59:48.460734Z",
     "iopub.status.idle": "2024-06-09T16:59:49.277740Z",
     "shell.execute_reply": "2024-06-09T16:59:49.277418Z",
     "shell.execute_reply.started": "2024-06-09T16:59:48.461249Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|current_date()|\n",
      "+--------------+\n",
      "|    2024-06-09|\n",
      "+--------------+\n",
      "\n",
      "+--------------------------+\n",
      "|current_timestamp()       |\n",
      "+--------------------------+\n",
      "|2024-06-09 18:59:48.692772|\n",
      "+--------------------------+\n",
      "\n",
      "+--------------+--------------------------+--------------+--------------------------+\n",
      "|current_date()|current_timestamp()       |current_date()|current_timestamp()       |\n",
      "+--------------+--------------------------+--------------+--------------------------+\n",
      "|2024-06-09    |2024-06-09 18:59:48.884812|2024-06-09    |2024-06-09 18:59:48.884812|\n",
      "+--------------+--------------------------+--------------+--------------------------+\n",
      "\n",
      "+------------+\n",
      "|current_date|\n",
      "+------------+\n",
      "|  2024-06-09|\n",
      "+------------+\n",
      "\n",
      "+--------------+--------------------+\n",
      "|current_date()| current_timestamp()|\n",
      "+--------------+--------------------+\n",
      "|    2024-06-09|2024-06-09 18:59:...|\n",
      "+--------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import current_date, current_timestamp, expr\n",
    "\n",
    "df.select(current_date()).show() #yyyy-MM-dd\n",
    "df.select(current_timestamp()).show(truncate=False) # #yyyy-MM-dd HH:mi:ss.SSS\n",
    "\n",
    "df.selectExpr('current_date', 'current_timestamp', 'current_date()', 'current_timestamp()').show(truncate=False)\n",
    "df.select(expr('current_date').alias('current_date')).show()\n",
    "spark.sql('select current_date, current_timestamp').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c2c847b1-3991-4ea6-aab7-0a2c75a7d6e2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-09T12:45:57.958203Z",
     "iopub.status.busy": "2024-06-09T12:45:57.957491Z",
     "iopub.status.idle": "2024-06-09T12:45:58.423244Z",
     "shell.execute_reply": "2024-06-09T12:45:58.422871Z",
     "shell.execute_reply.started": "2024-06-09T12:45:57.958161Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------+\n",
      "|to_date(20240609, yyyyMMdd)|\n",
      "+---------------------------+\n",
      "|                 2024-06-09|\n",
      "+---------------------------+\n",
      "\n",
      "+--------------------------------------------+\n",
      "|to_timestamp(20240609230501, yyyyMMddHHmmss)|\n",
      "+--------------------------------------------+\n",
      "|                         2024-06-09 23:05:01|\n",
      "+--------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import to_date, to_timestamp, lit\n",
    "df.select(to_date(lit('20240609'), 'yyyyMMdd')).show() \n",
    "df.select(to_timestamp(lit('20240609230501'), 'yyyyMMddHHmmss')).show() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72869c70-3e21-43d8-b6bd-8b553cdd350a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-09T12:54:14.295285Z",
     "iopub.status.busy": "2024-06-09T12:54:14.294494Z",
     "iopub.status.idle": "2024-06-09T12:54:14.304854Z",
     "shell.execute_reply": "2024-06-09T12:54:14.303109Z",
     "shell.execute_reply.started": "2024-06-09T12:54:14.295240Z"
    }
   },
   "source": [
    "## Date and Time Arithmetic using Spark Data Frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0429e5b-7261-425d-b355-3f73e86a5828",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-09T16:01:15.967056Z",
     "iopub.status.busy": "2024-06-09T16:01:15.966452Z",
     "iopub.status.idle": "2024-06-09T16:01:17.023631Z",
     "shell.execute_reply": "2024-06-09T16:01:17.023333Z",
     "shell.execute_reply.started": "2024-06-09T16:01:15.967023Z"
    }
   },
   "outputs": [],
   "source": [
    "datetimes = [\n",
    "    (\"2014-02-28\", \"2014-02-28 10:00:00.123\"),\n",
    "    (\"2016-02-29\", \"2016-02-29 08:08:08.999\"),\n",
    "    (\"2017-10-31\", \"2017-12-31 11:59:59.123\"),\n",
    "    (\"2019-11-30\", \"2019-08-31 00:00:00.000\")\n",
    "]\n",
    "\n",
    "datetimesDF = spark.createDataFrame(datetimes, schema='date string, timestamp string')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2acd1e25-389f-4231-9ab0-a7331ec519f4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-09T13:26:32.850024Z",
     "iopub.status.busy": "2024-06-09T13:26:32.849235Z",
     "iopub.status.idle": "2024-06-09T13:26:33.117885Z",
     "shell.execute_reply": "2024-06-09T13:26:33.117573Z",
     "shell.execute_reply.started": "2024-06-09T13:26:32.849977Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+------------------------------------------------+\n",
      "|to_date(date, yyyy-MM-dd)|to_timestamp(timestamp, yyyy-MM-dd HH:mm:ss.SSS)|\n",
      "+-------------------------+------------------------------------------------+\n",
      "|2014-02-28               |2014-02-28 10:00:00.123                         |\n",
      "|2016-02-29               |2016-02-29 08:08:08.999                         |\n",
      "|2017-10-31               |2017-12-31 11:59:59.123                         |\n",
      "|2019-11-30               |2019-08-31 00:00:00                             |\n",
      "+-------------------------+------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import date_add, date_sub\n",
    "\n",
    "datetimesDF.select(to_date('date', 'yyyy-MM-dd'), to_timestamp('timestamp', 'yyyy-MM-dd HH:mm:ss.SSS')).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "16343e2f-30f1-40ff-9781-42ea2b215165",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-09T13:28:22.493749Z",
     "iopub.status.busy": "2024-06-09T13:28:22.492871Z",
     "iopub.status.idle": "2024-06-09T13:28:22.717175Z",
     "shell.execute_reply": "2024-06-09T13:28:22.716845Z",
     "shell.execute_reply.started": "2024-06-09T13:28:22.493695Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------------+------------------+-----------------------+--------------------+-------------------------+\n",
      "|date      |timestamp              |date_10_days_later|timestamp_10_days_later|date_10_days_earlier|timestamp_10_days_earlier|\n",
      "+----------+-----------------------+------------------+-----------------------+--------------------+-------------------------+\n",
      "|2014-02-28|2014-02-28 10:00:00.123|2014-03-10        |2014-03-10             |2014-02-18          |2014-02-18               |\n",
      "|2016-02-29|2016-02-29 08:08:08.999|2016-03-10        |2016-03-10             |2016-02-19          |2016-02-19               |\n",
      "|2017-10-31|2017-12-31 11:59:59.123|2017-11-10        |2018-01-10             |2017-10-21          |2017-12-21               |\n",
      "|2019-11-30|2019-08-31 00:00:00.000|2019-12-10        |2019-09-10             |2019-11-20          |2019-08-21               |\n",
      "+----------+-----------------------+------------------+-----------------------+--------------------+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datetimesDF. \\\n",
    "    withColumn('date_10_days_later', date_add('date', 10)). \\\n",
    "    withColumn('timestamp_10_days_later', date_add('timestamp', 10)). \\\n",
    "    withColumn('date_10_days_earlier', date_sub('date', 10)). \\\n",
    "    withColumn('timestamp_10_days_earlier', date_sub('timestamp', 10)). \\\n",
    "    show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "617728f8-fcf0-4f9b-b1d9-5da4c32664ee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-09T13:35:09.622236Z",
     "iopub.status.busy": "2024-06-09T13:35:09.621643Z",
     "iopub.status.idle": "2024-06-09T13:35:09.853261Z",
     "shell.execute_reply": "2024-06-09T13:35:09.852557Z",
     "shell.execute_reply.started": "2024-06-09T13:35:09.622205Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------------+-------------+------------------+\n",
      "|date      |timestamp              |datediff_date|datediff_timestamp|\n",
      "+----------+-----------------------+-------------+------------------+\n",
      "|2014-02-28|2014-02-28 10:00:00.123|3754         |3754              |\n",
      "|2016-02-29|2016-02-29 08:08:08.999|3023         |3023              |\n",
      "|2017-10-31|2017-12-31 11:59:59.123|2413         |2352              |\n",
      "|2019-11-30|2019-08-31 00:00:00.000|1653         |1744              |\n",
      "+----------+-----------------------+-------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import current_date, current_timestamp, datediff\n",
    "datetimesDF. \\\n",
    "    withColumn('datediff_date', datediff(current_date(), 'date')). \\\n",
    "    withColumn('datediff_timestamp', datediff(current_timestamp(), 'timestamp')). \\\n",
    "    show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "330acac5-8d93-4ba2-bda6-4e739e0ebd6e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-09T16:10:17.892314Z",
     "iopub.status.busy": "2024-06-09T16:10:17.891600Z",
     "iopub.status.idle": "2024-06-09T16:10:18.320119Z",
     "shell.execute_reply": "2024-06-09T16:10:18.319751Z",
     "shell.execute_reply.started": "2024-06-09T16:10:17.892268Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------------+-------------------+------------------------+---------------+--------------------+\n",
      "|date      |timestamp              |months_between_date|months_between_timestamp|add_months_date|add_months_timestamp|\n",
      "+----------+-----------------------+-------------------+------------------------+---------------+--------------------+\n",
      "|2014-02-28|2014-02-28 10:00:00.123|123.38709677       |123.39807982            |2014-05-28     |2014-05-28          |\n",
      "|2016-02-29|2016-02-29 08:08:08.999|99.35483871        |99.36832773             |2016-05-29     |2016-05-29          |\n",
      "|2017-10-31|2017-12-31 11:59:59.123|79.29032258        |77.29861783             |2018-01-31     |2018-03-31          |\n",
      "|2019-11-30|2019-08-31 00:00:00.000|54.32258065        |57.31474649             |2020-02-29     |2019-11-30          |\n",
      "+----------+-----------------------+-------------------+------------------------+---------------+--------------------+\n",
      "\n",
      "+----------+--------------------+-------------------+------------------------+\n",
      "|      date|           timestamp|months_between_date|months_between_timestamp|\n",
      "+----------+--------------------+-------------------+------------------------+\n",
      "|2014-02-28|2014-02-28 10:00:...|             123.39|                   123.4|\n",
      "|2016-02-29|2016-02-29 08:08:...|              99.35|                   99.37|\n",
      "|2017-10-31|2017-12-31 11:59:...|              79.29|                    77.3|\n",
      "|2019-11-30|2019-08-31 00:00:...|              54.32|                   57.31|\n",
      "+----------+--------------------+-------------------+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import months_between, add_months, round, current_date, current_timestamp\n",
    "datetimesDF. \\\n",
    "    withColumn('months_between_date', months_between(current_date(), 'date')). \\\n",
    "    withColumn('months_between_timestamp', months_between(current_timestamp(), 'timestamp')). \\\n",
    "    withColumn('add_months_date', add_months('date', 3)). \\\n",
    "    withColumn('add_months_timestamp', add_months('timestamp', 3)). \\\n",
    "    show(truncate=False)\n",
    "# round to 2 decimal places\n",
    "datetimesDF. \\\n",
    "    withColumn('months_between_date', round(months_between(current_date(), 'date'), 2)). \\\n",
    "    withColumn('months_between_timestamp', round(months_between(current_timestamp(), 'timestamp'), 2)). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940ab4aa-f0db-435b-a150-76a2ee7bcb92",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Using Date and Time trunc function od Spark Data Frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "deec2fe4-3a50-40a9-866c-9933ee58e3cc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-09T17:24:42.042428Z",
     "iopub.status.busy": "2024-06-09T17:24:42.041852Z",
     "iopub.status.idle": "2024-06-09T17:24:42.318325Z",
     "shell.execute_reply": "2024-06-09T17:24:42.318018Z",
     "shell.execute_reply.started": "2024-06-09T17:24:42.042401Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+----------+----------+-------------+\n",
      "|dummy|trunc_month|trunc_year|trunc_week|trunc_quarter|\n",
      "+-----+-----------+----------+----------+-------------+\n",
      "|    x| 2024-06-01|2024-01-01|2024-06-03|   2024-04-01|\n",
      "+-----+-----------+----------+----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import current_date, current_timestamp, trunc, date_trunc, next_day\n",
    "\n",
    "df. \\\n",
    "    withColumn('trunc_month', trunc(current_timestamp(), format='month')). \\\n",
    "    withColumn('trunc_year', trunc(current_date(), format='year')). \\\n",
    "    withColumn('trunc_week', trunc(current_date(), format='week')). \\\n",
    "    withColumn('trunc_quarter', trunc(current_date(), format='quarter')). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "aff4efce-2b8a-49e6-8ad1-ac891110eb82",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-09T17:25:42.750915Z",
     "iopub.status.busy": "2024-06-09T17:25:42.750359Z",
     "iopub.status.idle": "2024-06-09T17:25:42.967651Z",
     "shell.execute_reply": "2024-06-09T17:25:42.967120Z",
     "shell.execute_reply.started": "2024-06-09T17:25:42.750885Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------------+-------------------+----------+\n",
      "|dummy|    date_trunc_year|  date_trunc_minute|  next_day|\n",
      "+-----+-------------------+-------------------+----------+\n",
      "|    x|2024-01-01 00:00:00|2024-06-09 19:25:00|2024-06-16|\n",
      "+-----+-------------------+-------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df. \\\n",
    "    withColumn('date_trunc_year', date_trunc('year', current_timestamp())). \\\n",
    "    withColumn('date_trunc_minute', date_trunc('minute', current_timestamp())). \\\n",
    "    withColumn('next_day',  next_day(current_timestamp(), 'SUN')). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231e7a62-f5fa-4a36-8d4c-eb1ce6976870",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Date and Time Extract Functions on Spark Data Frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18580696-d954-4539-a5b7-6b19f94c68d9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-10T18:38:37.071370Z",
     "iopub.status.busy": "2024-06-10T18:38:37.070692Z",
     "iopub.status.idle": "2024-06-10T18:38:37.096268Z",
     "shell.execute_reply": "2024-06-10T18:38:37.095653Z",
     "shell.execute_reply.started": "2024-06-10T18:38:37.071333Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m current_timestamp, year, month, weekofyear, dayofyear, dayofmonth, dayofweek, hour, minute, second\n\u001b[0;32m----> 2\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241m.\u001b[39m \\\n\u001b[1;32m      3\u001b[0m     withColumn(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124myear\u001b[39m\u001b[38;5;124m'\u001b[39m, year(current_timestamp()))\u001b[38;5;241m.\u001b[39m \\\n\u001b[1;32m      4\u001b[0m     withColumn(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmonth\u001b[39m\u001b[38;5;124m'\u001b[39m, month(current_timestamp()))\u001b[38;5;241m.\u001b[39m \\\n\u001b[1;32m      5\u001b[0m     withColumn(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweekofyear\u001b[39m\u001b[38;5;124m'\u001b[39m, dayofyear(current_timestamp()))\u001b[38;5;241m.\u001b[39m \\\n\u001b[1;32m      6\u001b[0m     withColumn(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdayofyear\u001b[39m\u001b[38;5;124m'\u001b[39m, dayofyear(current_timestamp()))\u001b[38;5;241m.\u001b[39m \\\n\u001b[1;32m      7\u001b[0m     withColumn(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdayofmonth\u001b[39m\u001b[38;5;124m'\u001b[39m, dayofmonth(current_timestamp()))\u001b[38;5;241m.\u001b[39m \\\n\u001b[1;32m      8\u001b[0m     withColumn(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdayofweek\u001b[39m\u001b[38;5;124m'\u001b[39m, dayofweek(current_timestamp()))\u001b[38;5;241m.\u001b[39m \\\n\u001b[1;32m      9\u001b[0m     withColumn(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhour\u001b[39m\u001b[38;5;124m'\u001b[39m, hour(current_timestamp()))\u001b[38;5;241m.\u001b[39m \\\n\u001b[1;32m     10\u001b[0m     withColumn(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mminute\u001b[39m\u001b[38;5;124m'\u001b[39m, minute(current_timestamp()))\u001b[38;5;241m.\u001b[39m \\\n\u001b[1;32m     11\u001b[0m     withColumn(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msecond\u001b[39m\u001b[38;5;124m'\u001b[39m, second(current_timestamp()))\u001b[38;5;241m.\u001b[39m \\\n\u001b[1;32m     12\u001b[0m     show()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import current_timestamp, year, month, weekofyear, dayofyear, dayofmonth, dayofweek, hour, minute, second\n",
    "df. \\\n",
    "    withColumn('year', year(current_timestamp())). \\\n",
    "    withColumn('month', month(current_timestamp())). \\\n",
    "    withColumn('weekofyear', dayofyear(current_timestamp())). \\\n",
    "    withColumn('dayofyear', dayofyear(current_timestamp())). \\\n",
    "    withColumn('dayofmonth', dayofmonth(current_timestamp())). \\\n",
    "    withColumn('dayofweek', dayofweek(current_timestamp())). \\\n",
    "    withColumn('hour', hour(current_timestamp())). \\\n",
    "    withColumn('minute', minute(current_timestamp())). \\\n",
    "    withColumn('second', second(current_timestamp())). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2ccccf-ac21-47cd-b112-d09926096666",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "52270085-6d55-4d52-9204-55a67177fbb5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Using to_date and to_timestamp on Spark Data Frames\n",
    "\n",
    "`yyyy-MM-dd`\n",
    "`yyyy-MM-dd HH:mm:ss.SSS`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0b17687d-52bd-4e9c-99f3-1389110f7a78",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-10T18:46:05.117868Z",
     "iopub.status.busy": "2024-06-10T18:46:05.117053Z",
     "iopub.status.idle": "2024-06-10T18:46:05.411934Z",
     "shell.execute_reply": "2024-06-10T18:46:05.411681Z",
     "shell.execute_reply.started": "2024-06-10T18:46:05.117820Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------------+\n",
      "|date    |timestamp               |\n",
      "+--------+------------------------+\n",
      "|20140228|28-Feb-2014 10:00:00.123|\n",
      "|20160229|20-Feb-2016 08:08:08.999|\n",
      "|20171031|31-Dec-2017 11:59:59.123|\n",
      "|20191130|31-Aug-2019 00:00:00.000|\n",
      "+--------+------------------------+\n",
      "\n",
      "root\n",
      " |-- date: long (nullable = true)\n",
      " |-- timestamp: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datetimes = [\n",
    "    (20140228, \"28-Feb-2014 10:00:00.123\"),\n",
    "    (20160229, \"20-Feb-2016 08:08:08.999\"),\n",
    "    (20171031, \"31-Dec-2017 11:59:59.123\"),\n",
    "    (20191130, \"31-Aug-2019 00:00:00.000\")\n",
    "]\n",
    "\n",
    "datetimesDF = spark.createDataFrame(datetimes, 'date bigint, timestamp string')\n",
    "datetimesDF.show(truncate=False)\n",
    "datetimesDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "11898f02-36f8-43e8-9f34-a0a78a76102a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-10T19:05:40.843649Z",
     "iopub.status.busy": "2024-06-10T19:05:40.842787Z",
     "iopub.status.idle": "2024-06-10T19:05:41.063063Z",
     "shell.execute_reply": "2024-06-10T19:05:41.062577Z",
     "shell.execute_reply.started": "2024-06-10T19:05:40.843593Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------------+----------+-----------------------+\n",
      "|date    |timestamp               |to_date   |to_timestamp           |\n",
      "+--------+------------------------+----------+-----------------------+\n",
      "|20140228|28-Feb-2014 10:00:00.123|2014-02-28|2014-02-28 10:00:00.123|\n",
      "|20160229|20-Feb-2016 08:08:08.999|2016-02-29|2016-02-20 08:08:08.999|\n",
      "|20171031|31-Dec-2017 11:59:59.123|2017-10-31|2017-12-31 11:59:59.123|\n",
      "|20191130|31-Aug-2019 00:00:00.000|2019-11-30|2019-08-31 00:00:00    |\n",
      "+--------+------------------------+----------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import to_date, to_timestamp, lit, col\n",
    "datetimesDF. \\\n",
    "    withColumn('to_date', to_date(lit(col('date')), 'yyyyMMdd')). \\\n",
    "    withColumn('to_timestamp', to_timestamp(lit(col('timestamp')), 'dd-MMM-yyyy HH:mm:ss.SSS')). \\\n",
    "    show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e21d8c09-c9d8-45b4-b628-db4964dd1f2d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-10T19:01:27.114818Z",
     "iopub.status.busy": "2024-06-10T19:01:27.114092Z",
     "iopub.status.idle": "2024-06-10T19:01:27.402796Z",
     "shell.execute_reply": "2024-06-10T19:01:27.402529Z",
     "shell.execute_reply.started": "2024-06-10T19:01:27.114771Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|dummy|\n",
      "+-----+\n",
      "|    x|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([('x',)]).toDF('dummy')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6e9e5c06-a544-49f8-8551-5d7500ddc259",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-10T19:03:06.976610Z",
     "iopub.status.busy": "2024-06-10T19:03:06.975629Z",
     "iopub.status.idle": "2024-06-10T19:03:07.374651Z",
     "shell.execute_reply": "2024-06-10T19:03:07.374327Z",
     "shell.execute_reply.started": "2024-06-10T19:03:06.976556Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------+\n",
      "|to_date(March 2, 2024, MMMM d, yyyy)|\n",
      "+------------------------------------+\n",
      "|                          2024-03-02|\n",
      "+------------------------------------+\n",
      "\n",
      "+---------------------------------+\n",
      "|to_date(Mar 2, 2024, MMM d, yyyy)|\n",
      "+---------------------------------+\n",
      "|                       2024-03-02|\n",
      "+---------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(to_date(lit('March 2, 2024'), 'MMMM d, yyyy')).show()\n",
    "df.select(to_date(lit('Mar 2, 2024'), 'MMM d, yyyy')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "63690a30-55da-4c5e-82bf-18a521d33a3b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-10T19:08:07.303395Z",
     "iopub.status.busy": "2024-06-10T19:08:07.302556Z",
     "iopub.status.idle": "2024-06-10T19:08:07.569079Z",
     "shell.execute_reply": "2024-06-10T19:08:07.568808Z",
     "shell.execute_reply.started": "2024-06-10T19:08:07.303333Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------+\n",
      "|to_timestamp(Mar 2, 2024, MMM d, yyyy)|\n",
      "+--------------------------------------+\n",
      "|                   2024-03-02 00:00:00|\n",
      "+--------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(to_timestamp(lit('Mar 2, 2024'), 'MMM d, yyyy')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a402dbc-7469-447d-a63c-6f1b1c3169f8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Using `date_format` on Spark Data Frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c11186af-487d-42c1-927a-e2bcff396da3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-10T19:18:58.742199Z",
     "iopub.status.busy": "2024-06-10T19:18:58.741375Z",
     "iopub.status.idle": "2024-06-10T19:18:59.024618Z",
     "shell.execute_reply": "2024-06-10T19:18:59.024306Z",
     "shell.execute_reply.started": "2024-06-10T19:18:58.742144Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------------+\n",
      "|date      |timestamp              |\n",
      "+----------+-----------------------+\n",
      "|2014-02-28|2014-02-28 10:00:00.123|\n",
      "|2016-02-29|2016-02-29 08:08:08.999|\n",
      "|2017-10-31|2017-12-31 11:59:59.123|\n",
      "|2019-11-30|2019-08-31 00:00:00.000|\n",
      "+----------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datetimes = [\n",
    "    (\"2014-02-28\", \"2014-02-28 10:00:00.123\"),\n",
    "    (\"2016-02-29\", \"2016-02-29 08:08:08.999\"),\n",
    "    (\"2017-10-31\", \"2017-12-31 11:59:59.123\"),\n",
    "    (\"2019-11-30\", \"2019-08-31 00:00:00.000\")\n",
    "]\n",
    "\n",
    "datetimesDF = spark.createDataFrame(datetimes, 'date STRING, timestamp STRING')\n",
    "datetimesDF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8befe07b-6a33-41aa-8003-07d828ffeea6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-10T19:22:33.725912Z",
     "iopub.status.busy": "2024-06-10T19:22:33.725094Z",
     "iopub.status.idle": "2024-06-10T19:22:33.941695Z",
     "shell.execute_reply": "2024-06-10T19:22:33.941232Z",
     "shell.execute_reply.started": "2024-06-10T19:22:33.725858Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------------+----------------+---------------------+\n",
      "|date      |timestamp              |date_format_date|date_format_timestamp|\n",
      "+----------+-----------------------+----------------+---------------------+\n",
      "|2014-02-28|2014-02-28 10:00:00.123|28-02-2014 00   |28-02-2014 10        |\n",
      "|2016-02-29|2016-02-29 08:08:08.999|29-02-2016 00   |29-02-2016 08        |\n",
      "|2017-10-31|2017-12-31 11:59:59.123|31-10-2017 00   |31-12-2017 11        |\n",
      "|2019-11-30|2019-08-31 00:00:00.000|30-11-2019 00   |31-08-2019 00        |\n",
      "+----------+-----------------------+----------------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import date_format\n",
    "datetimesDF. \\\n",
    "    withColumn('date_format_date', date_format('date', 'dd-MM-yyyy')). \\\n",
    "    withColumn('date_format_date', date_format('date', 'dd-MM-yyyy HH')). \\\n",
    "    withColumn('date_format_timestamp', date_format('timestamp', 'dd-MM-yyyy HH')). \\\n",
    "    show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0e761e3d-78ea-4a3d-9cf9-623c63a2efae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-10T19:37:20.863235Z",
     "iopub.status.busy": "2024-06-10T19:37:20.862596Z",
     "iopub.status.idle": "2024-06-10T19:37:21.133160Z",
     "shell.execute_reply": "2024-06-10T19:37:21.132843Z",
     "shell.execute_reply.started": "2024-06-10T19:37:20.863196Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------------+-------+------------+\n",
      "|date      |timestamp              |date_ym|timestamp_ym|\n",
      "+----------+-----------------------+-------+------------+\n",
      "|2014-02-28|2014-02-28 10:00:00.123|201402 |201402      |\n",
      "|2016-02-29|2016-02-29 08:08:08.999|201602 |201602      |\n",
      "|2017-10-31|2017-12-31 11:59:59.123|201710 |201712      |\n",
      "|2019-11-30|2019-08-31 00:00:00.000|201911 |201908      |\n",
      "+----------+-----------------------+-------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import date_format\n",
    "datetimesDF. \\\n",
    "    withColumn('date_ym', date_format('date', 'yyyyMM')). \\\n",
    "    withColumn('timestamp_ym', date_format('timestamp', 'yyyyMM')). \\\n",
    "    show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c88d3e8f-dd3c-42da-b98c-fac1b7d1c484",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-10T19:38:39.585567Z",
     "iopub.status.busy": "2024-06-10T19:38:39.584951Z",
     "iopub.status.idle": "2024-06-10T19:38:39.615649Z",
     "shell.execute_reply": "2024-06-10T19:38:39.615114Z",
     "shell.execute_reply.started": "2024-06-10T19:38:39.585532Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- date: string (nullable = true)\n",
      " |-- timestamp: string (nullable = true)\n",
      " |-- date_ym: integer (nullable = true)\n",
      " |-- timestamp_ym: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datetimesDF. \\\n",
    "    withColumn('date_ym', date_format('date', 'yyyyMM').cast('int')). \\\n",
    "    withColumn('timestamp_ym', date_format('timestamp', 'yyyyMM').cast('int')). \\\n",
    "    printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "06313d2c-3f18-43a1-892a-7420343e39fd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-10T19:41:24.643541Z",
     "iopub.status.busy": "2024-06-10T19:41:24.642056Z",
     "iopub.status.idle": "2024-06-10T19:41:24.859989Z",
     "shell.execute_reply": "2024-06-10T19:41:24.859663Z",
     "shell.execute_reply.started": "2024-06-10T19:41:24.643487Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- date: string (nullable = true)\n",
      " |-- timestamp: string (nullable = true)\n",
      " |-- date_long: long (nullable = true)\n",
      " |-- timestamp_long: long (nullable = true)\n",
      "\n",
      "+----------+--------------------+--------------+--------------+\n",
      "|      date|           timestamp|     date_long|timestamp_long|\n",
      "+----------+--------------------+--------------+--------------+\n",
      "|2014-02-28|2014-02-28 10:00:...|20140228000000|20140228100000|\n",
      "|2016-02-29|2016-02-29 08:08:...|20160229000000|20160229080808|\n",
      "|2017-10-31|2017-12-31 11:59:...|20171031000000|20171231115959|\n",
      "|2019-11-30|2019-08-31 00:00:...|20191130000000|20190831000000|\n",
      "+----------+--------------------+--------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datetimesDF. \\\n",
    "    withColumn('date_long', date_format('date', 'yyyyMMddHHmmss').cast('long')). \\\n",
    "    withColumn('timestamp_long', date_format('timestamp', 'yyyyMMddHHmmss').cast('long')). \\\n",
    "    printSchema()\n",
    "datetimesDF. \\\n",
    "    withColumn('date_long', date_format('date', 'yyyyMMddHHmmss').cast('long')). \\\n",
    "    withColumn('timestamp_long', date_format('timestamp', 'yyyyMMddHHmmss').cast('long')). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3633fe30-2e56-4c11-b747-d539dc6d9029",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-10T19:47:22.124683Z",
     "iopub.status.busy": "2024-06-10T19:47:22.123757Z",
     "iopub.status.idle": "2024-06-10T19:47:22.737754Z",
     "shell.execute_reply": "2024-06-10T19:47:22.737407Z",
     "shell.execute_reply.started": "2024-06-10T19:47:22.124649Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------------+-------+------------+\n",
      "|date      |timestamp              |date_ym|timestamp_ym|\n",
      "+----------+-----------------------+-------+------------+\n",
      "|2014-02-28|2014-02-28 10:00:00.123|2014059|2014059     |\n",
      "|2016-02-29|2016-02-29 08:08:08.999|2016060|2016060     |\n",
      "|2017-10-31|2017-12-31 11:59:59.123|2017304|2017365     |\n",
      "|2019-11-30|2019-08-31 00:00:00.000|2019334|2019243     |\n",
      "+----------+-----------------------+-------+------------+\n",
      "\n",
      "+----------+-----------------------+-----------------+\n",
      "|date      |timestamp              |date_desc        |\n",
      "+----------+-----------------------+-----------------+\n",
      "|2014-02-28|2014-02-28 10:00:00.123|February 28, 2014|\n",
      "|2016-02-29|2016-02-29 08:08:08.999|February 29, 2016|\n",
      "|2017-10-31|2017-12-31 11:59:59.123|October 31, 2017 |\n",
      "|2019-11-30|2019-08-31 00:00:00.000|November 30, 2019|\n",
      "+----------+-----------------------+-----------------+\n",
      "\n",
      "+----------+-----------------------+--------+-------------+\n",
      "|date      |timestamp              |week_day|week_full_day|\n",
      "+----------+-----------------------+--------+-------------+\n",
      "|2014-02-28|2014-02-28 10:00:00.123|Fri     |Friday       |\n",
      "|2016-02-29|2016-02-29 08:08:08.999|Mon     |Monday       |\n",
      "|2017-10-31|2017-12-31 11:59:59.123|Tue     |Tuesday      |\n",
      "|2019-11-30|2019-08-31 00:00:00.000|Sat     |Saturday     |\n",
      "+----------+-----------------------+--------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### DDD - day of the year\n",
    "### EE - name of the week day\n",
    "\n",
    "datetimesDF. \\\n",
    "    withColumn('date_ym', date_format('date', 'yyyyDDD')). \\\n",
    "    withColumn('timestamp_ym', date_format('timestamp', 'yyyyDDD')). \\\n",
    "    show(truncate=False)\n",
    "datetimesDF. \\\n",
    "    withColumn('date_desc', date_format('date', 'MMMM d, yyyy')). \\\n",
    "    show(truncate=False)\n",
    "datetimesDF. \\\n",
    "    withColumn('week_day', date_format('date', 'EE')). \\\n",
    "    withColumn('week_full_day', date_format('date', 'EEEE')). \\\n",
    "    show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b417d886-ee08-4be4-8d39-cb28c06741e3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-10T19:50:14.352657Z",
     "iopub.status.busy": "2024-06-10T19:50:14.352065Z",
     "iopub.status.idle": "2024-06-10T19:50:14.358917Z",
     "shell.execute_reply": "2024-06-10T19:50:14.357878Z",
     "shell.execute_reply.started": "2024-06-10T19:50:14.352622Z"
    },
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Dealing with Unix Timestamp in Spark Data Frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "818ff0c3-995a-4198-a7de-7e298e88065e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-10T21:26:14.935480Z",
     "iopub.status.busy": "2024-06-10T21:26:14.934792Z",
     "iopub.status.idle": "2024-06-10T21:26:15.200442Z",
     "shell.execute_reply": "2024-06-10T21:26:15.200150Z",
     "shell.execute_reply.started": "2024-06-10T21:26:14.935428Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+-------------------+\n",
      "|dateid  |date      |timestamp          |\n",
      "+--------+----------+-------------------+\n",
      "|20140228|2014-02-28|2014-02-28 10:00:00|\n",
      "|20160229|2016-02-29|2016-02-29 08:08:08|\n",
      "|20171031|2017-10-31|2017-12-31 11:59:59|\n",
      "|20191130|2019-11-30|2019-08-31 00:00:00|\n",
      "+--------+----------+-------------------+\n",
      "\n",
      "root\n",
      " |-- dateid: long (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- timestamp: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# unix_timestamp, from_unixtime\n",
    "\n",
    "datetimes = [\n",
    "    (20140228, '2014-02-28', '2014-02-28 10:00:00'),\n",
    "    (20160229, '2016-02-29', '2016-02-29 08:08:08'),\n",
    "    (20171031, '2017-10-31', '2017-12-31 11:59:59'),\n",
    "    (20191130, '2019-11-30', '2019-08-31 00:00:00')\n",
    "]\n",
    "\n",
    "datetimesDF = spark.createDataFrame(datetimes, 'dateid bigint, date string, timestamp string')\n",
    "datetimesDF.show(truncate=False)\n",
    "datetimesDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "17a223b5-7496-488f-93b9-e5b8c781786d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-10T21:26:27.682750Z",
     "iopub.status.busy": "2024-06-10T21:26:27.681795Z",
     "iopub.status.idle": "2024-06-10T21:26:27.890314Z",
     "shell.execute_reply": "2024-06-10T21:26:27.889946Z",
     "shell.execute_reply.started": "2024-06-10T21:26:27.682700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+-------------------+-----------+----------+--------------+\n",
      "|dateid  |date      |timestamp          |unix_dateid|unix_date |unix_timestamp|\n",
      "+--------+----------+-------------------+-----------+----------+--------------+\n",
      "|20140228|2014-02-28|2014-02-28 10:00:00|1393542000 |1393542000|1393578000    |\n",
      "|20160229|2016-02-29|2016-02-29 08:08:08|1456700400 |1456700400|1456729688    |\n",
      "|20171031|2017-10-31|2017-12-31 11:59:59|1509404400 |1509404400|1514717999    |\n",
      "|20191130|2019-11-30|2019-08-31 00:00:00|1575068400 |1575068400|1567202400    |\n",
      "+--------+----------+-------------------+-----------+----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import unix_timestamp, col\n",
    "datetimesDF. \\\n",
    "    withColumn('unix_dateid', unix_timestamp(col('dateid').cast('string'), 'yyyyMMdd')). \\\n",
    "    withColumn('unix_date', unix_timestamp(col('date'), 'yyyy-MM-dd')). \\\n",
    "    withColumn('unix_timestamp', unix_timestamp(col('timestamp'))). \\\n",
    "    show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "d0c4b66d-9ef7-4b7b-b27b-03438621c633",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-10T21:33:12.924611Z",
     "iopub.status.busy": "2024-06-10T21:33:12.924084Z",
     "iopub.status.idle": "2024-06-10T21:33:13.193547Z",
     "shell.execute_reply": "2024-06-10T21:33:13.193190Z",
     "shell.execute_reply.started": "2024-06-10T21:33:12.924584Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- unixtime: long (nullable = true)\n",
      "\n",
      "+----------+\n",
      "|  unixtime|\n",
      "+----------+\n",
      "|1393561700|\n",
      "|1456713488|\n",
      "|1514701799|\n",
      "|1567189800|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "unixtimes = [\n",
    "    (1393561700, ),\n",
    "    (1456713488, ),\n",
    "    (1514701799, ),\n",
    "    (1567189800, )\n",
    "]\n",
    "\n",
    "unixtimesDF =  spark.createDataFrame(unixtimes).toDF('unixtime')\n",
    "unixtimesDF.printSchema()\n",
    "unixtimesDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "7b8f3639-1b72-40b5-b7c9-c31d54dc67ce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-10T21:40:16.162069Z",
     "iopub.status.busy": "2024-06-10T21:40:16.161545Z",
     "iopub.status.idle": "2024-06-10T21:40:16.377474Z",
     "shell.execute_reply": "2024-06-10T21:40:16.377193Z",
     "shell.execute_reply.started": "2024-06-10T21:40:16.162041Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+-------------------+\n",
      "|  unixtime|    date|               time|\n",
      "+----------+--------+-------------------+\n",
      "|1393561700|20140228|2014-02-28 05:28:20|\n",
      "|1456713488|20160229|2016-02-29 03:38:08|\n",
      "|1514701799|20171231|2017-12-31 07:29:59|\n",
      "|1567189800|20190830|2019-08-30 20:30:00|\n",
      "+----------+--------+-------------------+\n",
      "\n",
      "root\n",
      " |-- unixtime: long (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- time: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import from_unixtime, col\n",
    "unixtimesDF. \\\n",
    "    withColumn('date', from_unixtime(col('unixtime'), 'yyyyMMdd')). \\\n",
    "    withColumn('time', from_unixtime(col('unixtime'))). \\\n",
    "    show()\n",
    "unixtimesDF. \\\n",
    "    withColumn('date', from_unixtime(col('unixtime'), 'yyyyMMdd')). \\\n",
    "    withColumn('time', from_unixtime(col('unixtime'))). \\\n",
    "    printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "04caef26-4e8e-4a5f-b4de-b8ec3c46cb16",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-10T21:40:41.687440Z",
     "iopub.status.busy": "2024-06-10T21:40:41.686691Z",
     "iopub.status.idle": "2024-06-10T21:40:41.885169Z",
     "shell.execute_reply": "2024-06-10T21:40:41.884750Z",
     "shell.execute_reply.started": "2024-06-10T21:40:41.687392Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|           unixtime|\n",
      "+-------------------+\n",
      "|2014-02-28 05:28:20|\n",
      "|2016-02-29 03:38:08|\n",
      "|2017-12-31 07:29:59|\n",
      "|2019-08-30 20:30:00|\n",
      "+-------------------+\n",
      "\n",
      "root\n",
      " |-- unixtime: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "unixtimesDF.select(col('unixtime').cast('timestamp')).show()\n",
    "unixtimesDF.select(col('unixtime').cast('timestamp')).printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bfbded8-7837-49f6-8ac1-33ecb4d34559",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-10T21:46:11.321388Z",
     "iopub.status.busy": "2024-06-10T21:46:11.320795Z",
     "iopub.status.idle": "2024-06-10T21:46:11.328037Z",
     "shell.execute_reply": "2024-06-10T21:46:11.326822Z",
     "shell.execute_reply.started": "2024-06-10T21:46:11.321357Z"
    }
   },
   "source": [
    "## Dealing with NULL values in Spark Data Frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce65041f-65dc-49ab-a4df-c6d2b9526c7b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-12T21:48:25.020284Z",
     "iopub.status.busy": "2024-06-12T21:48:25.019588Z",
     "iopub.status.idle": "2024-06-12T21:48:27.824552Z",
     "shell.execute_reply": "2024-06-12T21:48:27.824281Z",
     "shell.execute_reply.started": "2024-06-12T21:48:25.020220Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+--------------+-----+----------------+-----------+\n",
      "|employee_id|first_name|last_name|salary|       country|bonus|    phone_number|        ssn|\n",
      "+-----------+----------+---------+------+--------------+-----+----------------+-----------+\n",
      "|          1|     Scott|    Tiger|1000.0| united states|   10| +1 123 456 7890|123 45 6789|\n",
      "|          2|     Henry|     Ford|1250.0|         India| NULL|+91 234 567 8901|456 78 9123|\n",
      "|          3|      Nick|   Junior| 750.0|united KINGDOM|     |+44 111 111 1111|222 33 4444|\n",
      "|          4|      Bill|    Gomes|1500.0|     AUSTRALIA|   10|+61 987 654 3210|789 12 6118|\n",
      "+-----------+----------+---------+------+--------------+-----+----------------+-----------+\n",
      "\n",
      "root\n",
      " |-- employee_id: long (nullable = true)\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- salary: double (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- bonus: string (nullable = true)\n",
      " |-- phone_number: string (nullable = true)\n",
      " |-- ssn: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employees = [\n",
    "    (1, \"Scott\", \"Tiger\", 1000.0, \"united states\", 10, \"+1 123 456 7890\", \"123 45 6789\"),\n",
    "    (2, \"Henry\", \"Ford\", 1250.0, \"India\", None, \"+91 234 567 8901\", \"456 78 9123\"),\n",
    "    (3, \"Nick\", \"Junior\", 750.0,  \"united KINGDOM\", \"\", \"+44 111 111 1111\", \"222 33 4444\"),\n",
    "    (4, \"Bill\", \"Gomes\", 1500.0, \"AUSTRALIA\", 10, \"+61 987 654 3210\", \"789 12 6118\")\n",
    "]\n",
    "employeesDF =  spark.createDataFrame(employees).toDF('employee_id', 'first_name', 'last_name', 'salary', 'country', 'bonus', 'phone_number', 'ssn')\n",
    "employeesDF.show()\n",
    "employeesDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "d72520bd-e0b6-4417-9f32-c3253d53439b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-10T22:14:23.142533Z",
     "iopub.status.busy": "2024-06-10T22:14:23.141765Z",
     "iopub.status.idle": "2024-06-10T22:14:23.362024Z",
     "shell.execute_reply": "2024-06-10T22:14:23.361706Z",
     "shell.execute_reply.started": "2024-06-10T22:14:23.142484Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+--------------+-----+----------------+-----------+------+------+------+\n",
      "|employee_id|first_name|last_name|salary|       country|bonus|    phone_number|        ssn|bonus1|bonus2|bonus3|\n",
      "+-----------+----------+---------+------+--------------+-----+----------------+-----------+------+------+------+\n",
      "|          1|     Scott|    Tiger|1000.0| united states|   10| +1 123 456 7890|123 45 6789|    10|    10|    10|\n",
      "|          2|     Henry|     Ford|1250.0|         India| NULL|+91 234 567 8901|456 78 9123|     0|     0|     0|\n",
      "|          3|      Nick|   Junior| 750.0|united KINGDOM|     |+44 111 111 1111|222 33 4444|      |     0|     0|\n",
      "|          4|      Bill|    Gomes|1500.0|     AUSTRALIA|   10|+61 987 654 3210|789 12 6118|    10|    10|    10|\n",
      "+-----------+----------+---------+------+--------------+-----+----------------+-----------+------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import coalesce, nvl\n",
    "employeesDF. \\\n",
    "    withColumn('bonus1', coalesce('bonus', lit(0))). \\\n",
    "    withColumn('bonus2', coalesce(col('bonus').cast('int'), lit(0))). \\\n",
    "    withColumn('bonus3', nvl(col('bonus').cast('int'), lit(0))). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "eef5e533-076e-41bf-b975-fdc21bd4287a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-10T22:19:25.582098Z",
     "iopub.status.busy": "2024-06-10T22:19:25.581549Z",
     "iopub.status.idle": "2024-06-10T22:19:25.857113Z",
     "shell.execute_reply": "2024-06-10T22:19:25.856771Z",
     "shell.execute_reply.started": "2024-06-10T22:19:25.582066Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+--------------+-----+----------------+-----------+------+------+------+------+\n",
      "|employee_id|first_name|last_name|salary|       country|bonus|    phone_number|        ssn|bonus1|bonus2|bonus3|bonus4|\n",
      "+-----------+----------+---------+------+--------------+-----+----------------+-----------+------+------+------+------+\n",
      "|          1|     Scott|    Tiger|1000.0| united states|   10| +1 123 456 7890|123 45 6789|    10|    10|    10|    10|\n",
      "|          2|     Henry|     Ford|1250.0|         India| NULL|+91 234 567 8901|456 78 9123|     0|     0|     0|     0|\n",
      "|          3|      Nick|   Junior| 750.0|united KINGDOM|     |+44 111 111 1111|222 33 4444|      |     0|     0|     0|\n",
      "|          4|      Bill|    Gomes|1500.0|     AUSTRALIA|   10|+61 987 654 3210|789 12 6118|    10|    10|    10|    10|\n",
      "+-----------+----------+---------+------+--------------+-----+----------------+-----------+------+------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "\n",
    "employeesDF. \\\n",
    "    withColumn('bonus1', expr(\"nvl(bonus, 0)\")). \\\n",
    "    withColumn('bonus2', expr(\"nvl(cast(bonus as int), 0)\")). \\\n",
    "    withColumn('bonus3', expr(\"nvl(nullif(bonus, ''), 0)\")). \\\n",
    "    withColumn('bonus4', expr(\"coalesce(cast(bonus as int), 0)\")). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "b4e2042f-66dc-45ed-9a14-fce47ed4ee7a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-10T22:26:45.808011Z",
     "iopub.status.busy": "2024-06-10T22:26:45.807188Z",
     "iopub.status.idle": "2024-06-10T22:26:46.016077Z",
     "shell.execute_reply": "2024-06-10T22:26:46.015747Z",
     "shell.execute_reply.started": "2024-06-10T22:26:45.807959Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+--------------+-----+----------------+-----------+-------+\n",
      "|employee_id|first_name|last_name|salary|       country|bonus|    phone_number|        ssn|payment|\n",
      "+-----------+----------+---------+------+--------------+-----+----------------+-----------+-------+\n",
      "|          1|     Scott|    Tiger|1000.0| united states|   10| +1 123 456 7890|123 45 6789| 1100.0|\n",
      "|          2|     Henry|     Ford|1250.0|         India| NULL|+91 234 567 8901|456 78 9123| 1250.0|\n",
      "|          3|      Nick|   Junior| 750.0|united KINGDOM|     |+44 111 111 1111|222 33 4444|  750.0|\n",
      "|          4|      Bill|    Gomes|1500.0|     AUSTRALIA|   10|+61 987 654 3210|789 12 6118| 1650.0|\n",
      "+-----------+----------+---------+------+--------------+-----+----------------+-----------+-------+\n",
      "\n",
      "root\n",
      " |-- employee_id: long (nullable = true)\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- salary: double (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- bonus: string (nullable = true)\n",
      " |-- phone_number: string (nullable = true)\n",
      " |-- ssn: string (nullable = true)\n",
      " |-- payment: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF. \\\n",
    "    withColumn('payment', col('salary')+(col('salary')*coalesce(col('bonus').cast('int'), lit(0))/100)). \\\n",
    "    show()\n",
    "employeesDF. \\\n",
    "    withColumn('payment', col('salary')+(col('salary')*coalesce(col('bonus').cast('int'), lit(0))/100)). \\\n",
    "    printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "3058bf22-4ea3-45e7-ab06-e2934c7e47d5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-10T22:30:10.475062Z",
     "iopub.status.busy": "2024-06-10T22:30:10.474547Z",
     "iopub.status.idle": "2024-06-10T22:30:10.739155Z",
     "shell.execute_reply": "2024-06-10T22:30:10.738821Z",
     "shell.execute_reply.started": "2024-06-10T22:30:10.475032Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+--------------+-----+----------------+-----------+\n",
      "|employee_id|first_name|last_name|salary|       country|bonus|    phone_number|        ssn|\n",
      "+-----------+----------+---------+------+--------------+-----+----------------+-----------+\n",
      "|          1|     Scott|     NULL|1000.0| united states|   10| +1 123 456 7890|123 45 6789|\n",
      "|          2|     Henry|     Ford|1250.0|         India| NULL|+91 234 567 8901|456 78 9123|\n",
      "|          3|      Nick|     NULL|  NULL|united KINGDOM|     |+44 111 111 1111|222 33 4444|\n",
      "|          4|      Bill|    Gomes|1500.0|     AUSTRALIA|   10|+61 987 654 3210|789 12 6118|\n",
      "+-----------+----------+---------+------+--------------+-----+----------------+-----------+\n",
      "\n",
      "root\n",
      " |-- employee_id: long (nullable = true)\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- salary: double (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- bonus: string (nullable = true)\n",
      " |-- phone_number: string (nullable = true)\n",
      " |-- ssn: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### bulk NULLs\n",
    "### na, fillna, replacena, dropna\n",
    "employees = [\n",
    "    (1, \"Scott\", None, 1000.0, \"united states\", 10, \"+1 123 456 7890\", \"123 45 6789\"),\n",
    "    (2, \"Henry\", \"Ford\", 1250.0, \"India\", None, \"+91 234 567 8901\", \"456 78 9123\"),\n",
    "    (3, \"Nick\", None, None, \"united KINGDOM\", \"\", \"+44 111 111 1111\", \"222 33 4444\"),\n",
    "    (4, \"Bill\", \"Gomes\", 1500.0, \"AUSTRALIA\", 10, \"+61 987 654 3210\", \"789 12 6118\")\n",
    "]\n",
    "employeesDF = spark.createDataFrame(employees).toDF('employee_id', 'first_name', 'last_name', 'salary', 'country', 'bonus', 'phone_number', 'ssn')\n",
    "employeesDF.show()\n",
    "employeesDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "d6cc240f-e0ce-4c4b-a1bc-ae06b0d044fa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-10T22:35:37.393851Z",
     "iopub.status.busy": "2024-06-10T22:35:37.393001Z",
     "iopub.status.idle": "2024-06-10T22:35:38.214619Z",
     "shell.execute_reply": "2024-06-10T22:35:38.214130Z",
     "shell.execute_reply.started": "2024-06-10T22:35:37.393803Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+--------------+-----+----------------+-----------+\n",
      "|employee_id|first_name|last_name|salary|       country|bonus|    phone_number|        ssn|\n",
      "+-----------+----------+---------+------+--------------+-----+----------------+-----------+\n",
      "|          1|     Scott|     NULL|1000.0| united states|   10| +1 123 456 7890|123 45 6789|\n",
      "|          2|     Henry|     Ford|1250.0|         India| NULL|+91 234 567 8901|456 78 9123|\n",
      "|          3|      Nick|     NULL|   0.0|united KINGDOM|     |+44 111 111 1111|222 33 4444|\n",
      "|          4|      Bill|    Gomes|1500.0|     AUSTRALIA|   10|+61 987 654 3210|789 12 6118|\n",
      "+-----------+----------+---------+------+--------------+-----+----------------+-----------+\n",
      "\n",
      "+-----------+----------+---------+------+--------------+-----+----------------+-----------+\n",
      "|employee_id|first_name|last_name|salary|       country|bonus|    phone_number|        ssn|\n",
      "+-----------+----------+---------+------+--------------+-----+----------------+-----------+\n",
      "|          1|     Scott|       na|1000.0| united states|   10| +1 123 456 7890|123 45 6789|\n",
      "|          2|     Henry|     Ford|1250.0|         India|   na|+91 234 567 8901|456 78 9123|\n",
      "|          3|      Nick|       na|  NULL|united KINGDOM|     |+44 111 111 1111|222 33 4444|\n",
      "|          4|      Bill|    Gomes|1500.0|     AUSTRALIA|   10|+61 987 654 3210|789 12 6118|\n",
      "+-----------+----------+---------+------+--------------+-----+----------------+-----------+\n",
      "\n",
      "+-----------+----------+---------+------+--------------+-----+----------------+-----------+\n",
      "|employee_id|first_name|last_name|salary|       country|bonus|    phone_number|        ssn|\n",
      "+-----------+----------+---------+------+--------------+-----+----------------+-----------+\n",
      "|          1|     Scott|       na|1000.0| united states|   10| +1 123 456 7890|123 45 6789|\n",
      "|          2|     Henry|     Ford|1250.0|         India|   na|+91 234 567 8901|456 78 9123|\n",
      "|          3|      Nick|       na|   0.0|united KINGDOM|     |+44 111 111 1111|222 33 4444|\n",
      "|          4|      Bill|    Gomes|1500.0|     AUSTRALIA|   10|+61 987 654 3210|789 12 6118|\n",
      "+-----------+----------+---------+------+--------------+-----+----------------+-----------+\n",
      "\n",
      "+-----------+----------+---------+------+--------------+-----+----------------+-----------+\n",
      "|employee_id|first_name|last_name|salary|       country|bonus|    phone_number|        ssn|\n",
      "+-----------+----------+---------+------+--------------+-----+----------------+-----------+\n",
      "|          1|     Scott|       na|1000.0| united states|   10| +1 123 456 7890|123 45 6789|\n",
      "|          2|     Henry|     Ford|1250.0|         India| NULL|+91 234 567 8901|456 78 9123|\n",
      "|          3|      Nick|       na|   0.0|united KINGDOM|     |+44 111 111 1111|222 33 4444|\n",
      "|          4|      Bill|    Gomes|1500.0|     AUSTRALIA|   10|+61 987 654 3210|789 12 6118|\n",
      "+-----------+----------+---------+------+--------------+-----+----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF.fillna(0.0).show()\n",
    "employeesDF.fillna('na').show()\n",
    "employeesDF.fillna(0.0).fillna('na').show()\n",
    "employeesDF.fillna(0.0, 'salary').fillna('na', 'last_name').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a582a3-927b-4998-b312-98c32d59849b",
   "metadata": {},
   "source": [
    "## Using CASE and WHEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bdb28073-fd45-489c-a270-4da9f062e65f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-12T21:50:49.881269Z",
     "iopub.status.busy": "2024-06-12T21:50:49.880481Z",
     "iopub.status.idle": "2024-06-12T21:50:50.247312Z",
     "shell.execute_reply": "2024-06-12T21:50:50.247033Z",
     "shell.execute_reply.started": "2024-06-12T21:50:49.881216Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+--------------+-----+----------------+-----------+------+\n",
      "|employee_id|first_name|last_name|salary|       country|bonus|    phone_number|        ssn|bonus1|\n",
      "+-----------+----------+---------+------+--------------+-----+----------------+-----------+------+\n",
      "|          1|     Scott|    Tiger|1000.0| united states|   10| +1 123 456 7890|123 45 6789|    10|\n",
      "|          2|     Henry|     Ford|1250.0|         India| NULL|+91 234 567 8901|456 78 9123|     0|\n",
      "|          3|      Nick|   Junior| 750.0|united KINGDOM|     |+44 111 111 1111|222 33 4444|     0|\n",
      "|          4|      Bill|    Gomes|1500.0|     AUSTRALIA|   10|+61 987 654 3210|789 12 6118|    10|\n",
      "+-----------+----------+---------+------+--------------+-----+----------------+-----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, lit, cast, coalesce\n",
    "employeesDF. \\\n",
    "    withColumn('bonus1', coalesce(col('bonus').cast('int'), lit(0))). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a4175448-24aa-442c-bff0-f8f3dcb11b61",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-12T21:54:59.183554Z",
     "iopub.status.busy": "2024-06-12T21:54:59.182736Z",
     "iopub.status.idle": "2024-06-12T21:54:59.546600Z",
     "shell.execute_reply": "2024-06-12T21:54:59.546247Z",
     "shell.execute_reply.started": "2024-06-12T21:54:59.183505Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+--------------+-----+----------------+-----------+------+\n",
      "|employee_id|first_name|last_name|salary|       country|bonus|    phone_number|        ssn|bonus1|\n",
      "+-----------+----------+---------+------+--------------+-----+----------------+-----------+------+\n",
      "|          1|     Scott|    Tiger|1000.0| united states|   10| +1 123 456 7890|123 45 6789|    10|\n",
      "|          2|     Henry|     Ford|1250.0|         India| NULL|+91 234 567 8901|456 78 9123|     0|\n",
      "|          3|      Nick|   Junior| 750.0|united KINGDOM|     |+44 111 111 1111|222 33 4444|     0|\n",
      "|          4|      Bill|    Gomes|1500.0|     AUSTRALIA|   10|+61 987 654 3210|789 12 6118|    10|\n",
      "+-----------+----------+---------+------+--------------+-----+----------------+-----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "employeesDF. \\\n",
    "    withColumn('bonus1', expr(\"case when bonus is null or bonus = '' then 0 else bonus end\")).\\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e12362f8-b0d8-4a8b-af50-030982f8f191",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-12T22:03:43.983619Z",
     "iopub.status.busy": "2024-06-12T22:03:43.982944Z",
     "iopub.status.idle": "2024-06-12T22:03:44.223086Z",
     "shell.execute_reply": "2024-06-12T22:03:44.222790Z",
     "shell.execute_reply.started": "2024-06-12T22:03:43.983581Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+--------------+-----+----------------+-----------+------+\n",
      "|employee_id|first_name|last_name|salary|       country|bonus|    phone_number|        ssn|bonus1|\n",
      "+-----------+----------+---------+------+--------------+-----+----------------+-----------+------+\n",
      "|          1|     Scott|    Tiger|1000.0| united states|   10| +1 123 456 7890|123 45 6789| bonus|\n",
      "|          2|     Henry|     Ford|1250.0|         India| NULL|+91 234 567 8901|456 78 9123|     0|\n",
      "|          3|      Nick|   Junior| 750.0|united KINGDOM|     |+44 111 111 1111|222 33 4444|     0|\n",
      "|          4|      Bill|    Gomes|1500.0|     AUSTRALIA|   10|+61 987 654 3210|789 12 6118| bonus|\n",
      "+-----------+----------+---------+------+--------------+-----+----------------+-----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import when\n",
    "employeesDF. \\\n",
    "    withColumn('bonus1', when((col('bonus').isNull()) | (col('bonus') == lit('')), 0).otherwise('bonus')). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ebaafd3f-58e3-4091-882b-ab4e616ac438",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-12T22:08:42.961192Z",
     "iopub.status.busy": "2024-06-12T22:08:42.960336Z",
     "iopub.status.idle": "2024-06-12T22:08:43.259772Z",
     "shell.execute_reply": "2024-06-12T22:08:43.259442Z",
     "shell.execute_reply.started": "2024-06-12T22:08:42.961140Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "| id|age|\n",
      "+---+---+\n",
      "|  1|  1|\n",
      "|  2| 13|\n",
      "|  3| 18|\n",
      "|  4| 60|\n",
      "|  5|120|\n",
      "|  6|  0|\n",
      "|  7| 12|\n",
      "|  8|160|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "persons = [\n",
    "(1, 1),\n",
    "(2, 13),\n",
    "(3, 18),\n",
    "(4, 60),\n",
    "(5, 120),\n",
    "(6, 0),\n",
    "(7, 12),\n",
    "(8, 160),\n",
    "]\n",
    "personsDF = spark.createDataFrame(persons, schema='id INT, age INT')\n",
    "personsDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e847f070-33de-4206-a017-e34e63c02d99",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-12T22:15:36.210081Z",
     "iopub.status.busy": "2024-06-12T22:15:36.209299Z",
     "iopub.status.idle": "2024-06-12T22:15:36.435923Z",
     "shell.execute_reply": "2024-06-12T22:15:36.435498Z",
     "shell.execute_reply.started": "2024-06-12T22:15:36.210034Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-----------------+\n",
      "| id|age|         category|\n",
      "+---+---+-----------------+\n",
      "|  1|  1|         New Born|\n",
      "|  2| 13|          Toddler|\n",
      "|  3| 18|          Toddler|\n",
      "|  4| 60|              Kid|\n",
      "|  5|120|              Kid|\n",
      "|  6|  0|         New Born|\n",
      "|  7| 12|           Infant|\n",
      "|  8|160|Teenager or Adult|\n",
      "+---+---+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "personsDF. \\\n",
    "    withColumn('category', expr(\"\"\"\n",
    "                                case \n",
    "                                when age between 0 and 2 then 'New Born'\n",
    "                                when age > 2 and age <= 12 then 'Infant'\n",
    "                                when age >12 and age <= 48 then 'Toddler'\n",
    "                                when age >48 and age <= 144 then 'Kid'\n",
    "                                else 'Teenager or Adult'\n",
    "                                end\n",
    "                                \"\"\")). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ed3f2932-aaec-48ec-a449-087ad9b083d5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-12T22:24:08.330285Z",
     "iopub.status.busy": "2024-06-12T22:24:08.329387Z",
     "iopub.status.idle": "2024-06-12T22:24:08.556251Z",
     "shell.execute_reply": "2024-06-12T22:24:08.555864Z",
     "shell.execute_reply.started": "2024-06-12T22:24:08.330255Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-----------------+\n",
      "| id|age|         category|\n",
      "+---+---+-----------------+\n",
      "|  1|  1|         New Born|\n",
      "|  2| 13|          Toddler|\n",
      "|  3| 18|          Toddler|\n",
      "|  4| 60|              Kid|\n",
      "|  5|120|              Kid|\n",
      "|  6|  0|         New Born|\n",
      "|  7| 12|           Infant|\n",
      "|  8|160|Teenager or Adult|\n",
      "+---+---+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "personsDF. \\\n",
    "    withColumn('category', \n",
    "               when(col('age').between(0, 2), 'New Born').\n",
    "               when((col('age') > 2) & (col('age') <= 12), 'Infant').\n",
    "               when((col('age') > 12) & (col('age') <= 48), 'Toddler').\n",
    "               when((col('age') > 48) & (col('age') <= 144), 'Kid').\n",
    "               otherwise('Teenager or Adult')\n",
    "    ). \\\n",
    "    show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

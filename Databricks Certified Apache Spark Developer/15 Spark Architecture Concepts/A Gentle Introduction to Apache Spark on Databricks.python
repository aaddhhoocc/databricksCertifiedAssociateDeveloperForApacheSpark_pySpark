{"version":"NotebookV1","origId":1575837098434218,"name":"A Gentle Introduction to Apache Spark on Databricks","language":"python","commands":[{"version":"CommandV1","origId":1575837098434219,"guid":"142d095e-1b7b-427c-8ed2-2a8caa4fe044","subtype":"command","commandType":"auto","position":6.0,"command":"%md \n\n## Apache Spark Architecture\n\nBefore proceeding with our example, let's see an overview of the Apache Spark architecture. As mentioned before, Apache Spark allows you to treat many machines as one machine and this is done via a master-worker type architecture where there is a `driver` or master node in the cluster, accompanied by `worker` nodes. The master sends work to the workers and either instructs them to pull to data from memory or from disk (or from another data source like S3 or Redshift).\n\nThe diagram below shows an example Apache Spark cluster, basically there exists a Driver node that communicates with executor nodes. Each of these executor nodes have slots which are logically like execution cores. \n\n![spark-architecture](http://training.databricks.com/databricks_guide/gentle_introduction/videoss_logo.png)\n\nThe Driver sends Tasks to the empty slots on the Executors when work has to be done:\n\n![spark-architecture](http://training.databricks.com/databricks_guide/gentle_introduction/spark_cluster_tasks.png)\n\nNote: In the case of the Community Edition there is no Worker, and the Master, not shown in the figure, executes the entire code.\n\n![spark-architecture](http://training.databricks.com/databricks_guide/gentle_introduction/notebook_microcluster.png)\n\nYou can view the details of your Apache Spark application in the Apache Spark web UI.  The web UI is accessible in Databricks by going to \"Clusters\" and then clicking on the \"View Spark UI\" link for your cluster, it is also available by clicking at the top left of this notebook where you would select the cluster to attach this notebook to. In this option will be a link to the Apache Spark Web UI.\n\nAt a high level, every Apache Spark application consists of a driver program that launches various parallel operations on executor Java Virtual Machines (JVMs) running either in a cluster or locally on the same machine. In Databricks, the notebook interface is the driver program.  This driver program contains the main loop for the program and creates distributed datasets on the cluster, then applies operations (transformations & actions) to those datasets.\nDriver programs access Apache Spark through a `SparkSession` object regardless of deployment location.","commandVersion":2,"state":"finished","results":null,"errorSummary":null,"errorTraceType":null,"error":null,"workflows":[],"startTime":0,"submitTime":1587585610951,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"useConsistentColors":false,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","latestUserId":null,"commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"isLockedInExamMode":false,"iPythonMetadata":null,"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"tableResultSubCmdIds":[],"tableResultIndex":null,"nuid":"7b17836c-d207-4d7d-9f2d-f361beb03fb8"}],"dashboards":[],"guid":"1279f939-d3ea-4402-b74f-549fef9828b2","globalVars":{},"iPythonMetadata":null,"inputWidgets":{},"notebookMetadata":{"pythonIndentUnit":2}}
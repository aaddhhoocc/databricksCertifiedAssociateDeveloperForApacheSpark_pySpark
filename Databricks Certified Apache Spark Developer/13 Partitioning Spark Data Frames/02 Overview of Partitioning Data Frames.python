{"version":"NotebookV1","origId":1575837098434904,"name":"02 Overview of Partitioning Data Frames","language":"python","commands":[{"version":"CommandV1","origId":1575837098434905,"guid":"ace78c10-eb6b-4813-8e2b-874b518a6318","subtype":"command","commandType":"auto","position":1.0,"command":"df = spark.read.json('/public/retail_db_json/orders')","commandVersion":16,"state":"finished","results":{"type":"listResults","data":[{"type":"html","data":"<div class=\"ansiout\"></div>","arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[],"metadata":{}}],"arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[{"name":"df","typeStr":"pyspark.sql.dataframe.DataFrame","schema":{"fields":[{"metadata":{},"name":"order_customer_id","nullable":true,"type":"long"},{"metadata":{},"name":"order_date","nullable":true,"type":"string"},{"metadata":{},"name":"order_id","nullable":true,"type":"long"},{"metadata":{},"name":"order_status","nullable":true,"type":"string"}],"type":"struct"},"tableIdentifier":null}],"metadata":{}},"errorSummary":null,"errorTraceType":null,"error":null,"workflows":[],"startTime":1641773440358,"submitTime":1641773278128,"finishTime":1641773454780,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"useConsistentColors":false,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","latestUserId":null,"commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"isLockedInExamMode":false,"iPythonMetadata":null,"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"tableResultSubCmdIds":[],"tableResultIndex":null,"nuid":"96b0b639-f11d-4248-9d73-feaf46fcd9dd"},{"version":"CommandV1","origId":1575837098434906,"guid":"51c3143f-0ceb-4fdf-a693-fc32f7e73985","subtype":"command","commandType":"auto","position":2.0,"command":"df.write","commandVersion":7,"state":"finished","results":{"type":"listResults","data":[{"type":"html","data":"<div class=\"ansiout\">Out[3]: &lt;pyspark.sql.readwriter.DataFrameWriter at 0x7f67b6f58df0&gt;</div>","arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[],"metadata":{}}],"arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[],"metadata":{}},"errorSummary":null,"errorTraceType":null,"error":null,"workflows":[],"startTime":1641774364904,"submitTime":1641774364867,"finishTime":1641774364963,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"useConsistentColors":false,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","latestUserId":null,"commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"isLockedInExamMode":false,"iPythonMetadata":null,"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"tableResultSubCmdIds":[],"tableResultIndex":null,"nuid":"3a97d1f2-ab5c-4d7b-bbc2-4c8f9231ee35"},{"version":"CommandV1","origId":1575837098434907,"guid":"035d881e-bfec-4af1-a8e5-1c18b7ae3192","subtype":"command","commandType":"auto","position":3.0,"command":"help(df.write.partitionBy)","commandVersion":5,"state":"finished","results":{"type":"listResults","data":[{"type":"html","data":"<div class=\"ansiout\">Help on method partitionBy in module pyspark.sql.readwriter:\n\npartitionBy(*cols) method of pyspark.sql.readwriter.DataFrameWriter instance\n    Partitions the output by the given columns on the file system.\n    \n    If specified, the output is laid out on the file system similar\n    to Hive&#39;s partitioning scheme.\n    \n    .. versionadded:: 1.4.0\n    \n    Parameters\n    ----------\n    cols : str or list\n        name of columns\n    \n    Examples\n    --------\n    &gt;&gt;&gt; df.write.partitionBy(&#39;year&#39;, &#39;month&#39;).parquet(os.path.join(tempfile.mkdtemp(), &#39;data&#39;))\n\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[],"metadata":{}}],"arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[],"metadata":{}},"errorSummary":null,"errorTraceType":null,"error":null,"workflows":[],"startTime":1641774373383,"submitTime":1641774373351,"finishTime":1641774373418,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"useConsistentColors":false,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","latestUserId":null,"commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"isLockedInExamMode":false,"iPythonMetadata":null,"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"tableResultSubCmdIds":[],"tableResultIndex":null,"nuid":"874f135e-a0a7-4c64-b434-8083d0e601cc"},{"version":"CommandV1","origId":1575837098434908,"guid":"b1b1dbb7-68e7-4116-a4ce-62f4a13035e0","subtype":"command","commandType":"auto","position":3.5,"command":"# json does not have keyword argument related to partitioning\nhelp(df.write.json)","commandVersion":18,"state":"finished","results":{"type":"listResults","data":[{"type":"html","data":"<div class=\"ansiout\">Help on method json in module pyspark.sql.readwriter:\n\njson(path, mode=None, compression=None, dateFormat=None, timestampFormat=None, lineSep=None, encoding=None, ignoreNullFields=None) method of pyspark.sql.readwriter.DataFrameWriter instance\n    Saves the content of the :class:`DataFrame` in JSON format\n    (`JSON Lines text format or newline-delimited JSON &lt;http://jsonlines.org/&gt;`_) at the\n    specified path.\n    \n    .. versionadded:: 1.4.0\n    \n    Parameters\n    ----------\n    path : str\n        the path in any Hadoop supported file system\n    mode : str, optional\n        specifies the behavior of the save operation when data already exists.\n    \n        * ``append``: Append contents of this :class:`DataFrame` to existing data.\n        * ``overwrite``: Overwrite existing data.\n        * ``ignore``: Silently ignore this operation if data already exists.\n        * ``error`` or ``errorifexists`` (default case): Throw an exception if data already                 exists.\n    compression : str, optional\n        compression codec to use when saving to file. This can be one of the\n        known case-insensitive shorten names (none, bzip2, gzip, lz4,\n        snappy and deflate).\n    dateFormat : str, optional\n        sets the string that indicates a date format. Custom date formats\n        follow the formats at\n        `datetime pattern &lt;https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html&gt;`_.  # noqa\n        This applies to date type. If None is set, it uses the\n        default value, ``yyyy-MM-dd``.\n    timestampFormat : str, optional\n        sets the string that indicates a timestamp format.\n        Custom date formats follow the formats at\n        `datetime pattern &lt;https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html&gt;`_.  # noqa\n        This applies to timestamp type. If None is set, it uses the\n        default value, ``yyyy-MM-dd&#39;T&#39;HH:mm:ss[.SSS][XXX]``.\n    encoding : str, optional\n        specifies encoding (charset) of saved json files. If None is set,\n        the default UTF-8 charset will be used.\n    lineSep : str, optional\n        defines the line separator that should be used for writing. If None is\n        set, it uses the default value, ``\\n``.\n    ignoreNullFields : str or bool, optional\n        Whether to ignore null fields when generating JSON objects.\n        If None is set, it uses the default value, ``true``.\n    \n    Examples\n    --------\n    &gt;&gt;&gt; df.write.json(os.path.join(tempfile.mkdtemp(), &#39;data&#39;))\n\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[],"metadata":{}}],"arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[],"metadata":{}},"errorSummary":null,"errorTraceType":null,"error":null,"workflows":[],"startTime":1641774416911,"submitTime":1641774416881,"finishTime":1641774416940,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"useConsistentColors":false,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","latestUserId":null,"commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"isLockedInExamMode":false,"iPythonMetadata":null,"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"tableResultSubCmdIds":[],"tableResultIndex":null,"nuid":"0fa0c6ee-4cff-4d7c-8add-09f17c04da82"},{"version":"CommandV1","origId":1575837098434909,"guid":"1a602aa9-a0cf-44dc-a33b-1be173846869","subtype":"command","commandType":"auto","position":4.0,"command":"# parquet have keyword argument partitionBy\nhelp(df.write.parquet)","commandVersion":15,"state":"finished","results":{"type":"listResults","data":[{"type":"html","data":"<div class=\"ansiout\">Help on method parquet in module pyspark.sql.readwriter:\n\nparquet(path, mode=None, partitionBy=None, compression=None) method of pyspark.sql.readwriter.DataFrameWriter instance\n    Saves the content of the :class:`DataFrame` in Parquet format at the specified path.\n    \n    .. versionadded:: 1.4.0\n    \n    Parameters\n    ----------\n    path : str\n        the path in any Hadoop supported file system\n    mode : str, optional\n        specifies the behavior of the save operation when data already exists.\n    \n        * ``append``: Append contents of this :class:`DataFrame` to existing data.\n        * ``overwrite``: Overwrite existing data.\n        * ``ignore``: Silently ignore this operation if data already exists.\n        * ``error`` or ``errorifexists`` (default case): Throw an exception if data already                 exists.\n    partitionBy : str or list, optional\n        names of partitioning columns\n    compression : str, optional\n        compression codec to use when saving to file. This can be one of the\n        known case-insensitive shorten names (none, uncompressed, snappy, gzip,\n        lzo, brotli, lz4, and zstd). This will override\n        ``spark.sql.parquet.compression.codec``. If None is set, it uses the\n        value specified in ``spark.sql.parquet.compression.codec``.\n    \n    Examples\n    --------\n    &gt;&gt;&gt; df.write.parquet(os.path.join(tempfile.mkdtemp(), &#39;data&#39;))\n\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[],"metadata":{}}],"arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[],"metadata":{}},"errorSummary":null,"errorTraceType":null,"error":null,"workflows":[],"startTime":1641774405854,"submitTime":1641774405824,"finishTime":1641774405892,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"useConsistentColors":false,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","latestUserId":null,"commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"isLockedInExamMode":false,"iPythonMetadata":null,"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"tableResultSubCmdIds":[],"tableResultIndex":null,"nuid":"14574b5b-91f0-43fd-8e87-1c909238ca82"},{"version":"CommandV1","origId":1575837098434910,"guid":"045e2056-d7b0-4786-901b-02cb732e387e","subtype":"command","commandType":"auto","position":5.0,"command":"","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"errorTraceType":null,"error":null,"workflows":[],"startTime":0,"submitTime":0,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"useConsistentColors":false,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","latestUserId":null,"commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"isLockedInExamMode":false,"iPythonMetadata":null,"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"tableResultSubCmdIds":[],"tableResultIndex":null,"nuid":"38156da4-59a4-46fa-b575-c11f0d4d10ea"}],"dashboards":[],"guid":"b4377ee0-9581-4f88-92fb-37e58155f14b","globalVars":{},"iPythonMetadata":null,"inputWidgets":{},"notebookMetadata":{"pythonIndentUnit":4}}
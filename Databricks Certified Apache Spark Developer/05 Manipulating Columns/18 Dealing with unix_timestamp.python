{"version":"NotebookV1","origId":1575837098435414,"name":"18 Dealing with unix_timestamp","language":"python","commands":[{"version":"CommandV1","origId":1575837098435415,"guid":"0286401b-9053-4635-b25b-ad90c4d303e9","subtype":"command","commandType":"auto","position":1.0,"command":"%md\n## Dealing with Unix Timestamp\n\nLet us understand how to deal with Unix Timestamp in Spark.","commandVersion":1,"state":"finished","results":{"type":"raw","data":"","arguments":{},"metadata":{}},"errorSummary":null,"errorTraceType":null,"error":null,"workflows":[],"startTime":0,"submitTime":0,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":[],"yColumns":[],"pivotColumns":[],"pivotAggregation":null,"useConsistentColors":false,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","latestUserId":null,"commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"isLockedInExamMode":false,"iPythonMetadata":null,"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"tableResultSubCmdIds":[],"tableResultIndex":null,"nuid":"89d4d145-c1d3-4d24-886e-b9714800fb31"},{"version":"CommandV1","origId":1575837098435416,"guid":"3503cfde-6305-445a-a744-89c8749cf1e7","subtype":"command","commandType":"auto","position":3.0,"command":"%md\n* It is an integer and started from January 1st 1970 Midnight UTC.\n* Beginning time is also known as epoch and is incremented by 1 every second.\n* We can convert Unix Timestamp to regular date or timestamp and vice versa.\n* We can use `unix_timestamp` to convert regular date or timestamp to a unix timestamp value. For example `unix_timestamp(lit(\"2019-11-19 00:00:00\"))`\n* We can use `from_unixtime` to convert unix timestamp to regular date or timestamp. For example `from_unixtime(lit(1574101800))`\n* We can also pass format to both the functions.","commandVersion":1,"state":"finished","results":{"type":"raw","data":"","arguments":{},"metadata":{}},"errorSummary":null,"errorTraceType":null,"error":null,"workflows":[],"startTime":0,"submitTime":0,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":[],"yColumns":[],"pivotColumns":[],"pivotAggregation":null,"useConsistentColors":false,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","latestUserId":null,"commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"isLockedInExamMode":false,"iPythonMetadata":null,"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"tableResultSubCmdIds":[],"tableResultIndex":null,"nuid":"18561b07-9f5c-442b-9623-ff9f683ce1f7"},{"version":"CommandV1","origId":1575837098435417,"guid":"7195acc6-37eb-42cd-9acd-6df7bd60b4e1","subtype":"command","commandType":"auto","position":7.0,"command":"%md\n### Tasks\n\nLet us perform few tasks to understand how to deal with Unix Timestamp.\n\n*   Create a Dataframe by name datetimesDF with columns dateid, date and time.","commandVersion":1,"state":"finished","results":{"type":"raw","data":"","arguments":{},"metadata":{}},"errorSummary":null,"errorTraceType":null,"error":null,"workflows":[],"startTime":0,"submitTime":0,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":[],"yColumns":[],"pivotColumns":[],"pivotAggregation":null,"useConsistentColors":false,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","latestUserId":null,"commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"isLockedInExamMode":false,"iPythonMetadata":null,"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"tableResultSubCmdIds":[],"tableResultIndex":null,"nuid":"b2193bea-127a-4e8b-a53f-37e6d8e2b314"},{"version":"CommandV1","origId":1575837098435418,"guid":"a78c55e9-ce00-4332-a96d-444a55afdb8c","subtype":"command","commandType":"auto","position":8.0,"command":"datetimes = [(20140228, \"2014-02-28\", \"2014-02-28 10:00:00.123\"),\n                     (20160229, \"2016-02-29\", \"2016-02-29 08:08:08.999\"),\n                     (20171031, \"2017-10-31\", \"2017-12-31 11:59:59.123\"),\n                     (20191130, \"2019-11-30\", \"2019-08-31 00:00:00.000\")\n                ]","commandVersion":1,"state":"finished","results":{"type":"listResults","data":[{"type":"html","data":"<div class=\"ansiout\"></div>","arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[],"metadata":{}}],"arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[],"metadata":{}},"errorSummary":null,"errorTraceType":null,"error":null,"workflows":[],"startTime":1641160196352,"submitTime":1641160196312,"finishTime":1641160196384,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":[],"yColumns":[],"pivotColumns":[],"pivotAggregation":null,"useConsistentColors":false,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","latestUserId":null,"commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"isLockedInExamMode":false,"iPythonMetadata":{"pycharm":{"name":"#%%\n"}},"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"tableResultSubCmdIds":[],"tableResultIndex":null,"nuid":"ce134558-0e27-4417-bc1a-28e8fccbb1de"},{"version":"CommandV1","origId":1575837098435419,"guid":"2afc56ce-9e43-4440-96eb-309026384b5a","subtype":"command","commandType":"auto","position":9.0,"command":"datetimesDF = spark.createDataFrame(datetimes).toDF(\"dateid\", \"date\", \"time\")","commandVersion":1,"state":"finished","results":{"type":"listResults","data":[{"type":"html","data":"<div class=\"ansiout\"></div>","arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[],"metadata":{}}],"arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[{"name":"datetimesDF","typeStr":"pyspark.sql.dataframe.DataFrame","schema":{"fields":[{"metadata":{},"name":"dateid","nullable":true,"type":"long"},{"metadata":{},"name":"date","nullable":true,"type":"string"},{"metadata":{},"name":"time","nullable":true,"type":"string"}],"type":"struct"},"tableIdentifier":null}],"metadata":{}},"errorSummary":null,"errorTraceType":null,"error":null,"workflows":[],"startTime":1641160196663,"submitTime":1641160196640,"finishTime":1641160199486,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":[],"yColumns":[],"pivotColumns":[],"pivotAggregation":null,"useConsistentColors":false,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","latestUserId":null,"commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"isLockedInExamMode":false,"iPythonMetadata":{"pycharm":{"name":"#%%\n"}},"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"tableResultSubCmdIds":[],"tableResultIndex":null,"nuid":"bbb8eaf9-4667-4b9e-9bbb-a55e352e3b99"},{"version":"CommandV1","origId":1575837098435420,"guid":"72240d0b-4116-4c0c-814d-7c7075921f4e","subtype":"command","commandType":"auto","position":10.0,"command":"datetimesDF.show(truncate=False)","commandVersion":1,"state":"finished","results":{"type":"listResults","data":[{"type":"html","data":"<div class=\"ansiout\">+--------+----------+-----------------------+\n|dateid  |date      |time                   |\n+--------+----------+-----------------------+\n|20140228|2014-02-28|2014-02-28 10:00:00.123|\n|20160229|2016-02-29|2016-02-29 08:08:08.999|\n|20171031|2017-10-31|2017-12-31 11:59:59.123|\n|20191130|2019-11-30|2019-08-31 00:00:00.000|\n+--------+----------+-----------------------+\n\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[],"metadata":{}}],"arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[],"metadata":{}},"errorSummary":null,"errorTraceType":null,"error":null,"workflows":[],"startTime":1641160199492,"submitTime":1641160196952,"finishTime":1641160200302,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":[],"yColumns":[],"pivotColumns":[],"pivotAggregation":null,"useConsistentColors":false,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","latestUserId":null,"commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"isLockedInExamMode":false,"iPythonMetadata":{"pycharm":{"name":"#%%\n"}},"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"tableResultSubCmdIds":[],"tableResultIndex":null,"nuid":"b6cf4078-6ab3-4a80-9a9c-88207455051f"},{"version":"CommandV1","origId":1575837098435421,"guid":"6e49526e-6d53-4607-8d26-cb5404593e11","subtype":"command","commandType":"auto","position":11.0,"command":"%md\n* Get unix timestamp for dateid, date and time.","commandVersion":1,"state":"finished","results":{"type":"raw","data":"","arguments":{},"metadata":{}},"errorSummary":null,"errorTraceType":null,"error":null,"workflows":[],"startTime":0,"submitTime":0,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":[],"yColumns":[],"pivotColumns":[],"pivotAggregation":null,"useConsistentColors":false,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","latestUserId":null,"commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"isLockedInExamMode":false,"iPythonMetadata":null,"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"tableResultSubCmdIds":[],"tableResultIndex":null,"nuid":"2c95ccbe-30af-425f-9562-c0f07796d92f"},{"version":"CommandV1","origId":1575837098435422,"guid":"c95506c2-83de-444d-99f6-d22d79174e8f","subtype":"command","commandType":"auto","position":12.0,"command":"from pyspark.sql.functions import unix_timestamp, col","commandVersion":1,"state":"finished","results":{"type":"listResults","data":[{"type":"html","data":"<div class=\"ansiout\"></div>","arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[],"metadata":{}}],"arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[],"metadata":{}},"errorSummary":null,"errorTraceType":null,"error":null,"workflows":[],"startTime":1641160200314,"submitTime":1641160197362,"finishTime":1641160200336,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":[],"yColumns":[],"pivotColumns":[],"pivotAggregation":null,"useConsistentColors":false,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","latestUserId":null,"commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"isLockedInExamMode":false,"iPythonMetadata":null,"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"tableResultSubCmdIds":[],"tableResultIndex":null,"nuid":"80cd7a54-5f79-41da-9255-fa6f69df53a6"},{"version":"CommandV1","origId":1575837098435423,"guid":"0cba9f28-8857-474c-9e76-dc3ba4572b02","subtype":"command","commandType":"auto","position":13.0,"command":"datetimesDF. \\\n    withColumn(\"unix_date_id\", unix_timestamp(col(\"dateid\").cast(\"string\"), \"yyyyMMdd\")). \\\n    withColumn(\"unix_date\", unix_timestamp(\"date\", \"yyyy-MM-dd\")). \\\n    withColumn(\"unix_time\", unix_timestamp(\"time\")). \\\n    show()","commandVersion":1,"state":"error","results":{"type":"listResults","data":[{"type":"html","data":"<div class=\"ansiout\"></div>","arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[],"metadata":{}}],"arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[],"metadata":{}},"errorSummary":"org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 198.0 failed 4 times, most recent failure: Lost task 0.3 in stage 198.0 (TID 406) (10.139.64.4 executor driver): org.apache.spark.SparkUpgradeException: You may get a different result due to the upgrading of Spark 3.0: Fail to parse &#39;2014-02-28 10:00:00.123&#39; in the new parser. You can set spark.sql.legacy.timeParserPolicy to LEGACY to restore the behavior before Spark 3.0, or set to CORRECTED and treat it as an invalid datetime string.","errorTraceType":"html","error":"<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">Py4JJavaError</span>                             Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-1603018110318939&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-fg\">----&gt; 1</span><span class=\"ansi-red-fg\"> </span>datetimesDF<span class=\"ansi-blue-fg\">.</span><span class=\"ansi-red-fg\"> </span><span class=\"ansi-red-fg\">\\</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      2</span>     withColumn<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;unix_date_id&#34;</span><span class=\"ansi-blue-fg\">,</span> unix_timestamp<span class=\"ansi-blue-fg\">(</span>col<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;dateid&#34;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>cast<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;string&#34;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">&#34;yyyyMMdd&#34;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span><span class=\"ansi-red-fg\"> </span><span class=\"ansi-red-fg\">\\</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      3</span>     withColumn<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;unix_date&#34;</span><span class=\"ansi-blue-fg\">,</span> unix_timestamp<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;date&#34;</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">&#34;yyyy-MM-dd&#34;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span><span class=\"ansi-red-fg\"> </span><span class=\"ansi-red-fg\">\\</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      4</span>     withColumn<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;unix_time&#34;</span><span class=\"ansi-blue-fg\">,</span> unix_timestamp<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;time&#34;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span><span class=\"ansi-red-fg\"> </span><span class=\"ansi-red-fg\">\\</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      5</span>     show<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/dataframe.py</span> in <span class=\"ansi-cyan-fg\">show</span><span class=\"ansi-blue-fg\">(self, n, truncate, vertical)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    488</span>         &#34;&#34;&#34;\n<span class=\"ansi-green-intense-fg ansi-bold\">    489</span>         <span class=\"ansi-green-fg\">if</span> isinstance<span class=\"ansi-blue-fg\">(</span>truncate<span class=\"ansi-blue-fg\">,</span> bool<span class=\"ansi-blue-fg\">)</span> <span class=\"ansi-green-fg\">and</span> truncate<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 490</span><span class=\"ansi-red-fg\">             </span>print<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">.</span>_jdf<span class=\"ansi-blue-fg\">.</span>showString<span class=\"ansi-blue-fg\">(</span>n<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-cyan-fg\">20</span><span class=\"ansi-blue-fg\">,</span> vertical<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    491</span>         <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    492</span>             print<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">.</span>_jdf<span class=\"ansi-blue-fg\">.</span>showString<span class=\"ansi-blue-fg\">(</span>n<span class=\"ansi-blue-fg\">,</span> int<span class=\"ansi-blue-fg\">(</span>truncate<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">,</span> vertical<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py</span> in <span class=\"ansi-cyan-fg\">__call__</span><span class=\"ansi-blue-fg\">(self, *args)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1302</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   1303</span>         answer <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>gateway_client<span class=\"ansi-blue-fg\">.</span>send_command<span class=\"ansi-blue-fg\">(</span>command<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg\">-&gt; 1304</span><span class=\"ansi-red-fg\">         return_value = get_return_value(\n</span><span class=\"ansi-green-intense-fg ansi-bold\">   1305</span>             answer, self.gateway_client, self.target_id, self.name)\n<span class=\"ansi-green-intense-fg ansi-bold\">   1306</span> \n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">deco</span><span class=\"ansi-blue-fg\">(*a, **kw)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    115</span>     <span class=\"ansi-green-fg\">def</span> deco<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    116</span>         <span class=\"ansi-green-fg\">try</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 117</span><span class=\"ansi-red-fg\">             </span><span class=\"ansi-green-fg\">return</span> f<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    118</span>         <span class=\"ansi-green-fg\">except</span> py4j<span class=\"ansi-blue-fg\">.</span>protocol<span class=\"ansi-blue-fg\">.</span>Py4JJavaError <span class=\"ansi-green-fg\">as</span> e<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    119</span>             converted <span class=\"ansi-blue-fg\">=</span> convert_exception<span class=\"ansi-blue-fg\">(</span>e<span class=\"ansi-blue-fg\">.</span>java_exception<span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py</span> in <span class=\"ansi-cyan-fg\">get_return_value</span><span class=\"ansi-blue-fg\">(answer, gateway_client, target_id, name)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    324</span>             value <span class=\"ansi-blue-fg\">=</span> OUTPUT_CONVERTER<span class=\"ansi-blue-fg\">[</span>type<span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">(</span>answer<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">2</span><span class=\"ansi-blue-fg\">:</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> gateway_client<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    325</span>             <span class=\"ansi-green-fg\">if</span> answer<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">]</span> <span class=\"ansi-blue-fg\">==</span> REFERENCE_TYPE<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 326</span><span class=\"ansi-red-fg\">                 raise Py4JJavaError(\n</span><span class=\"ansi-green-intense-fg ansi-bold\">    327</span>                     <span class=\"ansi-blue-fg\">&#34;An error occurred while calling {0}{1}{2}.\\n&#34;</span><span class=\"ansi-blue-fg\">.</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    328</span>                     format(target_id, &#34;.&#34;, name), value)\n\n<span class=\"ansi-red-fg\">Py4JJavaError</span>: An error occurred while calling o372.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 198.0 failed 4 times, most recent failure: Lost task 0.3 in stage 198.0 (TID 406) (10.139.64.4 executor driver): org.apache.spark.SparkUpgradeException: You may get a different result due to the upgrading of Spark 3.0: Fail to parse &#39;2014-02-28 10:00:00.123&#39; in the new parser. You can set spark.sql.legacy.timeParserPolicy to LEGACY to restore the behavior before Spark 3.0, or set to CORRECTED and treat it as an invalid datetime string.\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:150)\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:141)\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.$anonfun$parse$1(TimestampFormatter.scala:86)\n\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:77)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.project_doConsume_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:757)\n\tat org.apache.spark.sql.execution.collect.UnsafeRowBatchUtils$.encodeUnsafeRows(UnsafeRowBatchUtils.scala:80)\n\tat org.apache.spark.sql.execution.collect.Collector.$anonfun$processFunc$1(Collector.scala:178)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:150)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:119)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:91)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:813)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1605)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:816)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:672)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.time.format.DateTimeParseException: Text &#39;2014-02-28 10:00:00.123&#39; could not be parsed, unparsed text found at index 19\n\tat java.time.format.DateTimeFormatter.parseResolved0(DateTimeFormatter.java:1952)\n\tat java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1777)\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.$anonfun$parse$1(TimestampFormatter.scala:78)\n\t... 27 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2828)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2775)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2769)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2769)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1305)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1305)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1305)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3036)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2977)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2965)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1067)\n\tat org.apache.spark.SparkContext.runJobInternal(SparkContext.scala:2476)\n\tat org.apache.spark.sql.execution.collect.Collector.runSparkJobs(Collector.scala:289)\n\tat org.apache.spark.sql.execution.collect.Collector.collect(Collector.scala:299)\n\tat org.apache.spark.sql.execution.collect.Collector$.collect(Collector.scala:82)\n\tat org.apache.spark.sql.execution.collect.Collector$.collect(Collector.scala:88)\n\tat org.apache.spark.sql.execution.collect.InternalRowFormat$.collect(cachedSparkResults.scala:75)\n\tat org.apache.spark.sql.execution.collect.InternalRowFormat$.collect(cachedSparkResults.scala:62)\n\tat org.apache.spark.sql.execution.ResultCacheManager.$anonfun$getOrComputeResultInternal$1(ResultCacheManager.scala:512)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.execution.ResultCacheManager.getOrComputeResultInternal(ResultCacheManager.scala:511)\n\tat org.apache.spark.sql.execution.ResultCacheManager.getOrComputeResult(ResultCacheManager.scala:399)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollectResult(limit.scala:59)\n\tat org.apache.spark.sql.Dataset.collectResult(Dataset.scala:3041)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3833)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2765)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3825)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$5(SQLExecution.scala:130)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:273)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:104)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:854)\n\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:77)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:223)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3823)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2765)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2972)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:317)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:354)\n\tat sun.reflect.GeneratedMethodAccessor317.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkUpgradeException: You may get a different result due to the upgrading of Spark 3.0: Fail to parse &#39;2014-02-28 10:00:00.123&#39; in the new parser. You can set spark.sql.legacy.timeParserPolicy to LEGACY to restore the behavior before Spark 3.0, or set to CORRECTED and treat it as an invalid datetime string.\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:150)\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:141)\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.$anonfun$parse$1(TimestampFormatter.scala:86)\n\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:77)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.project_doConsume_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:757)\n\tat org.apache.spark.sql.execution.collect.UnsafeRowBatchUtils$.encodeUnsafeRows(UnsafeRowBatchUtils.scala:80)\n\tat org.apache.spark.sql.execution.collect.Collector.$anonfun$processFunc$1(Collector.scala:178)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:150)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:119)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:91)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:813)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1605)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:816)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:672)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: java.time.format.DateTimeParseException: Text &#39;2014-02-28 10:00:00.123&#39; could not be parsed, unparsed text found at index 19\n\tat java.time.format.DateTimeFormatter.parseResolved0(DateTimeFormatter.java:1952)\n\tat java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1777)\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.$anonfun$parse$1(TimestampFormatter.scala:78)\n\t... 27 more\n</div>","workflows":[],"startTime":1641160197772,"submitTime":1641160197772,"finishTime":1641160205729,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":[],"yColumns":[],"pivotColumns":[],"pivotAggregation":null,"useConsistentColors":false,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","latestUserId":null,"commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"isLockedInExamMode":false,"iPythonMetadata":null,"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"tableResultSubCmdIds":[],"tableResultIndex":null,"nuid":"40d2fc3c-d716-41e9-b7b5-6211b2cb52a5"},{"version":"CommandV1","origId":1575837098435424,"guid":"0ad4f4da-c297-473a-a5fc-7f35e1078aae","subtype":"command","commandType":"auto","position":14.0,"command":"%md\n* Create a Dataframe by name unixtimesDF with one column unixtime using 4 values. You can use the unix timestamp generated for time column in previous task.","commandVersion":1,"state":"finished","results":{"type":"raw","data":"","arguments":{},"metadata":{}},"errorSummary":null,"errorTraceType":null,"error":null,"workflows":[],"startTime":0,"submitTime":0,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":[],"yColumns":[],"pivotColumns":[],"pivotAggregation":null,"useConsistentColors":false,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","latestUserId":null,"commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"isLockedInExamMode":false,"iPythonMetadata":null,"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"tableResultSubCmdIds":[],"tableResultIndex":null,"nuid":"0eabdb55-f8ce-4d9e-a6a8-6cc89b3f72e7"},{"version":"CommandV1","origId":1575837098435425,"guid":"2f2c0f31-afec-4a89-af87-2d4290030a01","subtype":"command","commandType":"auto","position":15.0,"command":"unixtimes = [(1393561800, ),\n             (1456713488, ),\n             (1514701799, ),\n             (1567189800, )\n            ]","commandVersion":1,"state":"finished","results":{"type":"listResults","data":[{"type":"html","data":"<div class=\"ansiout\"></div>","arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[],"metadata":{}}],"arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[],"metadata":{}},"errorSummary":null,"errorTraceType":null,"error":null,"workflows":[],"startTime":1641160205694,"submitTime":1641160198103,"finishTime":1641160205712,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":[],"yColumns":[],"pivotColumns":[],"pivotAggregation":null,"useConsistentColors":false,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","latestUserId":null,"commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"isLockedInExamMode":false,"iPythonMetadata":null,"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"tableResultSubCmdIds":[],"tableResultIndex":null,"nuid":"6d7c687a-5658-49c0-b4f4-254da1f788da"},{"version":"CommandV1","origId":1575837098435426,"guid":"0191ba5b-f539-4ce5-bbf7-7ec533f4bf56","subtype":"command","commandType":"auto","position":16.0,"command":"unixtimesDF = spark.createDataFrame(unixtimes).toDF(\"unixtime\")","commandVersion":1,"state":"finished","results":{"type":"listResults","data":[{"type":"html","data":"<div class=\"ansiout\"></div>","arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[],"metadata":{}}],"arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[{"name":"unixtimesDF","typeStr":"pyspark.sql.dataframe.DataFrame","schema":{"fields":[{"metadata":{},"name":"unixtime","nullable":true,"type":"long"}],"type":"struct"},"tableIdentifier":null}],"metadata":{}},"errorSummary":null,"errorTraceType":null,"error":null,"workflows":[],"startTime":1641160205717,"submitTime":1641160198428,"finishTime":1641160205812,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":[],"yColumns":[],"pivotColumns":[],"pivotAggregation":null,"useConsistentColors":false,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","latestUserId":null,"commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"isLockedInExamMode":false,"iPythonMetadata":null,"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"tableResultSubCmdIds":[],"tableResultIndex":null,"nuid":"a10f37bb-fa0b-49fe-beac-3aec7478b918"},{"version":"CommandV1","origId":1575837098435427,"guid":"080738bb-9d1d-494f-af26-0ad083eaeac6","subtype":"command","commandType":"auto","position":17.0,"command":"unixtimesDF.show()","commandVersion":1,"state":"finished","results":{"type":"listResults","data":[{"type":"html","data":"<div class=\"ansiout\">+----------+\n|  unixtime|\n+----------+\n|1393561800|\n|1456713488|\n|1514701799|\n|1567189800|\n+----------+\n\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[],"metadata":{}}],"arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[],"metadata":{}},"errorSummary":null,"errorTraceType":null,"error":null,"workflows":[],"startTime":1641160205817,"submitTime":1641160198924,"finishTime":1641160207096,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":[],"yColumns":[],"pivotColumns":[],"pivotAggregation":null,"useConsistentColors":false,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","latestUserId":null,"commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"isLockedInExamMode":false,"iPythonMetadata":null,"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"tableResultSubCmdIds":[],"tableResultIndex":null,"nuid":"4c0ed107-56b2-477c-beba-3553aabe8da5"},{"version":"CommandV1","origId":1575837098435428,"guid":"69ad69e8-6635-4da5-bfbd-501e1aed7fe9","subtype":"command","commandType":"auto","position":18.0,"command":"unixtimesDF.printSchema()","commandVersion":1,"state":"finished","results":{"type":"listResults","data":[{"type":"html","data":"<div class=\"ansiout\">root\n |-- unixtime: long (nullable = true)\n\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[],"metadata":{}}],"arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[],"metadata":{}},"errorSummary":null,"errorTraceType":null,"error":null,"workflows":[],"startTime":1641160207102,"submitTime":1641160199234,"finishTime":1641160207130,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":[],"yColumns":[],"pivotColumns":[],"pivotAggregation":null,"useConsistentColors":false,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","latestUserId":null,"commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"isLockedInExamMode":false,"iPythonMetadata":null,"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"tableResultSubCmdIds":[],"tableResultIndex":null,"nuid":"53cf0cf0-bf87-4c1f-9935-c4f3e942c0a9"},{"version":"CommandV1","origId":1575837098435429,"guid":"87bef1a4-eb24-4a8a-b0aa-cfac2633acd1","subtype":"command","commandType":"auto","position":19.0,"command":"%md\n* Get date in yyyyMMdd format and also complete timestamp.","commandVersion":1,"state":"finished","results":{"type":"raw","data":"","arguments":{},"metadata":{}},"errorSummary":null,"errorTraceType":null,"error":null,"workflows":[],"startTime":0,"submitTime":0,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":[],"yColumns":[],"pivotColumns":[],"pivotAggregation":null,"useConsistentColors":false,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","latestUserId":null,"commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"isLockedInExamMode":false,"iPythonMetadata":null,"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"tableResultSubCmdIds":[],"tableResultIndex":null,"nuid":"c4f91f28-083d-4a97-8bf2-42ea68a5c69d"},{"version":"CommandV1","origId":1575837098435430,"guid":"69ecde3b-00af-4b55-b231-9771ec4c57fb","subtype":"command","commandType":"auto","position":20.0,"command":"from pyspark.sql.functions import from_unixtime","commandVersion":1,"state":"finished","results":{"type":"listResults","data":[{"type":"html","data":"<div class=\"ansiout\"></div>","arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[],"metadata":{}}],"arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[],"metadata":{}},"errorSummary":null,"errorTraceType":null,"error":null,"workflows":[],"startTime":1641160207136,"submitTime":1641160199602,"finishTime":1641160207149,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":[],"yColumns":[],"pivotColumns":[],"pivotAggregation":null,"useConsistentColors":false,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","latestUserId":null,"commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"isLockedInExamMode":false,"iPythonMetadata":{"pycharm":{"name":"#%%\n"}},"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"tableResultSubCmdIds":[],"tableResultIndex":null,"nuid":"9d7a214e-660b-4266-8d17-916577d70d7b"},{"version":"CommandV1","origId":1575837098435431,"guid":"4c4edd69-6405-466d-afb4-376f359fb1e7","subtype":"command","commandType":"auto","position":21.0,"command":"unixtimesDF. \\\n    withColumn(\"date\", from_unixtime(\"unixtime\", \"yyyyMMdd\")). \\\n    withColumn(\"time\", from_unixtime(\"unixtime\")). \\\n    show()\n#yyyyMMdd","commandVersion":1,"state":"finished","results":{"type":"listResults","data":[{"type":"html","data":"<div class=\"ansiout\">+----------+--------+-------------------+\n|  unixtime|    date|               time|\n+----------+--------+-------------------+\n|1393561800|20140228|2014-02-28 04:30:00|\n|1456713488|20160229|2016-02-29 02:38:08|\n|1514701799|20171231|2017-12-31 06:29:59|\n|1567189800|20190830|2019-08-30 18:30:00|\n+----------+--------+-------------------+\n\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[],"metadata":{}}],"arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[],"metadata":{}},"errorSummary":null,"errorTraceType":null,"error":null,"workflows":[],"startTime":1641160207154,"submitTime":1641160199913,"finishTime":1641160207575,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":[],"yColumns":[],"pivotColumns":[],"pivotAggregation":null,"useConsistentColors":false,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","latestUserId":null,"commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"isLockedInExamMode":false,"iPythonMetadata":{"pycharm":{"name":"#%%\n"}},"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"tableResultSubCmdIds":[],"tableResultIndex":null,"nuid":"4f42d852-12d7-49b1-bc58-e5b10b90d5d2"},{"version":"CommandV1","origId":1575837098435432,"guid":"73c62b9f-cd77-4b40-92d5-ec635d1d378a","subtype":"command","commandType":"auto","position":22.0,"command":"","commandVersion":1,"state":"finished","results":{"type":"listResults","data":[],"arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[],"metadata":{}},"errorSummary":null,"errorTraceType":null,"error":null,"workflows":[],"startTime":0,"submitTime":0,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":[],"yColumns":[],"pivotColumns":[],"pivotAggregation":null,"useConsistentColors":false,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","latestUserId":null,"commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"isLockedInExamMode":false,"iPythonMetadata":null,"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"tableResultSubCmdIds":[],"tableResultIndex":null,"nuid":"73a81bed-f9e5-4169-bc0c-f9983cb82361"}],"dashboards":[],"guid":"cba76b3b-47fc-443d-8b1b-abb72635a350","globalVars":{},"iPythonMetadata":{"nbformat":4,"IPythonMetadata":{"kernelspec":{"display_name":"Pyspark 2","language":"python","name":"pyspark2"},"language_info":{"mimetype":"text/x-python","name":"python","pygments_lexer":"ipython3","codemirror_mode":{"name":"ipython","version":3},"version":"3.6.12","nbconvert_exporter":"python","file_extension":".py"}}},"inputWidgets":{},"notebookMetadata":{"pythonIndentUnit":4}}
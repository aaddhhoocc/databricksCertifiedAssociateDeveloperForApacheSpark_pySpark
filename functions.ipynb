{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26ca8869-edb3-4252-9d94-c72a844ee969",
   "metadata": {},
   "source": [
    "* col, expr, cast, alias\n",
    "* lit\n",
    "* concat, concat_ws\n",
    "* upper, lower, initcap\n",
    "\n",
    "* `split`, `explode`\n",
    "* `regexp_replace`, `regexp_extract`\n",
    "* lpad, rpad\n",
    "* ltrim, rtrim, trim\n",
    "\n",
    "* type,\n",
    "\n",
    "* `size` - array length\n",
    "* `length` - string/list length\n",
    "\n",
    "\n",
    "* current_date, current_timestamp, to_date, to_timestamp\n",
    "* date_add, date_sub, datediff, months_between, add_months, next_day\n",
    "* trunc, date_trunc\n",
    "* year, month, weekeofyear, dayofyear, dayofmonth, dayofweek, hour minute second\n",
    "* date_format\n",
    "* unix_timestamp, from_unixtime\n",
    "\n",
    "* coalesce, nvl \n",
    "* sql, selectExpr, expr\n",
    "\n",
    "* na, fillna, replacena, dropna, isnan, isin\n",
    "* isNull, isNotNull\n",
    "* distinct, drop_duplicates, dropDuplicates\n",
    "* df.na.drop, df.dropna\n",
    "\n",
    "* sort, orderBy\n",
    "* desc, asc, col -> asc(), asc_nulls_first(), asc_nulls_last(), desc(), desc_nulls_first(), desc_nulls_last()\n",
    "* agg\n",
    "\n",
    "* `spark.read.format(format).load()`\n",
    "* `DataFrame.write.format(format).save()` (`DataFrame.write` -> `DataFrameWritter`)\n",
    "* `option`, `options`, `schema`, `mode`\n",
    "* `dataFrame.inputFiles()`\n",
    "\n",
    "\n",
    "* `Column.astype()`, `Column.cast()`\n",
    "* `pyspark.sql.functions.udf()` requires a retutn type unless return type is a string, `SparkSession.register.udf()`\n",
    "  \n",
    "`spark.udf.register` is used to register UDF to be invoked in Spark SQL query.\n",
    "`pyspark.sql.functions.udf` is used to create UDF to be called when using DataFrame API.\n",
    "\n",
    "- `join`, `union`, `broadcast`\n",
    "-  `DataFrame.dropDuplicates`, `DataFrame.drop_duplicates` - alias  # subset as list only"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ffab82-1938-46e9-9c04-02d25a987cce",
   "metadata": {},
   "source": [
    "## Actions - immediate execution\n",
    "count, show, \n",
    "\n",
    "postponed execution until the action\n",
    "select, "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31cbaaaf-d83f-425b-9f4e-3b1a0075ba37",
   "metadata": {},
   "source": [
    "## Parameters\n",
    "\n",
    "`spark.conf.get()`, `spark.conf.set()`\n",
    "\n",
    "* `spark.sql.repl.eagerEval.enabled` \n",
    "* `spark.sql.autoBroadcastJoinThreshold` - default \n",
    "* `spark.sql.shuffle.partitions` - default 200\n",
    "* `spark.sql.parquet.compression.codec` - to overwrite default snappy compression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "710290fc-7110-4060-89b1-13da6a839f51",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-04T09:52:55.465232Z",
     "iopub.status.busy": "2024-07-04T09:52:55.464492Z",
     "iopub.status.idle": "2024-07-04T09:52:55.481356Z",
     "shell.execute_reply": "2024-07-04T09:52:55.479737Z",
     "shell.execute_reply.started": "2024-07-04T09:52:55.465185Z"
    }
   },
   "source": [
    "`cache`, `persist`, `unpersist`\n",
    "* take, collect, parallelize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22306e4-333c-4fe9-80ca-c0217c16c168",
   "metadata": {},
   "source": [
    "accumulator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9447878e-748a-4608-8dba-a9d7691a4b9c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-02T15:46:36.601178Z",
     "iopub.status.busy": "2024-07-02T15:46:36.600377Z",
     "iopub.status.idle": "2024-07-02T15:46:36.612600Z",
     "shell.execute_reply": "2024-07-02T15:46:36.610800Z",
     "shell.execute_reply.started": "2024-07-02T15:46:36.601117Z"
    }
   },
   "source": [
    "Spark Execution Hierachy - **Job** -> **Shuffle** -> **Task**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a619c63-eac5-4bc3-997e-df59013f1525",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-02T11:25:48.084702Z",
     "iopub.status.busy": "2024-07-02T11:25:48.083953Z",
     "iopub.status.idle": "2024-07-02T11:25:48.514824Z",
     "shell.execute_reply": "2024-07-02T11:25:48.514539Z",
     "shell.execute_reply.started": "2024-07-02T11:25:48.084650Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('instance').getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bebe8d76-7a4c-4ac1-b545-722894220a76",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-02T11:36:53.840555Z",
     "iopub.status.busy": "2024-07-02T11:36:53.839791Z",
     "iopub.status.idle": "2024-07-02T11:36:54.133730Z",
     "shell.execute_reply": "2024-07-02T11:36:54.133451Z",
     "shell.execute_reply.started": "2024-07-02T11:36:53.840507Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('apple', <pyspark.resultiterable.ResultIterable object at 0x11f937890>), ('orange', <pyspark.resultiterable.ResultIterable object at 0x11abb1850>)]\n",
      "<generator object <genexpr> at 0x104100520>\n"
     ]
    }
   ],
   "source": [
    "data = [('apple', 3), ('orange', 2), ('apple', 5), ('orange', 4)]\n",
    "rdd = sc.parallelize(data)\n",
    "result_rdd = rdd.groupByKey()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3380a76e-c70e-4514-9c21-166a0e751b82",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-02T11:44:37.640242Z",
     "iopub.status.busy": "2024-07-02T11:44:37.639679Z",
     "iopub.status.idle": "2024-07-02T11:44:37.693039Z",
     "shell.execute_reply": "2024-07-02T11:44:37.692525Z",
     "shell.execute_reply.started": "2024-07-02T11:44:37.640206Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- fruit: string (nullable = true)\n",
      " |-- amount: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([('apple', 3), ('orange', 2), ('apple', 5), ('orange', 4)], ['fruit', 'amount'])\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "713f60de-c01f-44be-ad9c-0ed901203201",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-02T11:53:40.278970Z",
     "iopub.status.busy": "2024-07-02T11:53:40.278408Z",
     "iopub.status.idle": "2024-07-02T11:53:40.732729Z",
     "shell.execute_reply": "2024-07-02T11:53:40.732403Z",
     "shell.execute_reply.started": "2024-07-02T11:53:40.278945Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+\n",
      "| fruit|sum(amount)|\n",
      "+------+-----------+\n",
      "| apple|          8|\n",
      "|orange|          6|\n",
      "+------+-----------+\n",
      "\n",
      "+------+-----------+\n",
      "| fruit|sum(amount)|\n",
      "+------+-----------+\n",
      "| apple|          8|\n",
      "|orange|          6|\n",
      "+------+-----------+\n",
      "\n",
      "+------+-----------+\n",
      "| fruit|sum(amount)|\n",
      "+------+-----------+\n",
      "| apple|          8|\n",
      "|orange|          6|\n",
      "+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('fruit').sum().show()\n",
    "df.groupBy(df.fruit).agg({'amount': 'sum'}).show()\n",
    "\n",
    "from pyspark.sql.functions import sum\n",
    "df.groupBy(df.fruit).agg(sum('amount')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ef2029dd-f4ac-4a9b-a617-eb4c26a1409e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-02T12:29:49.821000Z",
     "iopub.status.busy": "2024-07-02T12:29:49.820289Z",
     "iopub.status.idle": "2024-07-02T12:29:49.831532Z",
     "shell.execute_reply": "2024-07-02T12:29:49.830813Z",
     "shell.execute_reply.started": "2024-07-02T12:29:49.820964Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function persist in module pyspark.sql.dataframe:\n",
      "\n",
      "persist(self, storageLevel: pyspark.storagelevel.StorageLevel = StorageLevel(True, True, False, True, 1)) -> 'DataFrame'\n",
      "    Sets the storage level to persist the contents of the :class:`DataFrame` across\n",
      "    operations after the first time it is computed. This can only be used to assign\n",
      "    a new storage level if the :class:`DataFrame` does not have a storage level set yet.\n",
      "    If no storage level is specified defaults to (`MEMORY_AND_DISK_DESER`)\n",
      "    \n",
      "    .. versionadded:: 1.3.0\n",
      "    \n",
      "    .. versionchanged:: 3.4.0\n",
      "        Supports Spark Connect.\n",
      "    \n",
      "    Notes\n",
      "    -----\n",
      "    The default storage level has changed to `MEMORY_AND_DISK_DESER` to match Scala in 3.0.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    storageLevel : :class:`StorageLevel`\n",
      "        Storage level to set for persistence. Default is MEMORY_AND_DISK_DESER.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    :class:`DataFrame`\n",
      "        Persisted DataFrame.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> df = spark.range(1)\n",
      "    >>> df.persist()\n",
      "    DataFrame[id: bigint]\n",
      "    \n",
      "    >>> df.explain()\n",
      "    == Physical Plan ==\n",
      "    AdaptiveSparkPlan isFinalPlan=false\n",
      "    +- InMemoryTableScan ...\n",
      "    \n",
      "    Persists the data in the disk by specifying the storage level.\n",
      "    \n",
      "    >>> from pyspark.storagelevel import StorageLevel\n",
      "    >>> df.persist(StorageLevel.DISK_ONLY)\n",
      "    DataFrame[id: bigint]\n",
      "\n",
      "Help on class StorageLevel in module pyspark.storagelevel:\n",
      "\n",
      "class StorageLevel(builtins.object)\n",
      " |  StorageLevel(useDisk: bool, useMemory: bool, useOffHeap: bool, deserialized: bool, replication: int = 1)\n",
      " |  \n",
      " |  Flags for controlling the storage of an RDD. Each StorageLevel records whether to use memory,\n",
      " |  whether to drop the RDD to disk if it falls out of memory, whether to keep the data in memory\n",
      " |  in a JAVA-specific serialized format, and whether to replicate the RDD partitions on multiple\n",
      " |  nodes. Also contains static constants for some commonly used storage levels, MEMORY_ONLY.\n",
      " |  Since the data is always serialized on the Python side, all the constants use the serialized\n",
      " |  formats.\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __eq__(self, other: Any) -> bool\n",
      " |      Return self==value.\n",
      " |  \n",
      " |  __init__(self, useDisk: bool, useMemory: bool, useOffHeap: bool, deserialized: bool, replication: int = 1)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  __repr__(self) -> str\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __str__(self) -> str\n",
      " |      Return str(self).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  DISK_ONLY = StorageLevel(True, False, False, False, 1)\n",
      " |  \n",
      " |  DISK_ONLY_2 = StorageLevel(True, False, False, False, 2)\n",
      " |  \n",
      " |  DISK_ONLY_3 = StorageLevel(True, False, False, False, 3)\n",
      " |  \n",
      " |  MEMORY_AND_DISK = StorageLevel(True, True, False, False, 1)\n",
      " |  \n",
      " |  MEMORY_AND_DISK_2 = StorageLevel(True, True, False, False, 2)\n",
      " |  \n",
      " |  MEMORY_AND_DISK_DESER = StorageLevel(True, True, False, True, 1)\n",
      " |  \n",
      " |  MEMORY_ONLY = StorageLevel(False, True, False, False, 1)\n",
      " |  \n",
      " |  MEMORY_ONLY_2 = StorageLevel(False, True, False, False, 2)\n",
      " |  \n",
      " |  NONE = StorageLevel(False, False, False, False, 1)\n",
      " |  \n",
      " |  OFF_HEAP = StorageLevel(True, True, True, False, 1)\n",
      " |  \n",
      " |  __annotations__ = {'DISK_ONLY': typing.ClassVar[ForwardRef('StorageLev...\n",
      " |  \n",
      " |  __hash__ = None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from pyspark.storagelevel import StorageLevel\n",
    "\n",
    "#help(DataFrame.agg)\n",
    "\n",
    "help(DataFrame.persist)\n",
    "help(StorageLevel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a5b46ef5-fdc8-4458-899a-a0a75c5251db",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-02T17:39:18.976400Z",
     "iopub.status.busy": "2024-07-02T17:39:18.975636Z",
     "iopub.status.idle": "2024-07-02T17:39:18.985206Z",
     "shell.execute_reply": "2024-07-02T17:39:18.984532Z",
     "shell.execute_reply.started": "2024-07-02T17:39:18.976351Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function collect in module pyspark.sql.dataframe:\n",
      "\n",
      "collect(self) -> List[pyspark.sql.types.Row]\n",
      "    Returns all the records as a list of :class:`Row`.\n",
      "    \n",
      "    .. versionadded:: 1.3.0\n",
      "    \n",
      "    .. versionchanged:: 3.4.0\n",
      "        Supports Spark Connect.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    list\n",
      "        List of rows.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> df = spark.createDataFrame(\n",
      "    ...     [(14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n",
      "    >>> df.collect()\n",
      "    [Row(age=14, name='Tom'), Row(age=23, name='Alice'), Row(age=16, name='Bob')]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(DataFrame.collect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "0279a704-c179-4d5a-8d85-0f544a37cd17",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-02T18:02:37.345693Z",
     "iopub.status.busy": "2024-07-02T18:02:37.344951Z",
     "iopub.status.idle": "2024-07-02T18:02:37.356911Z",
     "shell.execute_reply": "2024-07-02T18:02:37.354906Z",
     "shell.execute_reply.started": "2024-07-02T18:02:37.345649Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function dropDuplicates in module pyspark.sql.dataframe:\n",
      "\n",
      "dropDuplicates(self, subset: Optional[List[str]] = None) -> 'DataFrame'\n",
      "    Return a new :class:`DataFrame` with duplicate rows removed,\n",
      "    optionally only considering certain columns.\n",
      "    \n",
      "    For a static batch :class:`DataFrame`, it just drops duplicate rows. For a streaming\n",
      "    :class:`DataFrame`, it will keep all data across triggers as intermediate state to drop\n",
      "    duplicates rows. You can use :func:`withWatermark` to limit how late the duplicate data can\n",
      "    be and the system will accordingly limit the state. In addition, data older than\n",
      "    watermark will be dropped to avoid any possibility of duplicates.\n",
      "    \n",
      "    :func:`drop_duplicates` is an alias for :func:`dropDuplicates`.\n",
      "    \n",
      "    .. versionadded:: 1.4.0\n",
      "    \n",
      "    .. versionchanged:: 3.4.0\n",
      "        Supports Spark Connect.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    subset : List of column names, optional\n",
      "        List of columns to use for duplicate comparison (default All columns).\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    :class:`DataFrame`\n",
      "        DataFrame without duplicates.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> from pyspark.sql import Row\n",
      "    >>> df = spark.createDataFrame([\n",
      "    ...     Row(name='Alice', age=5, height=80),\n",
      "    ...     Row(name='Alice', age=5, height=80),\n",
      "    ...     Row(name='Alice', age=10, height=80)\n",
      "    ... ])\n",
      "    \n",
      "    Deduplicate the same rows.\n",
      "    \n",
      "    >>> df.dropDuplicates().show()\n",
      "    +-----+---+------+\n",
      "    | name|age|height|\n",
      "    +-----+---+------+\n",
      "    |Alice|  5|    80|\n",
      "    |Alice| 10|    80|\n",
      "    +-----+---+------+\n",
      "    \n",
      "    Deduplicate values on 'name' and 'height' columns.\n",
      "    \n",
      "    >>> df.dropDuplicates(['name', 'height']).show()\n",
      "    +-----+---+------+\n",
      "    | name|age|height|\n",
      "    +-----+---+------+\n",
      "    |Alice|  5|    80|\n",
      "    +-----+---+------+\n",
      "\n",
      "Help on function distinct in module pyspark.sql.dataframe:\n",
      "\n",
      "distinct(self) -> 'DataFrame'\n",
      "    Returns a new :class:`DataFrame` containing the distinct rows in this :class:`DataFrame`.\n",
      "    \n",
      "    .. versionadded:: 1.3.0\n",
      "    \n",
      "    .. versionchanged:: 3.4.0\n",
      "        Supports Spark Connect.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    :class:`DataFrame`\n",
      "        DataFrame with distinct records.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> df = spark.createDataFrame(\n",
      "    ...     [(14, \"Tom\"), (23, \"Alice\"), (23, \"Alice\")], [\"age\", \"name\"])\n",
      "    \n",
      "    Return the number of distinct rows in the :class:`DataFrame`\n",
      "    \n",
      "    >>> df.distinct().count()\n",
      "    2\n",
      "\n",
      "Help on function isin in module pyspark.sql.column:\n",
      "\n",
      "isin(self, *cols: Any) -> 'Column'\n",
      "    A boolean expression that is evaluated to true if the value of this\n",
      "    expression is contained by the evaluated values of the arguments.\n",
      "    \n",
      "    .. versionadded:: 1.5.0\n",
      "    \n",
      "    .. versionchanged:: 3.4.0\n",
      "        Supports Spark Connect.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    cols\n",
      "        The result will only be true at a location if any value matches in the Column.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    :class:`Column`\n",
      "        Column of booleans showing whether each element in the Column is contained in cols.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> df = spark.createDataFrame(\n",
      "    ...      [(2, \"Alice\"), (5, \"Bob\")], [\"age\", \"name\"])\n",
      "    >>> df[df.name.isin(\"Bob\", \"Mike\")].collect()\n",
      "    [Row(age=5, name='Bob')]\n",
      "    >>> df[df.age.isin([1, 2, 3])].collect()\n",
      "    [Row(age=2, name='Alice')]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(DataFrame.dropDuplicates) # - subset as list only\n",
    "help(DataFrame.distinct) # - subset as list only\n",
    "from pyspark.sql import Column\n",
    "help(Column.isin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "e6ea5b74-02e2-455a-82b6-eb09186a42de",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-02T18:03:33.472241Z",
     "iopub.status.busy": "2024-07-02T18:03:33.471882Z",
     "iopub.status.idle": "2024-07-02T18:03:33.479617Z",
     "shell.execute_reply": "2024-07-02T18:03:33.478872Z",
     "shell.execute_reply.started": "2024-07-02T18:03:33.472211Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'ColumnOrName'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m'DataFrame'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Returns a new :class:`DataFrame` without specified columns.\n",
       "This is a no-op if the schema doesn't contain the given column name(s).\n",
       "\n",
       ".. versionadded:: 1.4.0\n",
       "\n",
       ".. versionchanged:: 3.4.0\n",
       "    Supports Spark Connect.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "cols: str or :class:`Column`\n",
       "    a name of the column, or the :class:`Column` to drop\n",
       "\n",
       "Returns\n",
       "-------\n",
       ":class:`DataFrame`\n",
       "    DataFrame without given columns.\n",
       "\n",
       "Notes\n",
       "-----\n",
       "When an input is a column name, it is treated literally without further interpretation.\n",
       "Otherwise, will try to match the equivalent expression.\n",
       "So that dropping column by its name `drop(colName)` has different semantic with directly\n",
       "dropping the column `drop(col(colName))`.\n",
       "\n",
       "Examples\n",
       "--------\n",
       ">>> from pyspark.sql import Row\n",
       ">>> from pyspark.sql.functions import col, lit\n",
       ">>> df = spark.createDataFrame(\n",
       "...     [(14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n",
       ">>> df2 = spark.createDataFrame([Row(height=80, name=\"Tom\"), Row(height=85, name=\"Bob\")])\n",
       "\n",
       ">>> df.drop('age').show()\n",
       "+-----+\n",
       "| name|\n",
       "+-----+\n",
       "|  Tom|\n",
       "|Alice|\n",
       "|  Bob|\n",
       "+-----+\n",
       ">>> df.drop(df.age).show()\n",
       "+-----+\n",
       "| name|\n",
       "+-----+\n",
       "|  Tom|\n",
       "|Alice|\n",
       "|  Bob|\n",
       "+-----+\n",
       "\n",
       "Drop the column that joined both DataFrames on.\n",
       "\n",
       ">>> df.join(df2, df.name == df2.name, 'inner').drop('name').sort('age').show()\n",
       "+---+------+\n",
       "|age|height|\n",
       "+---+------+\n",
       "| 14|    80|\n",
       "| 16|    85|\n",
       "+---+------+\n",
       "\n",
       ">>> df3 = df.join(df2)\n",
       ">>> df3.show()\n",
       "+---+-----+------+----+\n",
       "|age| name|height|name|\n",
       "+---+-----+------+----+\n",
       "| 14|  Tom|    80| Tom|\n",
       "| 14|  Tom|    85| Bob|\n",
       "| 23|Alice|    80| Tom|\n",
       "| 23|Alice|    85| Bob|\n",
       "| 16|  Bob|    80| Tom|\n",
       "| 16|  Bob|    85| Bob|\n",
       "+---+-----+------+----+\n",
       "\n",
       "Drop two column by the same name.\n",
       "\n",
       ">>> df3.drop(\"name\").show()\n",
       "+---+------+\n",
       "|age|height|\n",
       "+---+------+\n",
       "| 14|    80|\n",
       "| 14|    85|\n",
       "| 23|    80|\n",
       "| 23|    85|\n",
       "| 16|    80|\n",
       "| 16|    85|\n",
       "+---+------+\n",
       "\n",
       "Can not drop col('name') due to ambiguous reference.\n",
       "\n",
       ">>> df3.drop(col(\"name\")).show()\n",
       "Traceback (most recent call last):\n",
       "...\n",
       "pyspark.errors.exceptions.captured.AnalysisException: [AMBIGUOUS_REFERENCE] Reference...\n",
       "\n",
       ">>> df4 = df.withColumn(\"a.b.c\", lit(1))\n",
       ">>> df4.show()\n",
       "+---+-----+-----+\n",
       "|age| name|a.b.c|\n",
       "+---+-----+-----+\n",
       "| 14|  Tom|    1|\n",
       "| 23|Alice|    1|\n",
       "| 16|  Bob|    1|\n",
       "+---+-----+-----+\n",
       "\n",
       ">>> df4.drop(\"a.b.c\").show()\n",
       "+---+-----+\n",
       "|age| name|\n",
       "+---+-----+\n",
       "| 14|  Tom|\n",
       "| 23|Alice|\n",
       "| 16|  Bob|\n",
       "+---+-----+\n",
       "\n",
       "Can not find a column matching the expression \"a.b.c\".\n",
       "\n",
       ">>> df4.drop(col(\"a.b.c\")).show()\n",
       "+---+-----+-----+\n",
       "|age| name|a.b.c|\n",
       "+---+-----+-----+\n",
       "| 14|  Tom|    1|\n",
       "| 23|Alice|    1|\n",
       "| 16|  Bob|    1|\n",
       "+---+-----+-----+\n",
       "\u001b[0;31mFile:\u001b[0m      /opt/homebrew/lib/python3.11/site-packages/pyspark/sql/dataframe.py\n",
       "\u001b[0;31mType:\u001b[0m      method"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.drop?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
